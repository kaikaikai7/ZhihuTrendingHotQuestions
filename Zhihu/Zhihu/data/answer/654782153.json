[
    {
        "answer_id": 3485644480,
        "content": "根据现有的公开信息，我认为 KAN/Kolmogorov-Arnold Networks 处理实际问题时可能更复杂且效率可能更低，对现有 GPU 不友好、训练起来更慢而不稳定，其“可解释性”在工作内容更复杂时值得怀疑。看起来，KAN 取代不了 MLP。2024 年 5 月初，一些“中文顶刊”大肆吹嘘 KAN 在预印本论文里的表现。这大概又是一波来的易、去得快的炒作。https://doi.org/10.48550/arXiv.2404.19756",
        "voteup_count": 146,
        "updated_time": "2024-05-02 18:54:03",
        "question_id": 654782153,
        "user_id": "eb67c87bde5ed3b991f942e7d4b81838"
    },
    {
        "answer_id": 3485862467,
        "content": "我觉得这种结构早就有了。考古一下deep Kernel machine或者叫deep Gaussian process吧（现在做的人比较少了）AG Wilson, Z Hu, R Salakhutdinov, EP Xing， Deep  Kernel Learning,  AISTATS, 2016A Damianou, ND Lawrence , Deep Gaussian Processes,  AISTATS, 2013直接看deep GP的结构吧这种网络的收敛性分析也相对完善了， to name a fewMM Dunlop, MA Girolami, AM Stuart, AL Teckentrup, How deep are deep Gaussian processes?,  JMLR, 2018G Finocchio, J Schmidt-Hieber, Posterior contraction for deep Gaussian process priors  - JMLR, 2023他和Bayesian nerural net还有spline activation网络的等价性也已经有证明了（无法就是Kernel的傅里叶分解和小波分解。。。。）K Cutajar, EV Bonilla, P Michiardi, M Filippone,Random feature expansions for deep Gaussian processes  - ICML, 2017L Ding, R Tuo, S Shahrampour, A sparse expansion for deep Gaussian processes,  IISE Transactions, 2024结论是取代不了。deep GP也是很难无法train，解决training思路基本是稀疏化。但是如果对结构做稀疏化，就又会回到传统神经网络结构上去了。例如像这个KAN那样用spline做稀疏化，就会得出跟CNN在数学上基本等价的结构（小波、spline和convolution本来就是有极大关系）。那样的话，在各种大数据集上跑跟CNN的准确率也基本是相同的。即使做基于sparsity的网络结构优化，现在GPU上的硬件算法都不会考虑sparsity，所以即使做出来了实际效果也不会有加速，而且计算稳定性也比较麻烦，这是我踩过的坑╮(╯_╰)╭",
        "voteup_count": 114,
        "updated_time": "2024-05-03 02:39:45",
        "question_id": 654782153,
        "user_id": "a7921b463f824811404cbe3da3dbcd66"
    },
    {
        "answer_id": 3485804765,
        "content": "截止5月2号晚上11点，我去git了下来试了一下，感觉代码写的非常糟糕，急需优化。原生的代码我看作者还没检查过？指定device是cuda会出错，你跑进去看函数发现有些没有写指定device，导致一个cuda一个cpu。。。不知道作者想啥。好吧 ，我花了半个小时给他补上to device，跑起来了，但是gpu利用率只有40%，1000，100，100，10，k=3的网络，batch size256居然要4G显存，并且跑的比cpu还慢，所以我放弃了老老实实用回cpu。跑了大概15分钟，mnist能到85%，按上面的配置。所以，现在我不好评价。。。更新一下有人弄出了傅立叶版的kan https://github.com/GistNoesis/FourierKAN。 取代了那个又臭又长的B-spine，现在速度没问题了。我试了一下mnist能到97%，跟mlp差不多的样子？但是这个傅立叶版的问题还是有的，比如把grid调大，比如调到100，mnist就会严重不收敛，有待进一步验证。再度更新，截止5月3号晚上11点。由于原生kan写的太烂了，速度太慢，于是我采用了上面说能跑到97%的FourierKan做实验，尝试将这种kan替换transformer的mlp。我跑的实验是MAE pretrain，但是似乎怎么跑loss都比mlp高。KAN可以宣告完结了？？？",
        "voteup_count": 94,
        "updated_time": "2024-05-05 10:28:21",
        "question_id": 654782153,
        "user_id": "bcbe7c8deaa20f5cf15260c306857132"
    },
    {
        "answer_id": 3487357814,
        "content": "当你声称你的work能够击败xxx的时候，你要拿出来足够的，能够击败xxx的证据，而不是在一堆玩具数据集上测出来，哦，比xxx好，我打败了xxx，然后一通写作包装加一点噱头，又一个划时代的工作横空出世上一个这么样子的是mamba，声称自己能打败transformer，好笑的是iclr审稿就指出了实验不足，需要补足，小修小补，核心争议就是不碰，咱也不知道是自信还是心虚。过了几个月各种mamba全都出来了，都能凑一个international conference of mamba representations了，才发现时序时序不行，文本文本不行，视觉视觉不行，弄半天谁也打不过，吹这么大所以我的观点很直接，你kan你说能打败mlp，咱也别在这堆玩具上测了，你就直接把mlp拉出来，在mnist上碰一碰。别说训练成本高，一个mnist能有多高？我且不说后边还有真的高成本的nlp等着你，cv任务里的玩具数据集都不敢打，就开始说自己把mlp给打败了，半场开香槟。在成为第二个mamba上，我挺看好这个工作的",
        "voteup_count": 26,
        "updated_time": "2024-05-04 18:41:33",
        "question_id": 654782153,
        "user_id": "a94d57689acb88ee94294bea541f2d31"
    },
    {
        "answer_id": 3486533764,
        "content": "短期看KAN还很难取代MLP，主要原因还是因为KAN的训练算力要求明显高于MLP，想做大模型会有很高的算力壁垒。更何况KAN可以退化为MLP，可以把KAN看成MLP的Spline版本。就技术而言，KAN=MLP+Spline。关于KAN的详细解读，可以看我们团队的长文：陈巍 博士：KAN架构最全解析—最热KAN能否干掉MLP？（收录于GPT-4/ChatGPT技术与产业分析）我们团队在2019年上半年发表的EDA algorithm PhD dissertation中就定义了类似KAN层的算符隐层（具备非线性表征能力），直接将非线性函数/算子思路引入MLP结构，使之成为更加通用，适配范围更大的基础架构。这一工作比2024年KAN的发表早了5年，并应用于电路特征建模。KAN的工作做的更加系统，完善了理论。这个工作的本质是借鉴了MLP及Kernel Function的思路。把原来的MLP实数空间扩展到“实数+算子”空间，将算子/核函数视为空间中的离散元素，将MLP中的乘法扩展为算子空间的元素。再参考泰勒展开的思路，可以使用非线性函数累加做任意实数函数的逼近。这也就解释了为什么这类方法可以很好的应用于数学计算。本质上，这类结构与KAN类似，退化后就是MLP结构，是建立在MLP思路基础上的。谈不上替代，其实是新一代的MLP。",
        "voteup_count": 9,
        "updated_time": "2024-05-04 15:17:29",
        "question_id": 654782153,
        "user_id": "455fe9c3762126aad1a7bf808902266a"
    },
    {
        "answer_id": 3485844154,
        "content": "原作灵感论文是比自注意力还要早的论文，但是现在才发掘出来，X上说为什么现在才拿出来，说因为是被west忽视。。高下立判，一眼炒作，坐等打脸",
        "voteup_count": 10,
        "updated_time": "2024-05-02 23:29:17",
        "question_id": 654782153,
        "user_id": "bf0ee92924277665b975f8b103126dcd"
    },
    {
        "answer_id": 3486502677,
        "content": "KAN网络架构概述KAN（Knowledge-Augmented Network）是一种新兴的网络架构，它将知识图谱嵌入到神经网络中，以提高模型的性能。KAN架构的基本思想是利用知识图谱中的知识来增强模型对世界的理解，从而更好地完成诸如自然语言处理、计算机视觉和推荐系统等任务。KAN架构主要包括以下几个组件：嵌入层： 将知识图谱中的实体和关系编码成向量表示。知识图谱推理模块： 利用嵌入层中的表示进行知识推理，提取新的知识事实。神经网络： 基于知识图谱推理得到的新知识事实，训练一个神经网络来完成特定任务。KAN架构的优势KAN架构具有以下几个优势：提高模型的泛化能力： 知识图谱中的知识可以为模型提供额外的信息，帮助模型更好地理解数据，从而提高模型的泛化能力。提高模型的鲁棒性： 知识图谱中的知识可以帮助模型抵抗噪声和异常数据，从而提高模型的鲁棒性。提高模型的可解释性： 基于知识图谱构建的模型更容易解释，因为模型的决策过程可以追溯到知识图谱中的知识。KAN架构的局限性KAN架构也存在一些局限性，主要体现在以下几个方面：对知识图谱的依赖性： KAN架构模型的性能很大程度上依赖于知识图谱的质量。如果知识图谱不完整或不准确，则可能会影响模型的性能。模型的复杂度： KAN架构模型通常比传统的模型更加复杂，需要更多的计算资源。数据的稀疏性： 知识图谱中的数据通常是稀疏的，这可能会导致模型难以学习到有效的表示。KAN架构是否能取代MLP？目前尚无定论。MLP（Multi-Layer Perceptron）是一种经典的深度学习模型，它具有简单易用、鲁棒性强等优点。在许多任务上，MLP模型仍然能够取得良好的性能。KAN架构是一种新兴的网络架构，它具有提高模型泛化能力、鲁棒性和可解释性等优势。然而，KAN架构也存在对知识图谱的依赖性高、模型复杂度高等局限性。因此，KAN架构是否能取代MLP取决于具体的任务和数据集。在某些情况下，KAN架构可能能够取得比MLP更好的性能。而在其他情况下，MLP可能仍然是更好的选择。未来展望KAN架构是一个很有潜力的研究方向，随着知识图谱的不断完善和深度学习技术的进步，KAN架构有望在未来得到更广泛的应用。以下是一些关于KAN架构的最新研究成果：2022年，来自清华大学的研究人员提出了一种新的KAN架构，称为KG-BERT。KG-BERT将知识图谱嵌入到BERT模型中，在自然语言处理任务上取得了https://arxiv.org/pdf/1909.03193**2023年，来自中国科学技术大学的研究人员提出了一种新的KAN架构，称为RGCN-GAT。RGCN-GAT将关系图卷积网络（RGCN）和图注意力网络（GAT）结合起来，在计算机视觉任务上取得了https://arxiv.org/pdf/2212.09719**随着研究的深入，KAN架构有望在更多领域取得突破性进展。",
        "voteup_count": 6,
        "updated_time": "2024-05-03 19:31:23",
        "question_id": 654782153,
        "user_id": "6c6d72e9f948dcf69f0034872c71f291"
    },
    {
        "answer_id": 3485793997,
        "content": "想要坐等后面会不会像Mamba又出现一堆“交叉”论文呢[酷]",
        "voteup_count": 6,
        "updated_time": "2024-05-02 22:25:32",
        "question_id": 654782153,
        "user_id": "1034c4ec9af5b1640bcf1c609663ca2b"
    },
    {
        "answer_id": 3487710516,
        "content": "量子位说能取代，那么一般是不能的。",
        "voteup_count": 4,
        "updated_time": "2024-05-05 06:30:43",
        "question_id": 654782153,
        "user_id": "8c981dcd3c165da63b3833f4e18d1d37"
    },
    {
        "answer_id": 3487097457,
        "content": "1900年，在国际数学家大会的巴黎演讲中，希尔伯特（David Hilbert）曾提出了23个对数学发展至关重要的问题。Kolmogorov-Arnold表示定理正与其中的希尔伯特第13问题密切相关。该定理指出，如果f是任意一个定义在有界域上的多变量连续函数，则该函数可以表示为有限数量的单变量、连续函数的两层嵌套叠加的形式。这表明真正的多元函数是一种求和，所有多元函数都可以通过对单变量函数求和的方式得到。从神经网络作函数拟合的角度看，这意味着对任一高维多变量函数的学习最终都可以被归约为对一组单变量函数的学习。受到Kolmogorov-Arnold表示定理的启发，研究成员期望参数化Kolmogorov-Arnold表示，将该想法显式嵌入神经网络的设计之中。他们提出KANs（Kolmogorov-Arnold Networks, KANs）作为传统多层感知机（Multi-layer perceptrons, MLPs）的有力竞争模型。相较于传统的MLPs在节点（神经元）上使用固定的激活函数，KANs在边（权重）上采用可学习的激活函数。基于Kolmogorov-Arnold表示定理的基本思想，KANs网络中创新性采用B样条曲线来参数化表示定理中的每个单变量函数。其中B样条是一种局部、分段的多项式曲线，其系数是可学习的。为了将网络推广到任意宽度和深度，而非局限于内、外部函数对应的两层非线性层和一个隐藏层，研究人员进一步提出了更一般的定理版本作为模型构建的理论指导。同时，受MLPs层叠设计的启发，通过堆叠带有可学习参数的一维函数矩阵构成的KAN层，拓展了KANs模型的深度，同时保持了网络较好的可解释性和表达能力。研究通过丰富的实验设计证明了KANs模型的优越性。结果显示，KANs相比传统的MLPs拥有更好的参数效率、更强的表达能力，独到的模型设计显著增强了实验结果的准确性，使注入领域知识和归纳偏置变得容易，加上符号公式的组合构建能力和交互性设计使可解释验证得以做到，由此进一步增强了模型的可解释性。值得一提的是，研究针对KANs在数据量和模型参数量层面的神经标度率（scaling law）进行了详尽的讨论。在五个玩具案例中，KANs比MLPs有着更快的标度变化速度。在求解偏微分方程任务中，KANs也展现出更快的收敛速度、达到更低的损失，并有着更陡峭的标度率表现。在拟合特殊函数任务中，KANs在所有特殊函数对比中展现出比MLPs更好的帕累托前沿（Pareto Frontier）。而且，借助样条设计的局部性天然优势，KANs可以在新数据上实现持续学习（continue learning），规避了机器学习中存在的灾难性遗忘（catastrophic forgetting）问题。此外，令人震惊的是，研究人员使用仅有200参数量的KANs就成功复现了Deepmind在《Nature》杂志上发表的基于30万参数量MLPs发现扭结理论（knot theory）数学定律的研究工作。并且KANs能发现与代数和几何扭结不变量相关的全新关系，同时给出一定的解释，而无需像MLPs一样进行特征归因的后期分析。在物理研究方面，KANs在凝聚态物理中的一种相变——Anderson局域化的问题上展现出了出色的相变边界发现能力，说明了该模型在物理研究领域也具备强大的应用潜力。这项研究为未来可解释模型设计开辟了新的思路，并为我们展示了KANs模型广阔的应用前景。有了KANs的支持，对于科学工作者而言，快速、准确地发现大量科学数据中新的规律和趋势将变得更加容易。其强大的符号公式组合构建能力、可视化功能和优秀的交互性设计也有助于揭示\"黑盒\"数据处理流程的真相，增加实验结果的可解释性。因此，KANs无疑将成为赋能Science研究的重要AI工具，并推动科学研究加速向AI+Science范式进行转变。更多的潜在研究价值和应用的可能性有待进一步探索和挖掘。",
        "voteup_count": 4,
        "updated_time": "2024-05-04 13:03:18",
        "question_id": 654782153,
        "user_id": "716c4a807ce4ec1c7e12085fb1a4134c"
    },
    {
        "answer_id": 3487581777,
        "content": "这东西要是最新发的论文，是新搞出来的东西还靠谱点。结果是多年前的东西，只是因为以前被「忽视」，现在忽然发现很好用，然后就觉得可以变天了，有种民间科学家说自己忽然发明了永动机一样不靠谱。从 AI 作为一门学科诞生，到今天这般光景。其实挺不容易的，在 OpenAI.推出 ChatGPT 之前，AI 的发展经历了很多低谷，甚至在 ChatGPT 刚火爆的那段时间，很多人依然觉得这又是一次泡沫而已。人们对于 AI 的探索，对于 AGI 的向往，对于实现AI 的各种投入，一次次的失败，数不尽的弯路，让人们对任何号称可以实现AI 的技术充满热情，但同时也会怀疑。当然，相比过去，计算机软硬件技术的发展，让算力这个东西发生了巨变。算力的匮乏可能曾经真的让很多理论完全被站在 AI 最前沿的探索者们给忽视了。这就好比用算盘搞不定的海量计算，对于科学计算器来说完全就是小菜一碟。工具有时候就是会带来巨变，让生产力突飞猛进。但是，KAN 相比 MLP 的优点过于简单了，无法相信在  AI 演化的道路上，人们会完全忽视这样一个存在。",
        "voteup_count": 0,
        "updated_time": "2024-05-04 23:27:06",
        "question_id": 654782153,
        "user_id": "8a8c30566f2d89eb38a77f040d6acb76"
    },
    {
        "answer_id": 3486428959,
        "content": "这个和脉冲神经网络思路很像啊。。。",
        "voteup_count": 1,
        "updated_time": "2024-05-03 17:42:05",
        "question_id": 654782153,
        "user_id": "1397ff67d49338d9a3f7d2eeb52dfe12"
    },
    {
        "answer_id": 3487048561,
        "content": "新网络架构Kolmogorov–Arnold Networks（KAN）的出现，为深度学习领域带来了新的活力。在探讨KAN是否会取代多层感知器（MLP）之前，我们需要从多个角度对其进行深入分析。1. KAN的理论基础与设计KAN的设计理念源于Kolmogorov-Arnold表示定理，该定理表明任何多变量连续函数都可以表示为单变量连续函数和二元加法运算的有限组合。KAN的核心创新在于，它将激活函数视为可以学习的一维函数，即通过样条（spline）参数化，而不是像传统MLP那样在节点上使用固定的激活函数。这种设计使得KAN在理论上具有更高的表达能力和可解释性。2. KAN的性能表现根据搜索结果，KAN在多个任务上展现了出色的性能。特别是在数据拟合、求解偏微分方程（PDE）以及持续学习任务中，KAN相较于MLP显示出了更高的准确性和参数效率。此外，KAN的局部可塑性特征有助于避免灾难性遗忘，这是传统MLP所面临的一个挑战。3. KAN的可解释性在AI领域，模型的可解释性是一个重要的考量因素。KAN由于其基于样条的激活函数，提供了一种更直观的方式来理解和解释模型的决策过程。这种可解释性对于提升用户对AI系统的信任以及促进AI技术的应用具有重要意义。4. KAN与MLP的比较MLP作为深度学习的基础架构之一，因其简单、易于实现和训练而被广泛采用。然而，MLP也存在一些局限性，如容易过拟合和对输入特征的缩放敏感等问题。KAN的出现，提供了一种可能的解决方案，尤其是在需要高准确度和可解释性的应用场景中。5. KAN的挑战与局限性尽管KAN在理论上和实验中展现出了诸多优势，但它在实际应用中还面临一些挑战。首先，KAN的训练速度可能比MLP慢，这可能会影响到它在大规模数据集上的应用。其次，KAN的实现和优化可能需要特定的硬件支持和算法调整。此外，KAN在不同类型的任务和数据集上的泛化能力还需要进一步验证。6. KAN的未来发展方向对于KAN未来的发展方向，研究者们可能会集中在以下几个方面：效率优化：研究如何提高KAN的训练效率，使其更适合实际应用。泛化能力：在更多类型的任务和数据集上测试KAN的性能，评估其泛化能力。硬件兼容性：开发与KAN架构兼容的硬件加速器，以充分发挥其性能优势。集成应用：探索将KAN集成到现有的深度学习框架中，如Transformer，以提升大型语言模型（LLM）的性能。7. 结论综上所述，KAN作为一种新型的神经网络架构，在理论和实验上都展现出了显著的优势，特别是在准确性、参数效率和可解释性方面。然而，要确定KAN是否能取代MLP，还需要更多的研究和实践来评估其在不同应用场景下的表现。同时，KAN的推广应用还需要解决训练效率、硬件兼容性等挑战。在未来，KAN有望成为深度学习领域的一个重要分支，与MLP等传统架构并存，共同推动AI技术的发展。注：由于KAN是一个新兴的研究领域，上述分析基于当前可用的信息和理解。随着研究的深入，对KAN的评估和认识可能会有所变化。",
        "voteup_count": 1,
        "updated_time": "2024-05-04 12:03:23",
        "question_id": 654782153,
        "user_id": "13ddb31ec3d8c1bbfd6f27a9b50810f5"
    },
    {
        "answer_id": 3485929788,
        "content": "Hidden layer 1 512，Hidden layer 2 256，K=3, batch size 256需要4g vram，这玩意完全就是工业垃圾。这个参数下MLP不可能超过百兆的显存占用，只能说大显存容许了垃圾代码。虽然我承认我自己写不出对等的构架，但是现在来看KAN还需要更多的商业炒作形成新的泡沫，不然无法和传统架构对比。",
        "voteup_count": 5,
        "updated_time": "2024-05-03 02:51:20",
        "question_id": 654782153,
        "user_id": "5fc58c3d85f7cfc049add7aa10cf6a85"
    },
    {
        "answer_id": 3487370129,
        "content": "KAN（Knowledge-Aware Network）是一种新型的网络架构，其设计理念是结合知识图谱与深度学习网络，以增强模型对知识的理解和表示能力。这种架构试图通过引入结构化的知识信息来提升模型处理复杂问题的能力，特别是在需要常识、领域知识或逻辑推理的任务中。MLP（多层感知机）是一种较为传统的神经网络结构，它由多层节点组成，每一层节点的输出都通过激活函数传递到下一层。MLP在多个领域都有广泛的应用，包括图像识别、自然语言处理等。KAN与MLP并不是相互替代的关系，而是可以看作是不同工具，适用于不同场景：1. **功能与应用场景**：KAN专注于利用先验知识和结构化信息来提升模型的表现，适用于需要大量背景知识和逻辑推理的任务。而MLP更通用，适用于多种多样的任务，特别是在有大量数据的情况下，MLP可以自动学习数据中的复杂模式。2. **复杂性与训练难度**：KAN可能需要复杂的预处理步骤来构建和整合知识图谱，这可能涉及到领域专家的知识。MLP则相对简单，更容易训练和部署。3. **数据需求**：MLP通常需要大量的标注数据来训练，而KAN可能可以通过结合少量的数据和丰富的知识图谱来提升模型的表现。总的来说，KAN和MLP各有优势，它们之间的关系更多是互补而非替代。在实际应用中，选择哪种架构取决于具体任务的性质、可用的数据资源、以及模型需要满足的特定需求。随着技术的不断发展，未来可能会出现更多结合了两者优点的新型网络架构。最后，蹲一个反转。",
        "voteup_count": 2,
        "updated_time": "2024-05-04 18:55:24",
        "question_id": 654782153,
        "user_id": "c180b131b226388e073973c2f33cd461"
    },
    {
        "answer_id": 3487181026,
        "content": "新网络架构Kolmogorov-Arnold Networks（KAN）的出现引起了深度学习领域的关注，它与传统的多层感知器（MLP）在设计和功能上有一些显著的不同。以下是对KAN架构的一些看法，以及它是否能取代MLP的分析：1. **设计原理**：KAN的设计灵感来源于Kolmogorov-Arnold表示定理，它允许在边（权重）上放置可学习的激活函数，而不是在节点（神经元）上放置固定激活函数，这与传统的MLP不同。2. **准确性和可解释性**：KAN在数据拟合和偏微分方程（PDE）求解中展现出了比MLP更高的准确性，并且由于其结构特点，KAN具备更好的可解释性。3. **参数效率**：KAN能够在参数数量较少的情况下达到较高的精确度，这在处理科学问题时尤其有用。4. **训练效率**：KAN的一个主要瓶颈是其训练速度较慢，通常比MLP慢10倍，这可能限制了它在需要快速迭代的场景下的应用。5. **硬件兼容性**：目前KAN对现有硬件（如GPU）的兼容性不如MLP，这可能影响其在大规模应用中的可行性。6. **适用场景**：KAN在数学和物理研究中显示出潜力，特别适合于需要高准确度和可解释性的领域。7. **未来发展**：尽管KAN在理论上具有吸引力，但它是否能取代MLP还需要更多的研究和开发，特别是在提升训练效率和硬件兼容性方面。8. **社区看法**：目前社区对于KAN的看法是多元的，一些研究者对其在特定领域的应用持乐观态度，而另一些则认为它在通用性和效率上可能面临挑战。综合来看，KAN在某些特定任务上展现出了潜力，但要取代广泛使用的MLP，还需要解决一些关键的技术挑战，并在实践中证明其广泛的适用性和效率。目前，KAN和MLP可能会根据不同的应用场景和需求并存使用。",
        "voteup_count": 1,
        "updated_time": "2024-05-04 14:53:02",
        "question_id": 654782153,
        "user_id": "baa0abfa1e511f957df540b484dd193e"
    },
    {
        "answer_id": 3485644480,
        "content": "根据现有的公开信息，我认为 KAN/Kolmogorov-Arnold Networks 处理实际问题时可能更复杂且效率可能更低，对现有 GPU 不友好、训练起来更慢而不稳定，其“可解释性”在工作内容更复杂时值得怀疑。看起来，KAN 取代不了 MLP。2024 年 5 月初，一些“中文顶刊”大肆吹嘘 KAN 在预印本论文里的表现。这大概又是一波来的易、去得快的炒作。https://doi.org/10.48550/arXiv.2404.19756",
        "voteup_count": 147,
        "updated_time": "2024-05-02 18:54:03",
        "question_id": 654782153,
        "user_id": "eb67c87bde5ed3b991f942e7d4b81838"
    },
    {
        "answer_id": 3485862467,
        "content": "我觉得这种结构早就有了。考古一下deep Kernel machine或者叫deep Gaussian process吧（现在做的人比较少了）AG Wilson, Z Hu, R Salakhutdinov, EP Xing， Deep  Kernel Learning,  AISTATS, 2016A Damianou, ND Lawrence , Deep Gaussian Processes,  AISTATS, 2013直接看deep GP的结构吧这种网络的收敛性分析也相对完善了， to name a fewMM Dunlop, MA Girolami, AM Stuart, AL Teckentrup, How deep are deep Gaussian processes?,  JMLR, 2018G Finocchio, J Schmidt-Hieber, Posterior contraction for deep Gaussian process priors  - JMLR, 2023他和Bayesian nerural net还有spline activation网络的等价性也已经有证明了（无法就是Kernel的傅里叶分解和小波分解。。。。）K Cutajar, EV Bonilla, P Michiardi, M Filippone,Random feature expansions for deep Gaussian processes  - ICML, 2017L Ding, R Tuo, S Shahrampour, A sparse expansion for deep Gaussian processes,  IISE Transactions, 2024结论是取代不了。deep GP也是很难无法train，解决training思路基本是稀疏化。但是如果对结构做稀疏化，就又会回到传统神经网络结构上去了。例如像这个KAN那样用spline做稀疏化，就会得出跟CNN在数学上基本等价的结构（小波、spline和convolution本来就是有极大关系）。那样的话，在各种大数据集上跑跟CNN的准确率也基本是相同的。即使做基于sparsity的网络结构优化，现在GPU上的硬件算法都不会考虑sparsity，所以即使做出来了实际效果也不会有加速，而且计算稳定性也比较麻烦，这是我踩过的坑╮(╯_╰)╭",
        "voteup_count": 114,
        "updated_time": "2024-05-03 02:39:45",
        "question_id": 654782153,
        "user_id": "a7921b463f824811404cbe3da3dbcd66"
    },
    {
        "answer_id": 3485804765,
        "content": "截止5月2号晚上11点，我去git了下来试了一下，感觉代码写的非常糟糕，急需优化。原生的代码我看作者还没检查过？指定device是cuda会出错，你跑进去看函数发现有些没有写指定device，导致一个cuda一个cpu。。。不知道作者想啥。好吧 ，我花了半个小时给他补上to device，跑起来了，但是gpu利用率只有40%，1000，100，100，10，k=3的网络，batch size256居然要4G显存，并且跑的比cpu还慢，所以我放弃了老老实实用回cpu。跑了大概15分钟，mnist能到85%，按上面的配置。所以，现在我不好评价。。。更新一下有人弄出了傅立叶版的kan https://github.com/GistNoesis/FourierKAN。 取代了那个又臭又长的B-spine，现在速度没问题了。我试了一下mnist能到97%，跟mlp差不多的样子？但是这个傅立叶版的问题还是有的，比如把grid调大，比如调到100，mnist就会严重不收敛，有待进一步验证。再度更新，截止5月3号晚上11点。由于原生kan写的太烂了，速度太慢，于是我采用了上面说能跑到97%的FourierKan做实验，尝试将这种kan替换transformer的mlp。我跑的实验是MAE pretrain，但是似乎怎么跑loss都比mlp高。KAN可以宣告完结了？？？",
        "voteup_count": 94,
        "updated_time": "2024-05-05 10:28:21",
        "question_id": 654782153,
        "user_id": "bcbe7c8deaa20f5cf15260c306857132"
    },
    {
        "answer_id": 3487357814,
        "content": "当你声称你的work能够击败xxx的时候，你要拿出来足够的，能够击败xxx的证据，而不是在一堆玩具数据集上测出来，哦，比xxx好，我打败了xxx，然后一通写作包装加一点噱头，又一个划时代的工作横空出世上一个这么样子的是mamba，声称自己能打败transformer，好笑的是iclr审稿就指出了实验不足，需要补足，小修小补，核心争议就是不碰，咱也不知道是自信还是心虚。过了几个月各种mamba全都出来了，都能凑一个international conference of mamba representations了，才发现时序时序不行，文本文本不行，视觉视觉不行，弄半天谁也打不过，吹这么大所以我的观点很直接，你kan你说能打败mlp，咱也别在这堆玩具上测了，你就直接把mlp拉出来，在mnist上碰一碰。别说训练成本高，一个mnist能有多高？我且不说后边还有真的高成本的nlp等着你，cv任务里的玩具数据集都不敢打，就开始说自己把mlp给打败了，半场开香槟。在成为第二个mamba上，我挺看好这个工作的",
        "voteup_count": 26,
        "updated_time": "2024-05-04 18:41:33",
        "question_id": 654782153,
        "user_id": "a94d57689acb88ee94294bea541f2d31"
    },
    {
        "answer_id": 3486533764,
        "content": "短期看KAN还很难取代MLP，主要原因还是因为KAN的训练算力要求明显高于MLP，想做大模型会有很高的算力壁垒。更何况KAN可以退化为MLP，可以把KAN看成MLP的Spline版本。就技术而言，KAN=MLP+Spline。关于KAN的详细解读，可以看我们团队的长文：陈巍 博士：KAN架构最全解析—最热KAN能否干掉MLP？（收录于GPT-4/ChatGPT技术与产业分析）我们团队在2019年上半年发表的EDA algorithm PhD dissertation中就定义了类似KAN层的算符隐层（具备非线性表征能力），直接将非线性函数/算子思路引入MLP结构，使之成为更加通用，适配范围更大的基础架构。这一工作比2024年KAN的发表早了5年，并应用于电路特征建模。KAN的工作做的更加系统，完善了理论。这个工作的本质是借鉴了MLP及Kernel Function的思路。把原来的MLP实数空间扩展到“实数+算子”空间，将算子/核函数视为空间中的离散元素，将MLP中的乘法扩展为算子空间的元素。再参考泰勒展开的思路，可以使用非线性函数累加做任意实数函数的逼近。这也就解释了为什么这类方法可以很好的应用于数学计算。本质上，这类结构与KAN类似，退化后就是MLP结构，是建立在MLP思路基础上的。谈不上替代，其实是新一代的MLP。",
        "voteup_count": 9,
        "updated_time": "2024-05-04 15:17:29",
        "question_id": 654782153,
        "user_id": "455fe9c3762126aad1a7bf808902266a"
    },
    {
        "answer_id": 3485844154,
        "content": "原作灵感论文是比自注意力还要早的论文，但是现在才发掘出来，X上说为什么现在才拿出来，说因为是被west忽视。。高下立判，一眼炒作，坐等打脸",
        "voteup_count": 10,
        "updated_time": "2024-05-02 23:29:17",
        "question_id": 654782153,
        "user_id": "bf0ee92924277665b975f8b103126dcd"
    },
    {
        "answer_id": 3486502677,
        "content": "KAN网络架构概述KAN（Knowledge-Augmented Network）是一种新兴的网络架构，它将知识图谱嵌入到神经网络中，以提高模型的性能。KAN架构的基本思想是利用知识图谱中的知识来增强模型对世界的理解，从而更好地完成诸如自然语言处理、计算机视觉和推荐系统等任务。KAN架构主要包括以下几个组件：嵌入层： 将知识图谱中的实体和关系编码成向量表示。知识图谱推理模块： 利用嵌入层中的表示进行知识推理，提取新的知识事实。神经网络： 基于知识图谱推理得到的新知识事实，训练一个神经网络来完成特定任务。KAN架构的优势KAN架构具有以下几个优势：提高模型的泛化能力： 知识图谱中的知识可以为模型提供额外的信息，帮助模型更好地理解数据，从而提高模型的泛化能力。提高模型的鲁棒性： 知识图谱中的知识可以帮助模型抵抗噪声和异常数据，从而提高模型的鲁棒性。提高模型的可解释性： 基于知识图谱构建的模型更容易解释，因为模型的决策过程可以追溯到知识图谱中的知识。KAN架构的局限性KAN架构也存在一些局限性，主要体现在以下几个方面：对知识图谱的依赖性： KAN架构模型的性能很大程度上依赖于知识图谱的质量。如果知识图谱不完整或不准确，则可能会影响模型的性能。模型的复杂度： KAN架构模型通常比传统的模型更加复杂，需要更多的计算资源。数据的稀疏性： 知识图谱中的数据通常是稀疏的，这可能会导致模型难以学习到有效的表示。KAN架构是否能取代MLP？目前尚无定论。MLP（Multi-Layer Perceptron）是一种经典的深度学习模型，它具有简单易用、鲁棒性强等优点。在许多任务上，MLP模型仍然能够取得良好的性能。KAN架构是一种新兴的网络架构，它具有提高模型泛化能力、鲁棒性和可解释性等优势。然而，KAN架构也存在对知识图谱的依赖性高、模型复杂度高等局限性。因此，KAN架构是否能取代MLP取决于具体的任务和数据集。在某些情况下，KAN架构可能能够取得比MLP更好的性能。而在其他情况下，MLP可能仍然是更好的选择。未来展望KAN架构是一个很有潜力的研究方向，随着知识图谱的不断完善和深度学习技术的进步，KAN架构有望在未来得到更广泛的应用。以下是一些关于KAN架构的最新研究成果：2022年，来自清华大学的研究人员提出了一种新的KAN架构，称为KG-BERT。KG-BERT将知识图谱嵌入到BERT模型中，在自然语言处理任务上取得了https://arxiv.org/pdf/1909.03193**2023年，来自中国科学技术大学的研究人员提出了一种新的KAN架构，称为RGCN-GAT。RGCN-GAT将关系图卷积网络（RGCN）和图注意力网络（GAT）结合起来，在计算机视觉任务上取得了https://arxiv.org/pdf/2212.09719**随着研究的深入，KAN架构有望在更多领域取得突破性进展。",
        "voteup_count": 6,
        "updated_time": "2024-05-03 19:31:23",
        "question_id": 654782153,
        "user_id": "6c6d72e9f948dcf69f0034872c71f291"
    },
    {
        "answer_id": 3485793997,
        "content": "想要坐等后面会不会像Mamba又出现一堆“交叉”论文呢[酷]",
        "voteup_count": 6,
        "updated_time": "2024-05-02 22:25:32",
        "question_id": 654782153,
        "user_id": "1034c4ec9af5b1640bcf1c609663ca2b"
    },
    {
        "answer_id": 3487710516,
        "content": "量子位说能取代，那么一般是不能的。",
        "voteup_count": 4,
        "updated_time": "2024-05-05 06:30:43",
        "question_id": 654782153,
        "user_id": "8c981dcd3c165da63b3833f4e18d1d37"
    },
    {
        "answer_id": 3487097457,
        "content": "1900年，在国际数学家大会的巴黎演讲中，希尔伯特（David Hilbert）曾提出了23个对数学发展至关重要的问题。Kolmogorov-Arnold表示定理正与其中的希尔伯特第13问题密切相关。该定理指出，如果f是任意一个定义在有界域上的多变量连续函数，则该函数可以表示为有限数量的单变量、连续函数的两层嵌套叠加的形式。这表明真正的多元函数是一种求和，所有多元函数都可以通过对单变量函数求和的方式得到。从神经网络作函数拟合的角度看，这意味着对任一高维多变量函数的学习最终都可以被归约为对一组单变量函数的学习。受到Kolmogorov-Arnold表示定理的启发，研究成员期望参数化Kolmogorov-Arnold表示，将该想法显式嵌入神经网络的设计之中。他们提出KANs（Kolmogorov-Arnold Networks, KANs）作为传统多层感知机（Multi-layer perceptrons, MLPs）的有力竞争模型。相较于传统的MLPs在节点（神经元）上使用固定的激活函数，KANs在边（权重）上采用可学习的激活函数。基于Kolmogorov-Arnold表示定理的基本思想，KANs网络中创新性采用B样条曲线来参数化表示定理中的每个单变量函数。其中B样条是一种局部、分段的多项式曲线，其系数是可学习的。为了将网络推广到任意宽度和深度，而非局限于内、外部函数对应的两层非线性层和一个隐藏层，研究人员进一步提出了更一般的定理版本作为模型构建的理论指导。同时，受MLPs层叠设计的启发，通过堆叠带有可学习参数的一维函数矩阵构成的KAN层，拓展了KANs模型的深度，同时保持了网络较好的可解释性和表达能力。研究通过丰富的实验设计证明了KANs模型的优越性。结果显示，KANs相比传统的MLPs拥有更好的参数效率、更强的表达能力，独到的模型设计显著增强了实验结果的准确性，使注入领域知识和归纳偏置变得容易，加上符号公式的组合构建能力和交互性设计使可解释验证得以做到，由此进一步增强了模型的可解释性。值得一提的是，研究针对KANs在数据量和模型参数量层面的神经标度率（scaling law）进行了详尽的讨论。在五个玩具案例中，KANs比MLPs有着更快的标度变化速度。在求解偏微分方程任务中，KANs也展现出更快的收敛速度、达到更低的损失，并有着更陡峭的标度率表现。在拟合特殊函数任务中，KANs在所有特殊函数对比中展现出比MLPs更好的帕累托前沿（Pareto Frontier）。而且，借助样条设计的局部性天然优势，KANs可以在新数据上实现持续学习（continue learning），规避了机器学习中存在的灾难性遗忘（catastrophic forgetting）问题。此外，令人震惊的是，研究人员使用仅有200参数量的KANs就成功复现了Deepmind在《Nature》杂志上发表的基于30万参数量MLPs发现扭结理论（knot theory）数学定律的研究工作。并且KANs能发现与代数和几何扭结不变量相关的全新关系，同时给出一定的解释，而无需像MLPs一样进行特征归因的后期分析。在物理研究方面，KANs在凝聚态物理中的一种相变——Anderson局域化的问题上展现出了出色的相变边界发现能力，说明了该模型在物理研究领域也具备强大的应用潜力。这项研究为未来可解释模型设计开辟了新的思路，并为我们展示了KANs模型广阔的应用前景。有了KANs的支持，对于科学工作者而言，快速、准确地发现大量科学数据中新的规律和趋势将变得更加容易。其强大的符号公式组合构建能力、可视化功能和优秀的交互性设计也有助于揭示\"黑盒\"数据处理流程的真相，增加实验结果的可解释性。因此，KANs无疑将成为赋能Science研究的重要AI工具，并推动科学研究加速向AI+Science范式进行转变。更多的潜在研究价值和应用的可能性有待进一步探索和挖掘。",
        "voteup_count": 4,
        "updated_time": "2024-05-04 13:03:18",
        "question_id": 654782153,
        "user_id": "716c4a807ce4ec1c7e12085fb1a4134c"
    },
    {
        "answer_id": 3487581777,
        "content": "这东西要是最新发的论文，是新搞出来的东西还靠谱点。结果是多年前的东西，只是因为以前被「忽视」，现在忽然发现很好用，然后就觉得可以变天了，有种民间科学家说自己忽然发明了永动机一样不靠谱。从 AI 作为一门学科诞生，到今天这般光景。其实挺不容易的，在 OpenAI.推出 ChatGPT 之前，AI 的发展经历了很多低谷，甚至在 ChatGPT 刚火爆的那段时间，很多人依然觉得这又是一次泡沫而已。人们对于 AI 的探索，对于 AGI 的向往，对于实现AI 的各种投入，一次次的失败，数不尽的弯路，让人们对任何号称可以实现AI 的技术充满热情，但同时也会怀疑。当然，相比过去，计算机软硬件技术的发展，让算力这个东西发生了巨变。算力的匮乏可能曾经真的让很多理论完全被站在 AI 最前沿的探索者们给忽视了。这就好比用算盘搞不定的海量计算，对于科学计算器来说完全就是小菜一碟。工具有时候就是会带来巨变，让生产力突飞猛进。但是，KAN 相比 MLP 的优点过于简单了，无法相信在  AI 演化的道路上，人们会完全忽视这样一个存在。",
        "voteup_count": 0,
        "updated_time": "2024-05-04 23:27:06",
        "question_id": 654782153,
        "user_id": "8a8c30566f2d89eb38a77f040d6acb76"
    },
    {
        "answer_id": 3486428959,
        "content": "这个和脉冲神经网络思路很像啊。。。",
        "voteup_count": 1,
        "updated_time": "2024-05-03 17:42:05",
        "question_id": 654782153,
        "user_id": "1397ff67d49338d9a3f7d2eeb52dfe12"
    },
    {
        "answer_id": 3487048561,
        "content": "新网络架构Kolmogorov–Arnold Networks（KAN）的出现，为深度学习领域带来了新的活力。在探讨KAN是否会取代多层感知器（MLP）之前，我们需要从多个角度对其进行深入分析。1. KAN的理论基础与设计KAN的设计理念源于Kolmogorov-Arnold表示定理，该定理表明任何多变量连续函数都可以表示为单变量连续函数和二元加法运算的有限组合。KAN的核心创新在于，它将激活函数视为可以学习的一维函数，即通过样条（spline）参数化，而不是像传统MLP那样在节点上使用固定的激活函数。这种设计使得KAN在理论上具有更高的表达能力和可解释性。2. KAN的性能表现根据搜索结果，KAN在多个任务上展现了出色的性能。特别是在数据拟合、求解偏微分方程（PDE）以及持续学习任务中，KAN相较于MLP显示出了更高的准确性和参数效率。此外，KAN的局部可塑性特征有助于避免灾难性遗忘，这是传统MLP所面临的一个挑战。3. KAN的可解释性在AI领域，模型的可解释性是一个重要的考量因素。KAN由于其基于样条的激活函数，提供了一种更直观的方式来理解和解释模型的决策过程。这种可解释性对于提升用户对AI系统的信任以及促进AI技术的应用具有重要意义。4. KAN与MLP的比较MLP作为深度学习的基础架构之一，因其简单、易于实现和训练而被广泛采用。然而，MLP也存在一些局限性，如容易过拟合和对输入特征的缩放敏感等问题。KAN的出现，提供了一种可能的解决方案，尤其是在需要高准确度和可解释性的应用场景中。5. KAN的挑战与局限性尽管KAN在理论上和实验中展现出了诸多优势，但它在实际应用中还面临一些挑战。首先，KAN的训练速度可能比MLP慢，这可能会影响到它在大规模数据集上的应用。其次，KAN的实现和优化可能需要特定的硬件支持和算法调整。此外，KAN在不同类型的任务和数据集上的泛化能力还需要进一步验证。6. KAN的未来发展方向对于KAN未来的发展方向，研究者们可能会集中在以下几个方面：效率优化：研究如何提高KAN的训练效率，使其更适合实际应用。泛化能力：在更多类型的任务和数据集上测试KAN的性能，评估其泛化能力。硬件兼容性：开发与KAN架构兼容的硬件加速器，以充分发挥其性能优势。集成应用：探索将KAN集成到现有的深度学习框架中，如Transformer，以提升大型语言模型（LLM）的性能。7. 结论综上所述，KAN作为一种新型的神经网络架构，在理论和实验上都展现出了显著的优势，特别是在准确性、参数效率和可解释性方面。然而，要确定KAN是否能取代MLP，还需要更多的研究和实践来评估其在不同应用场景下的表现。同时，KAN的推广应用还需要解决训练效率、硬件兼容性等挑战。在未来，KAN有望成为深度学习领域的一个重要分支，与MLP等传统架构并存，共同推动AI技术的发展。注：由于KAN是一个新兴的研究领域，上述分析基于当前可用的信息和理解。随着研究的深入，对KAN的评估和认识可能会有所变化。",
        "voteup_count": 1,
        "updated_time": "2024-05-04 12:03:23",
        "question_id": 654782153,
        "user_id": "13ddb31ec3d8c1bbfd6f27a9b50810f5"
    },
    {
        "answer_id": 3485929788,
        "content": "Hidden layer 1 512，Hidden layer 2 256，K=3, batch size 256需要4g vram，这玩意完全就是工业垃圾。这个参数下MLP不可能超过百兆的显存占用，只能说大显存容许了垃圾代码。虽然我承认我自己写不出对等的构架，但是现在来看KAN还需要更多的商业炒作形成新的泡沫，不然无法和传统架构对比。",
        "voteup_count": 5,
        "updated_time": "2024-05-03 02:51:20",
        "question_id": 654782153,
        "user_id": "5fc58c3d85f7cfc049add7aa10cf6a85"
    },
    {
        "answer_id": 3487370129,
        "content": "KAN（Knowledge-Aware Network）是一种新型的网络架构，其设计理念是结合知识图谱与深度学习网络，以增强模型对知识的理解和表示能力。这种架构试图通过引入结构化的知识信息来提升模型处理复杂问题的能力，特别是在需要常识、领域知识或逻辑推理的任务中。MLP（多层感知机）是一种较为传统的神经网络结构，它由多层节点组成，每一层节点的输出都通过激活函数传递到下一层。MLP在多个领域都有广泛的应用，包括图像识别、自然语言处理等。KAN与MLP并不是相互替代的关系，而是可以看作是不同工具，适用于不同场景：1. **功能与应用场景**：KAN专注于利用先验知识和结构化信息来提升模型的表现，适用于需要大量背景知识和逻辑推理的任务。而MLP更通用，适用于多种多样的任务，特别是在有大量数据的情况下，MLP可以自动学习数据中的复杂模式。2. **复杂性与训练难度**：KAN可能需要复杂的预处理步骤来构建和整合知识图谱，这可能涉及到领域专家的知识。MLP则相对简单，更容易训练和部署。3. **数据需求**：MLP通常需要大量的标注数据来训练，而KAN可能可以通过结合少量的数据和丰富的知识图谱来提升模型的表现。总的来说，KAN和MLP各有优势，它们之间的关系更多是互补而非替代。在实际应用中，选择哪种架构取决于具体任务的性质、可用的数据资源、以及模型需要满足的特定需求。随着技术的不断发展，未来可能会出现更多结合了两者优点的新型网络架构。最后，蹲一个反转。",
        "voteup_count": 2,
        "updated_time": "2024-05-04 18:55:24",
        "question_id": 654782153,
        "user_id": "c180b131b226388e073973c2f33cd461"
    },
    {
        "answer_id": 3487181026,
        "content": "新网络架构Kolmogorov-Arnold Networks（KAN）的出现引起了深度学习领域的关注，它与传统的多层感知器（MLP）在设计和功能上有一些显著的不同。以下是对KAN架构的一些看法，以及它是否能取代MLP的分析：1. **设计原理**：KAN的设计灵感来源于Kolmogorov-Arnold表示定理，它允许在边（权重）上放置可学习的激活函数，而不是在节点（神经元）上放置固定激活函数，这与传统的MLP不同。2. **准确性和可解释性**：KAN在数据拟合和偏微分方程（PDE）求解中展现出了比MLP更高的准确性，并且由于其结构特点，KAN具备更好的可解释性。3. **参数效率**：KAN能够在参数数量较少的情况下达到较高的精确度，这在处理科学问题时尤其有用。4. **训练效率**：KAN的一个主要瓶颈是其训练速度较慢，通常比MLP慢10倍，这可能限制了它在需要快速迭代的场景下的应用。5. **硬件兼容性**：目前KAN对现有硬件（如GPU）的兼容性不如MLP，这可能影响其在大规模应用中的可行性。6. **适用场景**：KAN在数学和物理研究中显示出潜力，特别适合于需要高准确度和可解释性的领域。7. **未来发展**：尽管KAN在理论上具有吸引力，但它是否能取代MLP还需要更多的研究和开发，特别是在提升训练效率和硬件兼容性方面。8. **社区看法**：目前社区对于KAN的看法是多元的，一些研究者对其在特定领域的应用持乐观态度，而另一些则认为它在通用性和效率上可能面临挑战。综合来看，KAN在某些特定任务上展现出了潜力，但要取代广泛使用的MLP，还需要解决一些关键的技术挑战，并在实践中证明其广泛的适用性和效率。目前，KAN和MLP可能会根据不同的应用场景和需求并存使用。",
        "voteup_count": 1,
        "updated_time": "2024-05-04 14:53:02",
        "question_id": 654782153,
        "user_id": "baa0abfa1e511f957df540b484dd193e"
    }
]