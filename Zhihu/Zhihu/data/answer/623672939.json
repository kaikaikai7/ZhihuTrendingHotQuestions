[
    {
        "answer_id": 3482263122,
        "content": "开源模型的发展实在是太快了，Gemma，Mixtral，LLama3。。。“最强开源模型一夜易主”这样的新闻标题，几个月换换主角就发一遍，都快用烂了。不同的基座模型加上徒子徒孙们，数是数不完的，所以还是按着谱系缕会更清晰一些。文中提到的一些宝藏合集、工具和资源链接我会附在文章末尾，Here we go。开源大模型谱系常见底座模型概览：转自：Awesome Chinese LLM不同的底座模型各有特色和优势。比如，ChatGLM作为一款可商用的模型，其参数大小和训练token数都非常可观，适合于生成式对话等应用场景（尤其擅长中文）。LLaMA以其多样的参数大小和训练最大长度，提供了更多的选择空间，但是在商用方面是有一定限制的。而DeepSeek则在参数大小上提供了更多的选择，可以适应不同的应用需求。这些开源基座衍生出无数分支与变体，共同构筑了一个生机勃勃、枝繁叶茂的开源项目森林。转自：Awesome Chinese LLM常见的开源大模型介绍Llama 3基于超过15万亿token的数据集进行训练，是Llama2数据集的7倍还多，为模型提供了更丰富的信息基础。 支持8K长文本，配备了改进的tokenizer，词汇量高达128K，性能表现优异。缺点是会有飙英文的现象，建议试试开源社区里的中文微调版本。ChatGLM3智谱AI和清华大学 KEG 实验室联合发布的新一代对话预训练模型，国产之光。ChatGLM3在保留了前两代模型对话流畅、部署门槛低等众多优秀特性的基础上，采用了更多样的训练数据、更充分的训练步数和更合理的训练策略。ChatGLM3系列原生支持工具调用、代码执行和Agent任务等复杂场景，为用户提供了更为全面和高效的功能支持。Qwen通义千问阿里云研发的通义千问大模型系列模型，包括参数规模为18亿（1.8B）、70亿（7B）、140亿（14B）和720亿（72B）。拥有广泛的知识领域和强大的分析能力，可应用于智能搜索、问答系统等领域，为用户提供便捷、高效的智能交互体验，能有效调用插件以及升级为Agent。Mistral 7B由Mistral AI开发的70亿参数LLM。它在所有基准测试中都优于Llama 2 13B，在许多基准测试中超过了Llama 1 34B，并使用滑动窗口注意力（SWA）来优化模型的注意力过程，显著提高了速度。不过国外的都有个共同的伤，更擅长英语任务。Yi 模型李开复团队零一万物发布的开源预训练大模型。能够一次处理40万汉字，曾成功登顶Hugging Face开源模型榜首的中文模型。中英均表现优异，具有强大的语言理解和生成能力，适用于多种自然语言处理任务。在本地搭建部署开源模型从个人学习的角度来说，其实还是有点门槛的，硬件资源就是一关。建议从参数规模较小的7B左右开源模型开启尝试。比如清华的GLM系列，meta的llama系列都可以。推荐使用Ollama，Ollama是在Docker容器中部署和管理大型语言模型（LLM）的工具，可以大大简化部署过程。安装好ollama后，通过命令就可以直接下载大模型。Ollama官网有列出它支持的模型库，可以看到连Llama3都包括了，还是很香的：不过Ollama是命令行交互，不太方便，可以通过安装一个OpenWebUI来在浏览器使用。如果嫌自己摸索太麻烦，正好最近有一个知乎知学堂联合AGI课堂推出的程序员的AI大模型进阶之旅免费公开课。这个课正是为了适应当下AI大模型的发展而推出的，由几位业内大佬主讲，主要面向的就是想系统性学习AI大模型的同学。会体系化地介绍开源、闭源大模型的技术路径，以及在此基础之上的应用开发技术栈。像如何fine-tune模型、Langchain技术等等都会有全面涉及。重要的是有 AI 大模型学习资源包，以及好用的 AI工具等。感兴趣的可以关注一下 ↓ ↓ ↓线上体验ShirtAI如果想先从线上进行体验，可以试下ShirtAI。ShirtAI是一款集成全球大模型全方位AI产品，问答、绘画、PDF对话解析、AI TTS语音等功能都有了，非常省心。主流的开源、闭源模型都可以选择。网页版：https://www.myshirtai.com/IOS版本：https://apps.apple.com/us/app/shirtai/id6474819973huggingface chat有科学上网工具的，可以到huggingface 的体验页面直接体验，支持很多开源模型：https://huggingface.co/chat/如何选择开源模型选择开源 LLM ，一定要从实际情况出发。任务目标：每种模型都有自己的优势和短板，在模型选择时，一定要结合自己的任务需求，寻找适合的模型。成本：开源模型看似免费，但是需要考虑计算资源、存储资源、能源消耗、技术储备、后期维护等一系列成本。毋庸置疑的是，LMM 规模越大、越复杂，成本越高。精度：可以通过对模型在类似任务上的性能基准进行考察，比较不同的 LLMs 执行你所需的任务类型的准确程度。数据安全：尤其在商业领域，数据安全是重要的考虑因素。 可以从隐私保护、合规等方面进行综合考虑。社区和支持：一个活跃的开源社区可以提供技术支持、更新和改进，良完善的文档和教程可以帮助你更快地掌握模型的使用。参考资料：非常宝藏的中文LLM大合集：https://www.github-zh.com/projects/643916827-awesome-chinese-llmHuggingFace的开源大模型排行榜Open LLM Leaderboard - a Hugging Face Space by HuggingFaceH4集成多个开源模型的聊天页面Huggingchat：https://huggingface.co/chat/网页体验Llama3：Chat with Meta Llama 3 on Replicate (llama2.ai)Ollama：GitHub - ollama/ollama: Get up and running with Llama 3, Mistral, Gemma, and other large language models.OpenWebUI：GitHub - open-webui/open-webui: User-friendly WebUI for LLMs (Formerly Ollama WebUI)",
        "voteup_count": 85,
        "updated_time": "2024-04-29 09:42:40",
        "question_id": 623672939,
        "user_id": "6736256b9f1cc119e59101b6562cc6c5"
    },
    {
        "answer_id": 3439756668,
        "content": "GPT-4的爆火，彻底掀起了学术界对于多模态大模型的研究热潮；而开源大模型更是霸占了模型界的半边天；看基本上大家介绍得都七七八八了，那我就就着最近几个热度比较高的开源模型介绍一下吧；① Gemma北京时间2月21日晚21点，美国科技巨头谷歌宣布推出全球性能最强大、轻量级的开源模型系列Gemma；分为2B（20亿参数）和7B（70亿）两种尺寸版本，2B版本甚至可直接在笔记本电脑上运行，7B性能远远超越Llama 2 13B！Google DeepMind CEO Demis Hassabis表示，轻量开源的Gemma是同类尺寸中性能最佳的模型！如今，谷歌不仅将对手瞄向OpenAI，而且打算独占鳌头，新模型将Llama-2碾在脚底，遥遥领先。② Grok-13月18日，马斯克旗下AI初创企业xAI今天发布新闻稿，宣布正在开源3140亿参数的混合专家模型 Grok-1，该模型遵循 Apache 2.0 协议开放模型权重和架构，号称是“迄今为止全球参数量最大的开源大语言模型”。Grok-1是一个混合专家（Mixture-of-Experts，MOE）大模型，这种MOE架构重点在于提高大模型的训练和推理效率，这个技术将大模型划分为不同的子专家模型，每次推理只选择部分专家网络进行推理，在降低成本的同时保证模型的效果；Grok-1包含了8个专家，总参数量高达314B（3140亿），处理Token时，其中的两个专家会被激活，激活参数量为86B；并且在马斯克数次嘲讽Open AI是“Close AI”之后，xAI现已将Grok-1的权重和架构在GitHub上开源，似乎有股想要“身体力行”敦促Open AI恢复开源的意味了~大部分模型在多种任务上表现优秀，但是仍需要大量的计算资源和专业知识来训练和优化，对于普通人来说，其实许多现成的落地产品，就已经足够解决日常需求；✨ 智能表格处理：Sheet+Sheet+是一个AI驱动的Excel和Google Sheets工具，可以从文本生成Google Sheets和Excel公式，将公式转换为简单的解释，调试公式等，帮助我们节省时间，简化电子表格工作；Sheet+是一个创新的AI-powered工具，它可以将Google表格和Excel公式编写速度提高10倍，抛弃繁琐的公式编写，将公式转换为简单的解释、调试公式等~Sheet+最大的特点就是它的易用性，它与Excel和Google Sheets兼容，使我们可以在任何自己喜欢的平台上使用，即使你不是Excel或Google Sheets的专家，你也仍然可以使用Sheet+轻松创建复杂的公式~✨ 灵感创意生成：AI创意生成家“创意生成家”大概就是所有使用过这个工具的人对它的印象了！“创造力”绝对是这款软件的最大亮点！集合了AI写作、AI绘画、AI聊天等AI功能于一体，通过AI让我们可以将更多的精力投入到创造性的任务中，释放出更多的创意和想象力！软件AI写作功能，能够支持到工作总结、培训方案、岗位事迹、论文开题报告、商业计划书、活动策划、视频脚本等等文章内容的方向，都属于非常实用的写作类型！而且其它功能表现也不俗！譬如AI绘画，功能能够支持“图生图”、“文生图”两大主流AI绘画模式，并且简洁干净的UI界面，也让许多新手小白表示能够快速上手；各种类型的模板对于一些毫无基础的用户非常友好，完美地契合这些用户的眼光和需求，只需要选择到钟意的风格，简单输入自己的需求，调整设置后即可生成输出！当然，软件中的其它各种AI配音、AI特效、AI转换等等功能，输出创意也都非常不错！✨ 视频智能合成：Runway Gen-2曾经AI视频界的王者，仅突破几秒内的连贯性就搏得业内欢呼，而如今却被Sora的光芒完全掩盖，但大鹏依然觉得值得推荐！Gen-2是一个多模态的人工智能系统，可以生成带有文本、图像或视频剪辑的新颖视频，8种视频模式自由选择，只需要打字或上传图像就能生成逼真的合成视频~Gen-2的操作方法非常简单，在屏幕中间点击上传一段视频，完成上传后，左侧可以预览视频，右上方可以选择各种不同风格，下方可以调整参数，自定义程度非常高~大模型开源与闭源之争是人工智能领域的一个热门话题，开源和闭源各有优缺点，应根据不同的应用场景选择合适的模式；未来，即便发展方向再怎么变，开源成了主流，但闭源也永远不会沉没。又是干货满满的分享，我看谁还没点赞收藏喜欢，有什么意见也可以在评论区直说，@视频编辑助手绝对欢迎！",
        "voteup_count": 11,
        "updated_time": "2024-03-22 16:19:59",
        "question_id": 623672939,
        "user_id": "63417af9dec64a7241e1465f2ecadf08"
    },
    {
        "answer_id": 3225832217,
        "content": "据我所知，目前广泛可用性的开源模型是Llama-2-70B，可以根据提示词，写出不错的创意性文章，影评等主观性比较强的文本，内容丰富，情绪饱满，强于ChatGPT 3.5，ChatGPT 3.5的英语版本的创意表现，也比不过Llama270。但问答逊于ChatGPT 3.5，有时候，会比ChatGPT更好，稳定性不如ChatGPT。Llama-13B也不错。你可以用ChatGPT（问答)+Llama 270（创意），搭配出一个不错的免费体验。其他的开源模型，都在爬各种测试榜，我感觉实际可用性不大。测试榜有大的水分，针对性优化，有极强的迷惑性。大模型，并不是容易的事。我认为中国还没有掌握大模型的核心技巧，虽然他们声明自己可以在榜单上做得有多好。",
        "voteup_count": 7,
        "updated_time": "2023-09-25 14:27:52",
        "question_id": 623672939,
        "user_id": "bea84fe95aee12a6939ee59dde16d1b3"
    },
    {
        "answer_id": 3239611227,
        "content": "现在开源大模型挺多，感觉新闻一会一个，容易花多眼乱。不过，其实只要捋一捋，关系就清晰了。怎么捋呢？关键在于谱系。开源模型虽然多，但大多数都是从Meta的Llama派生出来的。简单来说，就是用了Llama作为基模型，然后选用其它不同的训练方法微调。所以，记住Llama，就记住了大多数开源大模型。比如说Alpaca。Alpaca同样也是一款具有里程碑意义的开源大模型，听说的人很多，但这款模型其实也是Llama的派生模型，也就是说，其底层结构和Llama是完全一致的，不同之处在于微调。Alpaca使用Llama作为基模型，然后使用了自建的数据集进行有监督学习。自建数据集的过程比较有意思，研究人员首先人工编写了175条提示，输入openai的text-davinci-003模型（因为ChatGPT当时还没提供API），然后得到52K条instruction-following的样本，形成数据集。最后用来给Llama-7B做有监督学习微调，得到的新模型就是Alpaca。类似的做法很多，另一个很有名的比如Vicuna，用的是http://ShareGPT.com的数据。后来又出现了一些中文Llama，复杂一点，因为Llama原生不怎么支持中文，具体来说就是词表只有少量中文词元，需要对词表做二次处理。但模型结构也是一致的。Llama还在不断迭代，现在已经出到2了，meta还推出了Code Llama，不过底层模型还是Llama，在代码生成上做了优化。国外的开源大模型知名的还有Falcon，有一段时间说很厉害，远超Llama，后来又说是指标计算问题，还是Llama强一点。国内也有一些自研的开源大模型，不是中文Llama而是底层结构也自己设计自己训练，一个字，有钱。名气大的有三个，阿里通义千问的开源版Qwen模型，我用过觉得不错的ChatGLM，现在出了2，还有王小川的Baichuan，现在也出了2。不过，从名气从能力从生态来说，Llama都是开源大模型的扛把子。总之，记住Llama，就记住了大多数开源大模型。上面说的是开源大语言模型，也就是LLM。开源AI模型就更多了，比如说AI绘画的SD，还有一些搞多模态的。",
        "voteup_count": 17,
        "updated_time": "2023-10-07 10:29:38",
        "question_id": 623672939,
        "user_id": "92c8c683498d310e028124afff90fe6b"
    },
    {
        "answer_id": 3286312505,
        "content": "本文是开源 LLM 发展史系列文章的第三部分。此前，第一部分《开源语言大模型演进史：早期革新》回顾了创建开源 LLM 的最初尝试。第二部分《开源语言大模型演进史：高质量基础模型竞赛》研究了目前可用的最受欢迎的开源基础模型（即已进行预训练但尚未微调或对齐的语言模型）。本文将介绍如何通过微调/对齐那些更出色的LLaMA-2等开源模型来提升它们的效果，并缩小开源和私有LLM之间的差距。​（本文作者为Rebuy公司AI总监、深度学习博士Cameron R. Wolfe。以下内容经授权后由OneFlow编译发布，转载请联系授权。原文：https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-imitation）作者 | Cameron R. WolfeOneFlow编译翻译｜杨婷、宛子琳（引自[1,2,12]）在开源语言大模型之前的研究中，大部分关注点都集中在创建预训练基础模型上。然而，由于这些模型没有经过微调，缺乏对齐，它们的质量无法匹敌顶级的闭源 LLM (如 ChatGPT 或 Claude )。付费模型通常使用 SFT（监督微调）和 RLHF（人类反馈强化学习）等技术进行了全面对齐，这极大地提高了模型的可用性。相比之下，开源模型通常只使用较小的公开数据集进行小规模微调。本文将着眼于最近的研究，通过更广泛的微调和对齐来提高开源 LLM 的质量。（引自[17,18]）对齐过程。本文将研究开源 LLM 的微调和对齐过程。在进入正题之前，我们需要了解什么是对齐以及如何对齐。语言模型的训练通常可分为几部分，如上图所示，首先我们需要对模型进行预训练，然后进行多个步骤的微调。预训练之后，LLM 能够准确预测下一个词元，但其输出的内容可能会比较重复和单调。因此，需要对模型进行微调以改善其对齐能力，让模型生成与人类用户期望相一致的文本（如遵循指令、避免有害输出、防止虚假陈述，生成有趣或富有创造性的输出等）。（引自[17]）监督微调（SFT）。对齐可通过两种微调技术实现：监督微调和从人类反馈中进行强化学习。可参考上图获取更多详细信息。SFT 对模型进行简单微调，使用标准的语言建模目标，在高质量的提示和响应示例上进行训练。LLM 可以从这些示例中学习到如何进行适当的回应！SFT 非常简单有效，但需要精心策划一个能够捕捉到“正确”行为的数据集。RLHF。直接使用来自人类标注者的反馈对 LLM 进行训练，人类标注者会识别出他们喜欢的输出，然后 LLM 再学习如何生成更多类似的输出。为实现这一目标，我们首先需要一组提示，并在每个提示上由 LLM 生成多个不同的输出。之后，让人类标注者根据输出的质量对每个生成的回应进行评分。这些评分可用来训练奖励模型（即带有额外回归头的微调版本的 LLM ），用于预测每个回应的分数。接着，RLHF 通过 PPO 强化学习算法对模型进行微调，将得分最大化。通常，高质量的 LLM 会通过 SFT 和 RLHF（使用大量人类反馈）按顺序进行对齐。1 模仿学习（引自[16]）在 LLaMA [3]发布后，开源研究社区终于能够使用强大的基础 LLM 进行各种不同应用的微调或对齐，这使得开源 LLM 的研究经历了爆发式增长，从业者们争相在自己选择的任务上对 LLaMA 模型进行微调。有趣的是，在此期间，研究中最常见的方向之一是模仿学习。模仿学习在某种程度上可以看作是一种对齐方法，它通过对另一个更强大的 LLM 的输出进行微调来优化 LLM。这种方法受到了知识蒸馏的启发。具体细节可参考上图。“模型模仿的前提是，一旦通过 API 提供了专有 LM，就可以收集 API 的输出数据集，并使用该数据集对开源 LLM 进行微调。” – 引自 [6]开源模仿学习研究提出的问题很简单：我们是否可以通过仅对 ChatGPT 或 GPT-4 生成的响应进行微调，创建出与它们一样强大的模型？对此，我们可采用以下的简单方法进行预测： 收集这些模型的对话示例（如使用 OpenAI API） 在这些数据上进行（监督）微调（使用正常的语言建模目标）研究社区对于模仿学习是否为一种有价值的学习方法进行了长时间的激烈讨论。最终，我们发现这种方法在实践中能够奏效，但只在特定条件下才能发挥出良好效果。2 模仿学习的初步努力LLaMA 催生了众多的模仿模型（引自[7,8,9,10]）LLaMA 发布之后，研究人员使用 ChatGPT 的派生对话，迅速发布了各类模仿模型。通常，用于训练这些模型的数据（禁止用于商业用途）可以从 OpenAI API 或类似 ShareGPT 处获得。下面按时间顺序总结了一些广为人知的模仿模型。Alpaca [7] 通过 self-instruct [11]框架，在 GPT-3.5（即 text-davinci-003 ）中自动收集微调数据集，对 LLaMA-7B 进行微调。收集数据和对 Alpaca 进行微调仅需600美元。Vicuna [8] 使用 ChatGPT（从 ShareGPT 派生）的 70,000 个对话示例对 LLaMA-13B 进行微调。有趣的是，Vicuna 的整个微调过程仅需 300 美元。Koala [9] 在来自 Alpaca 的微调数据集和其他来源（如 ShareGPT、HC3、OIG、Anthropic HH 和 OpenAI WebGPT/Summarization）的大量对话示例上对 LLaMA-13B 进行了微调。与之前的模仿模型相比，Koala 在更大的数据集上进行了微调，并进行了更全面的评估。GPT4ALL [16] 在来自 GPT-3.5-turbo 的80万个聊天补全（chat completions）上对 LLaMA-7B 进行了微调。作者还发布了训练/推理代码和量化模型权重，可以在计算资源较少的情况下进行推理（如使用笔记本电脑）。（引自[8,9]）模仿的影响。这些模仿模型相继发布，并声称其质量可与 ChatGPT 和 GPT-4 等顶级专有模型相媲美。例如，Vicuna 能达到相当于 GPT-4 模型质量的 92%，Koala 在多数情况下能达到或超过 ChatGPT 模型表现，可参考上图。上述结果表明，通过模仿学习，我们可以提取任一专有模型的能力，并将其转化为一个更小的开源 LLM 。假如情况果真如此，那么人们将能够轻松复制最优秀的专有 LLM ，从而导致专有 LLM 丧失其优势。“开源模型具有快速、可定制、更私密、更高质量等特点。开源社区仅需 100 美元和 13B 参数，就能做到谷歌需花费 1000 万美元和 540B 个参数才能勉强完成的任务。此外，谷歌为了实现这些目标需要几个月的时间，而开源模型只需几周便可完成。” ——引自[9]模仿模型的爆炸式增长使开源模型首次被真正视为闭源 LLM 的潜在替代品之一，自 GPT-3 提出以来，闭源 LLM 就主导了这一领域。尽管使用付费 API 逐渐成为了标准，但模仿模型的出色表现为开源 LLM 带来了希望。3 模仿模型是否现实可行？（引自[6]）尽管模仿模型的出色表现给人们带来了希望，但 [6] 表明，我们忽略了一些重要因素，即需要对这些模型进行更具针对性的评估，经过评估，我们发现模仿模型的表现远不及 ChatGPT 和 GPT-4等顶级专有 LLM 。事实上，在大多数情况下，通过模仿来微调基础模型并不能弥合开源模型与专有模型在效果上的巨大差距。相反，由此产生的模型仅能更好地处理在微调集中大量呈现的任务，但也更容易出现幻觉。（引自[6]）实验设置：为评估模仿学习的效果，[6]中的作者从 ChatGPT 中筛选了约 130000 个不同的对话示例，并构建了一个数据集。然后，他们对不同规模的语言模型使用不同数量的模仿数据进行微调，并衡量了它们的效果。上述实验可得出以下几个有趣的观察结果：在人类评估试验中，用于微调的模仿数据量并不能提高模型质量。模仿模型在标准化基准测试中的表现通常不如基础模型（当使用更多模仿数据时，质量甚至会下降）。增加基础模型的大小能逐步提升模仿模型的质量。其中到底发生了什么？当在更广泛的自然语言评估基准上对模仿模型进行评估时，我们发现模仿模型的质量与其对应的基础 LLM 相当或稍差。换句话说，模仿模型实际上并不能与 ChatGPT 等模型的质量相匹敌。与专有 LLM 相比，这些模型的知识基础较为有限，这一点可以从更大的基础模型的质量改进中得到印证。“在我们看来，相比模仿专有系统，改进开源模型的最佳策略是解决核心难题，即开发出更优秀的基础 LLM 。”——引自[6]关于这一点，我们首先需要考虑：为什么这些模型的表现如此出色？在[6]中我们看到，模仿模型学会了模仿 ChatGPT 等模型的风格。因此，即使模型更频繁地生成事实错误信息（这些信息更难以进行简单的检查或验证），人类工作者仍会受蒙蔽，认为模型输出的是优质内容。4 模仿学习是否真的有用？“研究表明，从逐步解释中学习（无论这些解释由人类生成还是由更先进的 AI 模型生成）是改进模型能力和技能的一种有效途径。”——引自[1]在[6]中发现，模仿模型的表现并不像最初预计的那样出色，研究界对于模仿模型的真正价值感到困惑。然而，值得注意的是，[6]的分析中指出，局部模仿——即学习模仿模型在特定任务上的行为，而非模仿其整体行为——是相当有效的。然而，这并不意味着模仿模型可大致与专有模型的质量相匹配。为了整体提升模仿模型的质量，[6]的作者提出了两个途径：生成一个更大、更全面的模仿数据集创建一个更好的基础模型用于模仿学习有趣的是，随后的研究对这两条途径进行了深入探索，结果表明二者都能产生积极效果。（引自[12]）Orca [12] 是基于 LLaMA-13B 的一种模仿模型。然而，与之前的模仿学习工作相比，Orca 是在从 ChatGPT 和 GPT-4 收集的质量更高、更详细、更全面的数据集上进行训练的。之前用于模仿学习的数据集是一种“浅层”数据，只包含了 ChatGPT 等模型生成的提示和响应示例，可参考上图。“我们得出的结论是，如果要纯粹通过模仿来广泛匹配 ChatGPT，就需要集中精力收集大量的模仿数据集，并收集比当前可用数据更加多样、质量更高的模仿数据。”——引自[6]为改进浅层模仿，Orca 尝试通过以下方式增加由 ChatGPT 或 GPT-4 等模型生成的模仿数据集：解释跟踪逐步思考过程复杂指令为此，私有LLM被提示通过指令或系统信息提供其响应的详细解释。通过向模仿模型可见的数据增加额外的有用信息，这种方法超越了简单的提示——响应对。当从 ChatGPT 等强大模型中进行学习时，Orca 不仅只看到模型的响应，还可以从详细解释和思考过程中学习，这些解释和思考过程与模型的响应在复杂提示上共同生成。下面是一个示例以作说明。（引自[12]）在使用这种详细的模仿数据集（来自 ChatGPT 的 500 万个示例和来自 GPT-4 的 100 万个示例）进行大规模微调之后，与先前的模仿模型相比，Orca 的表现非常出色，可参考下图。虽然 Orca 显著缩小了开源模仿模型与私有 LLM 之间的差距，但我们仍然可以从下表中看到，该模型在质量上始终不及 GPT-4。不幸的是，即便改进了模仿方法，开源模仿模型也无法完全匹敌顶级专有模型的质量。然而，Orca 的出色表现表明，模仿学习是一种有价值的微调策略，可显著改善高质量基础 LLM 的质量。进一步的研究发现，在[12]中，成功利用模仿学习有两个重要要求：一个大规模、全面的模仿数据集每个响应都包含详细的解释跟踪更好的基础 LLM。尽管[6]中的作者认为，收集足够大且多样化的模仿学习数据集非常困难，但 Orca 的例子证明了这种做法的可行性。此外，随后的研究还探索了[6]中的另一个途径：创建更强大的（开源）基础模型。虽然最初开源预训练 LLM 的表现不佳，但我们最近看到了各种强大的预训练 LLM 的发布，如 LLaMA [3]、MPT [14，15]和Falcon [13]。鉴于模型预训练是后续微调（如模仿学习、SFT、RLHF 等）的起点，预训练更强大的基础模型会改善下游模仿模型的质量！本系列文章的第二部分涵盖了所有最优秀的开源预训练语言模型。5 对齐开源 LLM（引自[5]）模仿学习试图通过对专有语言大模型（LLM）的响应（及其解释轨迹）进行训练，从而提升开源基础模型的质量。尽管在一些特定情况下，这种方式取得了可喜成果，但（显然）这并不是顶级专有模型接受训练的方式——模仿只是快速构建强大开源模型的捷径。如果我们期望开源 LLM 能与专有模型的表现相媲美，就需要在对齐方面加大投入。“这些闭源的产品级 LLM 都经过了大规模微调，使其更符合人类偏好，从而极大地提升了模型的易用性和安全性。然而，这个步骤可能需要投入大量的计算资源和人工标注，且往往缺乏透明度，难以被复现。\" ——引自 [1]对齐的阻碍是什么？对齐开源模仿模型的想法看似很简单，既然已经有了出色的基础模型，为什么不直接复制 GPT-4 这样的模型的对齐过程呢？问题在于，对齐过程需要大量的计算和人工标注资源，并且严重依赖专有数据，这限制了透明度，使得复现结果变得十分困难。因此，一段时间以来，开源模型在对齐研究方面一直滞后于其对应的专有模型。接下来我们将探讨两项最近的研究 —— LIMA [2] 和 LLaMA-2 [1]，它们通过更好的对齐显著提高了开源 LLM 的质量。6 开源对齐的前期工作在介绍 LIMA 和 LLaMA-2 之前，值得注意的是，开源研究社区并没有完全回避对齐预训练模型。例如，Falco-40B-Instruct [13] 对来自 Baize（Chatbot） 的 1.5 亿个词元进行了结构微调（SFT）。类似地，还发布了许多经微调的 MPT-7B [14] 和 MPT-30B [15] 的变体，其中包括 Chat/Instruct 变体，在公共数据集上经过 SFT 处理，以及 StoryWriter 变体，在上下文更长的的数据上进行了微调。（引自开源LLM排行榜）此外，如果我们简单观察一下开源 LLM 排行榜（如上图所示），会发现不同模型都在各类数据集上通过 SFT 进行了微调，开源 LLM 并没有完全回避对齐过程。然而，顶级的专有模型则同时使用了 SFT 和 RLHF，在大规模高质量的对话和人类反馈数据集上进行对齐；与之相比，大多数开源模型仅使用质量较低且缺乏多样性的公共数据集进行对齐。因此，要想真正与专有模型的质量相媲美，开源 LLM 需要尝试复现其对齐过程。7 LIMA：数据高效对齐[2]“模型的知识和能力几乎完全来自于预训练阶段的学习，对齐过程则教会了模型在与用户交互时应使用哪个子分布格式。”——引自 [2]如上文所述，长期以来，开源 LLM 主要通过在公共数据集上进行 SFT 来对齐。考虑到对 SFT 的重视，[2]中的作者对预训练 LLM 中 SFT 的作用进行了深入探究。目标在于揭示预训练和通过 SFT 对齐在创建高质量 LLM 中的相对重要性，并揭示在经过 SFT 后最大化模型性能的最佳实践。数据集：为实现这一目标，[2]中的作者构建了一个包含 1000 个对话示例的小型数据集用于SFT。尽管数据量看起来不大，但这个数据集中的示例都经过精心筛选，通过提示的多样性和输出风格或语气的统一性，确保了数据集的质量。详见下图。（引自 [2]）尽管用于训练 LIMA 的 SFT 数据集规模较小，但其质量非常出色。有趣的是，在 [2] 中我们可以看到，LIMA 经过该数据集的微调后表现异常出色，甚至接近于 GPT-4 和 Claude 等 SOTA LLM 的表现，详见下图。（引自 [2]）这一结果表明，语言模型可以通过少量精心筛选的示例有效对齐。尽管 LIMA 的表现仍然不及 GPT-4，但能够用如此少的数据实现如此高质量的对齐，这样的结果既出乎意料又令人印象深刻。这一发现告诉我们，在进行 SFT 时，数据质量似乎是最重要的因素。我们能够学到什么？我们从 LIMA 中学到了许多有价值的经验。首先，对于 SFT 而言，仅增加数据量是不够的，数据质量也至关重要，这一点在上图中有详细说明。此外，在[2]中，关于\"表面对齐假设\"提出了一个独特的全新视角来解决对齐问题。简而言之，这个假设认为大多数 LLM 的核心知识是在预训练过程中学习的，而对齐的关键则是找到适当的格式或风格来展现这些知识。因此，对齐能够以一种数据高效的方式进行学习。8 LLaMA-2：提高对齐研究的透明度 [1]“Llama 2-Chat 是数月研究和对齐技术迭代应用的结果，其中包括指令调整和RLHF，需要大量的计算和标注资源。”——引自 [1]近期发布的 LLaMA-2[1] 套件由几个开源模型组成，其参数大小在 70 亿到 700 亿之间。与其前身 LLaMA-1 相比，LLaMA-2 的预训练数据要多 40%（2 万亿个词元），具有更长的上下文长度，并采用了针对快速推理进行优化的架构（分组查询注意力[4]）。LLaMA-2 成为了开源模型的 SOTA。然而，LLaMA-2 套件中不仅包含预训练的 LLM，还通过大规模对话数据和人类反馈对每个模型进行了微调（同时使用 SFT 和 RLHF），并投入了大量精力进行对齐。由此得到的模型被称为LLaMA-2-Chat。（引自 [5]）LLaMA-2 这些经优化的版本表现出色，并在弥合开源和专有 LLM 之间的对齐差距迈出了重要一步。LLaMA-2 的对齐过程强调两个关键的行为特性：1. 有用性：模型满足用户的请求并提供所需信息。2. 安全性：模型避免回复“不安全”的内容。为确保对齐后的模型既有用且安全，根据以上原则，对为 SFT 和 RLHF 提供的数据进行了筛选、收集和标注。（引自 [1]）SFT：LLaMA-2 对齐过程的第一步是使用 SFT 进行微调。与其他开源 LLM 类似，LLaMA-2 首先在公开可用的指令调优数据上进行微调。然而，这样的数据往往缺乏多样性，质量欠佳，正如LIMA [2]中所示，这会严重影响模型效果。因此，[1]中的作者集中收集了一小部分高质量数据用于 SFT。这些数据来源多样，包括手动创建或标注的示例，以及从公开来源获取并经过质量过滤的数据。最终，LLaMA-2 通过使用 27540 个高质量对话示例进行第二阶段的微调。具体示例可参考上图。“令人惊讶的是，我们发现经过 SFT 模型采样的输出往往可与人工标注的 SFT 数据相媲美，这表明我们可以重新调整优先级，将更多的标注工作投入到基于偏好的 RLHF 的标注中。” ——引自[1]有趣的是，[1]中的作者观察到，对于 SFT 的收集数据而言，超过 27K 个高质量示例所带来的效益是递减的。这些发现与 LIMA [2]的实证分析结果相一致。我们并不需要大量的数据进行 SFT，但这些数据必须是高质量的。同时，[1]中的作者还注意到，经过 SFT 的 LLaMA-2 模型似乎能够自动生成 SFT 数据。RLHF：LLaMA-2 进一步使用超 1百万 个人类反馈示例的 RLHF 进行微调。为收集这些反馈，采用了二进制协议，要求人工标注者编写一个提示，并从 LLM 生成的两个响应中择优选择其中之一，进而根据有用性和安全性标准，收集人类偏好数据。例如，注重安全性的人类偏好标注可能会鼓励标注者设计一个可能会引发不安全响应的对抗性提示。然后，人类标注者可以标注哪个响应更可取、更安全（如果有的话）。“在其他条件相同的情况下，奖励模型的改进可以直接转化为对 LLaMA 2-Chat 的改进。” ——引自[1]人类反馈数据按批次收集，而 LLaMA-2 在每个批次之间通过 RLHF 进行微调。因此，在每次 RLHF 试验后，每个 LLaMA-2-Chat 模型将迭代创建多个版本，总共有五个版本。在[1]中，我们看到每次收集新的人类偏好数据时，都会训练一个新的奖励模型用于 RLHF，以确保奖励模型准确捕捉到最新模型的人类偏好。此外，出乎意料的是，生成的奖励模型的质量能总体上预测 LLaMA-2-Chat 的模型质量。总之，在整个迭代 RLHF 的过程中，LLaMA-2 会利用超一百万次的人类反馈示例来进行微调。（引自 [1])如上图所示，LLaMA-2-Chat 的质量（考虑有用性和安全性）在与 SFT 和 RLHF 对齐的多次迭代中平稳提升。这个可视化清晰地展示了每种技术对结果模型质量的影响程度。也就是说，单独使用 SFT 的效果有限。但即便仅应用 SFT，每个 RLHF 阶段的执行都显著提高了模型的对齐水平。Open LLM 排行榜上的前五名模型均基于 LLaMA-2（来自 OpenLLM 排行榜）质量：如上述 Open LLM 排行榜所示，LLaMA-2-Chat 模型目前是开源 LLM 领域的 SOTA。相比[1]中其他流行 LLM ，我们可以看到 LLaMA-2-Chat 模型在有用性和安全性方面远远优于其他开源模型。详见下图。（引自 [1]）此外，LLaMA-2 在有用性和安全性方面的表现甚至可与 ChatGPT 这样的顶级专有模型相媲美。简而言之，以上结果充分表明 LLaMA-2-Chat 模型的对齐质量很高，生成的模型能够准确捕捉并遵守人类期望的有用性和安全性标准。“[对齐]可能需要大量的计算和人工标注成本，且通常不够透明，难以复现，这阻碍了社区推动 AI 对齐研究的进展。\" ——引自[1]。LLaMA-2 的重要性：对开源 LLM 研究而言，LLaMA-2 不仅树立了新的质量标杆，还采用了与之前工作根本不同的方法。通过参考[2]，我们了解到专有 LLM 通常依赖大量专门标注的数据进行对齐，而在开源研究中，这个过程更加难以复现。尽管之前的开源模型主要利用 SFT 和公开的对话数据源，但 LLaMA-2 是最早在开源 LLM 中广泛投入对齐过程的模型之一，它精心策划了大量高质量的对话和人类偏好，用于 SFT 和 RLHF。这使得 LLaMA-2 不仅在质量上有所突破，也在方法上为开源 LLM 研究做出了重要贡献。9 结语本系列文章全面探讨了从 OPT 到 LLaMA-2 的整个开源语言模型的发展历程。尽管在这两个模型之间进行了大量研究，但它们的提出仅相隔一年！开源人工智能研究社区的发展十分迅速，跟进该领域的研究非常令人兴奋，这一过程有趣且有益。LLaMA-2-Chat 等强大的模型令人敬畏。作为从业者和研究人员，我们能够使用这些模型，从中学习，并深入了解它们的工作原理，这样的机会独一无二，理应珍惜。特别对于 LLM 来说，开源研究真的非常酷！参考文献（请上下滑动）[1] Touvron, Hugo, et al. \"Llama 2: Open Foundation and Fine-Tuned Chat Models.\" arXiv preprint arXiv:2307.09288 (2023).  [2] Zhou, Chunting, et al. \"Lima: Less is more for alignment.\" arXiv preprint arXiv:2305.11206 (2023). [3] Touvron, Hugo, et al. \"Llama: Open and efficient foundation language models.\" arXiv preprint arXiv:2302.13971 (2023). [4] Ainslie, Joshua, et al. \"GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.\" arXiv preprint arXiv:2305.13245 (2023). [5] “Introducing Llama2: The next generation of our open source large language model”, Meta, https://ai.meta.com/llama/. [6] Gudibande, Arnav, et al. \"The false promise of imitating proprietary llms.\" arXiv preprint arXiv:2305.15717 (2023). [7] Taori,  Rohan et al. “Stanford Alpaca: An Instruction-following LLaMA model.” (2023). [8] Chiang, Wei-Lin et al. “Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality.” (2023). [9] Geng, Xinyang et al. “Koala: A Dialogue Model for Academic Research.” (2023). [10] Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar. GPT4All: Training an assistant-style chatbot with large scale data distillation from GPT-3.5-Turbo, 2023. [11] Wang, Yizhong, et al. \"Self-instruct: Aligning language model with self generated instructions.\" arXiv preprint arXiv:2212.10560 (2022). [12] Mukherjee, Subhabrata, et al. \"Orca: Progressive Learning from Complex Explanation Traces of GPT-4.\" arXiv preprint arXiv:2306.02707 (2023). [13] “Introducing Falcon LLM”, Technology Innovation Institute, https://falconllm.tii.ae/. [14] “Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable Llms.” MosaicML, http://www.mosaicml.com/blog/mpt-7b. [15] “MPT-30B: Raising the Bar for Open-Source Foundation Models.” MosaicML, http://www.mosaicml.com/blog/mpt-30b. [16] Gou, Jianping, et al. \"Knowledge distillation: A survey.\" International Journal of Computer Vision 129 (2021): 1789-1819. [17] Ouyang, Long, et al. \"Training language models to follow instructions with human feedback.\" Advances in Neural Information Processing Systems 35 (2022): 27730-27744. [18] Glaese, Amelia, et al. \"Improving alignment of dialogue agents via targeted human judgements.\" arXiv preprint arXiv:2209.14375 (2022).注释1.暂时先写到这里！我确信在继续开源 LLM 研究之后，会写另一篇文章进行分享。2.这个“配方”——通常被称为三步技术——是由InstructGPT（ChatGPT 的姐妹模型）提出的，自提出以来已被许多强大的 LLM 所采用！3.我不确定模仿学习是否算作对齐。它与 SFT 非常相似，我们从现有的强大 LLM（例如 GPT-4）中选择对话示例进行 SFT。人们可以将模仿学习看作是一种通用微调，甚至是一种指令微调的变体。4.这个指标是通过使用 GPT-4 作为评判者进行自动评估获得的。5.Orca 使用 FLAN 收集的提示生成模仿数据集，由于 OpenAI API 的速率/词元限制，这个过程需要几周时间。6.有趣的是，[1]中的作者采用了两种不同的 RLHF 方法，包括典型的 PPO， RLHF 变体和一种拒绝采样微调变体：i) 从模型中采样 K 个输出；ii) 选择最佳输出；iii) 在该示例上进行微调。值得注意的是，这两种方法都基于强化学习。7.正如模仿学习一样，这些公开数据甚至可以来自其他的强大 LLM。例如，我们可以参考 ShareGPT 提供的对话数据。其他人都在看GPU架构与计算入门指南为什么开源大模型终将胜出OpenAI规模经济与第二护城河微调语言大模型选LoRA还是全参数全面对比GPT-3.5与LLaMA 2微调语言大模型推理性能工程：最佳实践开源LLM演进史：高质量基础模型竞赛试用OneFlow: GitHub - Oneflow-Inc/oneflow: OneFlow is a deep learning framework designed to be user-friendly, scalable and efficient.",
        "voteup_count": 4,
        "updated_time": "2023-11-12 20:24:03",
        "question_id": 623672939,
        "user_id": "c3e085905d3d937577852b893d204cc7"
    },
    {
        "answer_id": 3486021492,
        "content": "ChatGLM 绝对是固步自封，最后被市场策略击败。",
        "voteup_count": 1,
        "updated_time": "2024-05-03 08:40:54",
        "question_id": 623672939,
        "user_id": "a18b203fce545918c5a60a0a99aa2991"
    },
    {
        "answer_id": 3484407526,
        "content": "最近，Meta公司再次引起了技术界的广泛关注，他们推出了新一代开源大模型——Llama 3，这个模型不仅提高了技术的可访问性，也为AI的未来开辟了新的道路。然而，虽然这一大模型听起来很美，但对于中文用户来说，Llama 3似乎还有一段距离要走。今天给大家分享一个Llama 3 8B中文优化版整合包。Llama 3简介Llama 3在发布时包括了两个版本，8B和70B，这两个模型的规模巨大，性能强大，分别对标业界的顶尖AI模型。但是，如果你用中文向它提问，可能会收到英文或中英混合的答复，这显然不利于国内用户的体验。因此，针对这一点，需要对模型进行适当的微调才能更好地服务于中文语境。不过，幸好我们有一群充满才华和热情的开发者正在努力解决这个问题。在Github和HuggingFace平台上，已经出现了一些专门针对中文优化的Llama 3项目。其中，“llama3-Chinese-chat”项目尤为引人注目。“llama3-Chinese-chat”是一个由开发者Ke Bai创建的GitHub项，这个项目基于Meta的Llama-3-8B-Instruct模型进行了微调，专门优化了中文处理的效果。这个版本的Llama 3显著减少了中文问题英文回复的现象，同时也优化了中英文混合回复的问题，使得答案更加贴近中文用户的使用习惯。离线懒人包来了而为了让大家更容易上手，我出手为该项目推出了一款离线整合包。这个懒人包极大简化了部署过程，用户只需下载并解压，然后双击“一键启动”，即可在本地部署并运行这一强大的AI模型。①双击“一键启动.exe”。②双击一键启动程序后，会打开一个命令提示窗口，项目会自动运行。加载成功后，项目会自动打开浏览器加载主界面。记得点点关注不迷路哦，后续还有更多酷炫的AI项目分享~③打开页面后，可以看到项目主界面。加载完模型后，就可以在本地快速使用啦~使用很简单:输入提示词：跟使用ChatGPT一样，输入提示词即可。点击提交：可以跟Llama 3 8B大模型在本地无限免费聊天了。这种本地化的AI应用不仅方便用户探索和利用最新的AI技术，还极大地推动了开源模型在中国的应用和发展。随着越来越多的开发者加入优化和本地化的行列，未来Llama 3在中文处理方面的表现只会越来越好。这不仅是Meta的胜利，更是全球AI社区共同努力的成果。喜欢尝鲜的你，不妨试试这款离线整合包，体验一下在家与AI聊天的乐趣吧！注意事项：①该项目建议使用英伟达显卡运行，建议8G显存以上②请确保安装路径不包含中文，不然可能会引起奇怪的适配问题懒人包球友专享下载链接：（本文首发于最强开源大模型Llama-3中文特别版来了！）",
        "voteup_count": 1,
        "updated_time": "2024-05-01 08:58:32",
        "question_id": 623672939,
        "user_id": "9b68b2c43555437ca8bd565753c6035b"
    },
    {
        "answer_id": 3483759456,
        "content": "本周，开源领域迎来多项技术成果更新：开源大语言模型迎来 Meta Llama3 和微软的 WizardLM 2，CodeQwen1.5-7B 加入开源代码领域，Mistral-22b-v0.2 在开源中探索 MOE 与稠密模型的转换，Mini-Gemini 和 Hugging Face 开源的视觉语言模型 Idefics2 则是在开源多模态模型中不断演进。除了技术演进外，商业领域裁员与融资并存。之前占据融资热点的 AI 明星企业 Stability AI 和 Tome 相继宣布裁员计划，与之相对的则是大模型领域动辄数亿美元的融资。这也为诸多公司敲下了警钟，在应用淘汰赛中，如何在可控成本下，找寻能赚取稳定现金流的场景，是 AI 产品能否持续运营的关键。并且随着市场竞争的加剧，这一淘汰赛正在迅速展开。具体内容大模型持续更新4 月 12 日，知识管理厂商印象笔记宣布其自研大语言模型被正式命名为「印象大模型」，并已根据《生成式人工智能服务管理暂行办法》及相关法律法规完成模型备案，其 AI 产品印象 AI 也迎来多项功能更新，未来将为更多用户提供包含阅读、总结在内的多项智能化知识管理服务。4 月 14 日，OpenAI 在官宣日本办事处的同时，宣布推出针对日语优化 GPT-4 定制模型。Open AI 表示，以 Speak 为代表的本地企业已经可以使用自定义模型，该模型在翻译和总结日语文本方面提供了更高的性能。最重要的是，其运行速度比 GPT-4 Turbo 快三倍，这样的成本效益将成为满足当地各种需求的合适选择。4 月 17 日，MiniMax 稀宇科技 正式发布其 MoE 模型 abab 6.5 系列，该系列包含 abab 6.5 和 abab 6.5s，其中 abab 6.5 包含万亿参数，并支持 200k tokens 的上下文长度，abab 6.5s 同样支持 200k tokens 的上下文长度，但更高效，可以在 1 秒内处理近 3 万字的文本。多模态领域4 月 13 日，xAI 在其官网推文中宣布推出多模态模型 Grok-1.5 Vision，这也意味着，除了文本信息，Grok 现在还可以处理各种包含图表、表格、截图和照片在内的视觉信息，并将于近期邀请现有的 Grok 用户进行测试。4 月 15 日，香港中文大学终身教授贾佳亚团队提出的开源多模态模型 Mini-Gemini 宣布其 130 亿参数的 demo 上线 Hugging Face。此前于 3 月 28 日，Mini-Gemini 即宣布其代码、模型、数据已经全部开源。4 月 16 日，Hugging Face 更新了其视觉语言模型 Idefics2。该模型能够理解和生成基于图像和文本的文字回复，并且在 OCR 识别能力方面显著增强。开源领域4 月 13 日，Mistral AI 在发布 Mistral-22b-v0.1 仅仅两天之后，宣布开源 Mistral-22b-v0.2。该模型实现了从 MOE 到稠密（Dense）模型的转换，并且其训练数据是 v0.1 的 8 倍。相较于 v0.1， v0.2 在数学才能和编程能力获得明显提升，并且在多轮对话中也能保持高度的对话流畅性。Mistral AI 同时宣布 v0.3 已经在训练过程中，并将有更多 220 亿参数的模型发布，直到其找到将 MOE 压缩的最佳成果。4 月 15 日，微软发布并开源其新一代大语言模型系列 WizardLM 2，此系列包括三个模型，分别是 WizardLM-2 8x22B（MOE）、WizardLM-2 70B 和 WizardLM-2 7B。但 4 月 16 日，微软宣布因为其不熟悉新模型的发布流程，未能对 WizardLM 2 进行毒性测试（toxicity testing），并已将代码文件从 Github 以及 Hugging Face 上删除，在完成测试后会尽快重新发布。4 月 16 日，通义千问团队开源了基于 Qwen1.5 的代码模型 CodeQwen1.5-7B 及其对话模型。CodeQwen1.5-7B 支持 92 种编程语言，并且能够处理最长 64 K 的上下文输入，并展现出了优秀的代码生成、长序列建模、代码修改等能力。4 月 17 日，AGI 公司 Zyphra Technologies 宣布推出其新一代开源基础模型 Zamba-7B。这个 70 亿参数的模型定位于 AI 设备的装载上，并声称在基准测试中优于 LLaMA 1、LLaMA 2-7B。同时其模型权重也即将开源，以供大家判断实际效果。4 月 17 日，昆仑万维宣布其基座大模型——天工 3.0 开启公测。天工 3.0 拥有 4000 亿参数，是目前全球最大的开源 MoE 大模型（但目前在 Github 和 ModelScope 未见其开源项目）。同时，天工 3.0 新增了图表对比生成、研究模式、增强模式、扩图修图等功能。4 月 18 日，Meta 正式发布 Llama3，目前已经上架官网和 Hugging Face。此次开源的 Llama3 共包括 2 个模型，Meta-Llama-3-8B 和 Meta-Llama-3-70B。在 MMLU、GPQA、HumanEval、GSM-8K、MATH 这五个评测集的表现上，不仅超过了 Mistral 7B，甚至部分评测集中，Meta-Llama-3-8B 模型的得分超过了 Meta-Llama-2-70B。而且在未来几个月内，Meta 还会推出更多的版本。应用探索新产品新功能 / 插件4 月 16 日，Poe 宣布推出其 3.0 版本，并新增多机器人聊天功能，用户可以通过 @指令，在不同任务场景下调用多个大模型进行对话，以发挥不同大模型的优势。Poe 致力成为对话 AI 应用商店，提供变现工具和企业服务。4 月 16 日，Adobe 宣布推出一款适配 Adobe Acrobat Reader 和 Adobe Acrobat 的 AI 助手——Adobe Acrobat AI Assistant，以帮助用户快速处理、检索、阅读和总结吸收 PDF 文档中的内容。目前，该功能仅支持英文，预计未来还会扩展至更多语言。4 月 16 日，Adobe 宣布了 Premiere Pro 全新版本的更新计划，本次更新中包含了为第三方 AI 视频生成模型添加插件。这意味着在 Adobe Firefly 自身的能力之外，用户即将可以直接通过 Adobe 工具体系调用 OpenAI Sora、Runway Gen-2 和 Pika。4 月 17 日，昆仑万维宣布基于天工 3.0 打造的天工 SkyMusic 登录天工 APP ，并开启全面公测。天工 SkyMusic 可以生成 80 秒 44100Hz 采样率双声道立体声歌曲，支持生成说唱、民谣、放克、古风、电子等多种音乐风格，还能学习颤音、歌剧、吟唱、男女对唱、自动和声等歌唱技巧。同时，也支持参考音乐与方言歌曲两种生成方式。4 月 18 日，钉钉 AI 助理市场（AI Agent Store）正式上线，首批将推出超过 200 个 AI 助理，覆盖企业服务、效率工具、财税法务、教育学习等类别。根据钉钉披露，截至 2024 年 3 月底，钉钉 AI 已超过 220 万家企业使用，月活跃企业超过 170 万家。终端 AI4 月 12 日，蔚来宣布端云多模态大模型 NOMI GPT 正式启动推送。NOMI GPT 内置的认知中枢、情感引擎和端侧多模态感知架构赋予了 NOMI 与用户进行开放式问答的交互能力。本次升级后，用户可在车内体验到大模型百科、无限趣聊、魔法氛围、趣玩表情、用车问答、AI 场景生成在内的多项全新交互体验。4 月 17 日，Rewind 宣布推出一款可穿戴 AI 设备 Limitless。Limitless 可以记录用户的日常对话内容，并利用 AI 进行会议准备、实时传译、记录和总结。这款产品预计在 2024 年 8 月份发货，预计售价为 99 美元。4 月 18 日，联想在 TechWorld 2024 上发布了内嵌个性化 AI 智能体「联想小天」的 AI PC 系列产品，价格从 5999 到 17999 元不等，目前已开启预购。其他4 月 12 日，已发布大模型安全基座和 AI 生成内容检测基座的瑞莱智慧在其公众号宣布，已经完成新一轮战略融资。本轮融资由光源资本担任独家财务顾问，投资方包括北京市人工智能产业投资基金等。4 月 13 日，估值 3 亿美元的 AI 初创公司 Tome 解雇了 12 名员工，在解雇之前该团队拥有 59 名员工。Tome 产品专注于 AI 生成 PPT，截至 4 月初，Tome 付费专业版每月收入约为 30 万美元。4 月 15 日，微软在其官网宣布，其将向阿联酋 AI 公司 G42 投资 15 亿美元，并持有少数股权和董事会席位。G42 将在微软云计算平台 Azure 上运行其人工智能应用和服务，来为中东地区、中亚和非洲国家的各行各业提供先进的 AI 解决方案。4 月 16 日，根据媒体消息，由王小川创立的百川智能正在进行新一轮数亿美元的融资，本轮融资也将成为今年以来国内 AI 领域最大的融资之一。4 月 18 日，根据内部电子邮件，Stability AI 新任命的联席 CEO Shan Shan Wong 和 Christian Laforte 宣布，Stability AI 裁员 20 多名员工，这涉及这个 200 人 团队的 10%。此前于 3 月 23 日，Stability AI 宣布其 CRO Emad Mostaque 离职，并退出董事会。",
        "voteup_count": 0,
        "updated_time": "2024-04-30 14:39:00",
        "question_id": 623672939,
        "user_id": "dd0cf3f9b8809033760b59336b599f6d"
    },
    {
        "answer_id": 3471719929,
        "content": "1、导读3D高斯泼溅最近已经成为新视角合成的一种高效表示方法。本工作研究了其编辑能力，特别是着重于补全任务，旨在为不完整的3D场景补充高斯，以实现视觉上更好的渲染效果。与2D图像补全任务相比，补全3D高斯模型的关键是要确定新增点的相关高斯属性，这些属性的优化很大程度上受益于它们初始的3D位置。为此，我们提出使用一个图像指导的深度补全模型来指导点的初始化，该模型基于2D图像直接恢复深度图。这样的设计使我们的模型能够以与原始深度对齐的比例填充深度值，并且利用大规模扩散模型的强大先验。得益于更精确的深度补全，我们的方法，称为InFusion，在各种复杂场景下以足够更好的视觉保真度和效率（约快20倍）超越现有的替代方案。并且具有符合用户指定纹理或插入新颖物体的补全能力。原文链接：最新开源 | 又快又好的扩散模型助力3D高斯场景补全(a) InFusion 能够无缝删除 3D 对象，以用户友好的方式进行纹理编辑和对象插入。(b) InFusion 通过扩散先验学习深度补全，显着提高深度修复质量。下面一起来阅读一下这项工作~2、论文信息标题：InFusion: Inpainting 3D Gaussians via Learning Depth Completion from Diffusion Prior作者：Zhiheng Liu等人机构单位: 中科大，港科大，蚂蚁，阿里巴巴项目主页地址: https://johanan528.github.io/Infusion/Github仓库: https://github.com/ali-vilab/infusion3、背景3D高斯作为新视角合成的一种重要方法，因能够以惊人的渲染速度制作出具有真实感的图像而受到重视。3D高斯提供了明确的表示能力和实时处理的可能性，大大提高了编辑3D场景的实用性。特别是对于虚拟现实（VR）和增强现实（AR）等互动式下游应用，研究如何编辑3D高斯变得越来越重要。我们的研究关注于3D高斯的补全任务，这对于3d场景编辑至关重要，有效填补了确实部分，并为进一步的移动物体，增加新物体，改变纹理等编辑方式奠定基础。现有方法对3D高斯补全的初步探索通常是使用对不同角度的渲染图象进行图像层次的补全，迭代的使用修复后的2D多视图图像作为新的训练数据。但是，这种方法往往会因生成过程中的不一致而产生模糊的纹理，且速度缓慢。值得注意的是，当初始点在3D场景中精确地定位时，高斯模型的训练质量会显著提高。因此一个实际的解决方案是将需要补全位置的高斯设置到正确的初始点，从而简化整个训练过程。因此，在为需补全高斯分配初始高斯点时，进行深度补全是关键的，将修复后的深度图投影回3D场景能够实现向3D空间的无缝过渡。因此，我们引入了InFusion，一种创新的3D高斯补全方法，我们利用了预训练扩散模型先验，训练了一个深度补全模型。我们的方法表明，Infusion可以准确确定初始点的位置，显著提高了3D高斯图像修复的保真度和效率。该模型在与未修复区域的对齐以及重构物体深度方面展现了显著的优越性。这种增强的对齐能力确保了补全高斯和原3D场景的无缝合成。此外，为了应对涉及大面积遮挡的挑战性场景， InFusion可以通过渐进的补全方式，体现了它解决此类复杂案例的能力。4、方法如上图InFusion技术方案的核心是一个以输入的RGB图像为条件的深度补全模型。这个模型能够根据观测到的单视图图像来预测和修复缺失的深度信息。它利用了预训练的潜在扩散模型先验，这些模型在大规模图像数据集上进行训练，从而具备了强大的生成能力和泛化性。整体流程如下：场景编辑初始化：首先，根据编辑需求和提供的掩码，在训练3d高斯场景的过程中，利用预先标记的掩码，构造残缺的高斯场景。深度补全：总体来说，选择一个参考视图，并对该视角渲染得到的单张RGB图像利用图像修复模型如（Stable Diffusion XL Inpainting ）进行修复。再利用深度补全模型基于观测图像预测出缺失区域的深度信息，生成补全后的深度图。具体来说，深度补全模型接受三个输入：从3D高斯渲染得到的深度图、相应的修复后彩色图像和一个掩码，其中掩码定义了需要补全的区域。先使用变分自编码器（VAE）将深度图和彩色图像编码到潜在空间中。其中通过将深度图重复使其适合VAE的输入要求，并应用线性归一化，使得深度值主要位于[-1,1]区间内。后将编码后的深度图加噪得到的近高斯噪声，将掩码区域设置为0的编码后的深度图，编码后的RGB指导图像，以及掩码图像，在channel维度进行连接，输入到U-Net网络进行去噪，逐步从噪声中恢复出干净的深度潜在表示。再次通过VAE解码得到补全后的深度图。3D点云构建：使用补全后的深度图和对应的彩色图像，通过3D空间中的反投影操作，将2D图像点转换为3D点云，这些点云随后与原始的3D高斯体集 合合并。Gaussian模型优化：合并后的3D点云通过进一步很少迭代次数的优化过程进行调整，以确保新补全的高斯体与原始场景在视觉上的一致性和平滑过渡。5、实验结果与过往方法对比，Infusion表现出保持 3D 连贯性的清晰纹理，而基线方法通常会产生模糊的纹理，尤其是复杂场景下。在更具有挑战性的场景下，包括具有多对象遮挡的场景，Infusion相比于其他方法也能够产生令人满意的效果同时通过与广泛使用的其他基线方法的比较，以及相应的点云可视化。比较清楚地表明，我们的方法成功地能够补出与现有几何形状对齐的正确形状。Infusion可以通过迭代的方式，对复杂的残缺gaussian进行补全。得益于Infusion补全3d高斯点的空间准确性，用户可以修改补全区域的外观和纹理。通过编辑单个图像，用户可以将物体投影到真实的三维场景中。此过程将虚拟对象无缝集成到物理环境中，为场景定制提供直观的工具。7、结论本文提出的方法InFusion，为3D高斯场景提供了高质量且高效的补全能力。此外，我们证明了结合扩散先验能够显著增强了我们的深度图像修复模型。这个改进的深度补全模型对于各种3D应用，特别是在新视角合成领域有着很大的应用前景。我们的方法为潜在扩散模型（LDM）与3D场景编辑之间建立了联系。这种协同作用对于未来的进一步发展和优化具有重大潜力。移步公众号「3D视觉工坊」第一时间获取工业3D视觉、自动驾驶、SLAM、三维重建、最新最前沿论文和科技动态。推荐阅读1、基于NeRF/Gaussian的全新SLAM算法2、移动机器人规划控制入门与实践：基于Navigation23、自动驾驶的未来：BEV与Occupancy网络全景解析与实战4、面向三维视觉的Python从入门到实战5、工业深度学习异常缺陷检测实战6、[第二期]基于面结构光三维重建高阶班实战",
        "voteup_count": 2,
        "updated_time": "2024-04-19 17:38:07",
        "question_id": 623672939,
        "user_id": "019de10470299bb171ad42ae0293aa68"
    },
    {
        "answer_id": 3229464690,
        "content": "国内开源的大模型：ChatGLM-6B、ChatGLM2-6BBaichuan-13B、Baichuan2-13BQianwen-7B、Qianwen-14BXVERSE-7B国外大模型Phi-1.5Llama2系列模型Falcon180BCodeLlama-34B",
        "voteup_count": 11,
        "updated_time": "2023-09-28 06:38:21",
        "question_id": 623672939,
        "user_id": "11a1ee87d7a260edff9ef6c1fc84f2d4"
    },
    {
        "answer_id": 3348162505,
        "content": "开源大模型现在很多，但是能力参差不齐，你可在huggingface中查询，并得到模型最新的排名！但是开源模型的能力一定会超越GPT-4，这只是时间问题！最近看到了网友的一篇分享，网友认为2024年开源模型能力一定会追赶上并超越GPT-4。下面分享下网友内容：是的，开源将会在今年超越GPT-4！下面是理由：1、人才方面：来自斯坦福大学、卡内基梅隆大学等顶尖学府的数千名研究人员正在研发开源人工智能模型；一些非常聪明且灵活的公司和开源开发者也在致力于改善开源人工智能。2、技术方面：从2023年取得的巨大进步来看，我们将开发出更快、更有效的训练、微调和优化方法；换言之，我们不再需要庞大的计算资源。3、大科技公司的支持：像Meta这样的重要大科技公司，以及可能的AWS，都在支持开源，并为开源作出了重大贡献。4、数据方面：已经多次证明，高质量的数据对于培养优秀的LLMs至关重要。手工精选的合成数据集比大量私有数据更有用；事实上，在发明GPT-4时，OAI并没有数据优势。5、基础设施/计算方面：2023年，风险投资者向开发开源LLMs投入了超过10亿美元。这个数字还会继续增长，并足够购买GPU。6、进展方面：这是开源最有说服力的理由！目前的结果显示，开源已经排在第三位，仅次于OAI和表现最佳的Claude模型；如果我们在2023年取得同样的进展，我们将得到一个GPT-4级别的模型。开源人工智能的趋势正在受到支持；到2024年底，我们将接近GPT-4。我愿意为此下注！",
        "voteup_count": 0,
        "updated_time": "2024-01-03 13:50:46",
        "question_id": 623672939,
        "user_id": "21d380338cafa98ae77093f0d88d73eb"
    },
    {
        "answer_id": 3240358390,
        "content": "一般大模型主要指的是大语言模型，如chatgpt等，除了语言模型，还有视觉任务，也出现了大模型，比如sam。1.大语言模型大语言模型开源的很多，列出2个代表模型：1.1 llama2llama2是meta最新开源的语言大模型，训练数据集2万亿token，上下文长度是由llama的2048扩展到4096，可以理解和生成更长的文本，包括7B、13B和70B三个模型，在各种基准集的测试上表现突出，最重要的是，该模型可用于研究和商业用途。关于llama2，具体可以参考以下文章：yeyan：【llm大语言模型】一文看懂llama2(原理,模型,训练)yeyan：【llm大语言模型】code llama详解与应用1.2 通义千问通义千问是阿里开源的大语言模型，参数规模为70亿（7B）和140亿（14B）。开源包括基础模型Qwen，即Qwen-7B和Qwen-14B，以及对话模型Qwen-Chat，即Qwen-7B-Chat和Qwen-14B-Chat。具体参考官方文档【千问中文文档】。同时，千问还开源了多模态大模型，具体看【继70亿参数大模型后，阿里云又开源通义千问「多模态大模型」，如何看待这一举措？将对行业产生哪些影响？】2.视觉大模型视觉大模型主要提供视觉的基础模型，目前比较有代表的就是sam。2.1 sam在网络数据集上预训练的大语言模型具有强大的zero-shot(零样本)和few-shot(少样本)的泛化能力，这些\"基础模型\"可以推广到超出训练过程中的任务和数据分布。sam就是利用sa1b数据集，实现了无监督的分割图像的基础模型，分割效果非常好。具体参考文章：yeyan：【论文解读】MetaAi SAM(Segment Anything) 分割一切yeyan：Segment Anything(sam)项目整理汇总[2023.9.2]2.2 dinov2dinov2是一种自监督方法，基于其LVD-142M数据集，通过在大型数据集上预训练图像编码器，获得具有语义的通用视觉特征，这些特征可用于广泛的视觉任务，不用微调，就可以获得与有监督模型相当的性能，dinov2结合一个简单的线型分类器，就能在分割任务达到很好的效果,算是对sam的一个补充。支持深度估计、语义分割、图片检索等任务。深度估计语义分割实例检索具体参考【如何评价Meta最新发布的DINOv2：无需监督学习稳健的视觉特征？】2.3 InternImage-书生通用视觉模型书生通用视觉大模型，参数高达30亿，图像分类标杆数据集ImageNet 90.1% Top1准确率，开源模型中准确度最高，物体检测标杆数据集COCO 65.5 mAP。各视觉任务的指标如下图，基本都是sota或者接近sota。参考链接：书生大模型中文文档",
        "voteup_count": 5,
        "updated_time": "2023-11-06 19:54:56",
        "question_id": 623672939,
        "user_id": "3ecbd82685cc5122ac602c81f72fd742"
    },
    {
        "answer_id": 3240314843,
        "content": "国外大模型：llama2 7,13,70b调试好的：xwin70bsynthia-13b还有合成模型：https://huggingface.co/Undi95/MLewd-ReMM-L2-Chat-20B-GGUFTheBloke/Speechless-Llama2-Hermes-Orca-Platypus-WizardLM-13B-GGUFfalcon180b:还没调试好的，如果调试好那么接近gpt4也是可能的mistral-7b调试好的:mistral-openorca开源大模型每天都有进展，过一两周就会有惊喜。",
        "voteup_count": 8,
        "updated_time": "2023-10-07 19:16:58",
        "question_id": 623672939,
        "user_id": "4d70177af48d36b23d5c66459f1dbfa8"
    },
    {
        "answer_id": 3483832985,
        "content": "国内外开源的大模型那可太多了！好比国外马斯克家的GROK、谷歌家的Gemma、Meta家的LLama3就都比较有名；国内提到开源大模型的话，那智谱的ChatGLM3、百川智能的Baichuan2、元象的XVERSE也都榜上有名~像最近刚刚新鲜出炉且比较有看头的还有微软家开源的生成式AI模型Phi-3系列。小而精的这一开源AI生成式模型不仅具备“苗条的身材”，同时也有着良好的性能~作为当下开源模型中最小规模的AI模型系列，其中最小参数量的Phi-3 mini在各项专业基准测试、跑分性能中，更是成功超越了Mixtral8x7B和GPT-3.5等业界知名模型。可见这两年来，开源大模型的发展和突破可丝毫不输给闭源赛道~不过不管是开源也好，闭源也好，终究还是得落到实处才是有用的~伴随着开源大模型的稳步前行，国内外也在此基础上衍生推出了许多实用AI落地工具，像下面提到的这4个就是很好的实例：「1」AI写作宝✨多功能型AI综合百宝袋入门AI工具不知道选什么，那这一堪称“六边形战士”的免费国产AI百宝袋就值得闭眼冲。写文创作、绘图生成、对话问答在这软件中就能一并得到实现~每日刷新登录即可GET到免费的使用机会，既不用付费，也不要求网络环境，简直不要太香！既然它都叫AI写作宝了，那“AI写作”必然是它的拿手好戏~支持辅助创作的文本类型有很多，活动策划、毕业论文、述职报告、种草笔记、视频脚本等等，多个领域的使用需求都能一键安排到位。「2」功夫量化✨专业级别的数据分析金融人士值得码住收藏的一个AI数据分析工具，不需要懂得编程也照样可以通过简单的交互问答实现数据的快速分析与决策。不仅是专业金融分析师可以利用它来进行更加高效的数据分析与处理，就连没什么经验的普通投资者也可以借助它快速捕获市场信息、做出更为明智的投资决策。Windows、macOS、Linux以及手机小程序多个端口均可打开使用，登录上账号还能云同步多个平台上的历史信息~「3」花生图像✨AI商品设计&抠图去背景有点AI版PS那味的一个在线AI图片编辑器，诸如PS可实现的抠图、消除笔、图片设计、文本添加等操作它一样可以满足，而且操作起来还要更加的简单好上手~绝大部分的功能选项都能一步到位，几乎不怎么需要手动去处理，对于美工设计岗的小伙伴而言，它属实是个提升效率不可或缺的得力助手~简化修图流程、优化图像处理效果、提升设计效率它可是专业的！「4」小微助手✨桌面端AI效率工具现在的智能手机自带有各种智能语音助手，电脑端自然也不落后~出自微信家族的这一桌面端AI交互效率工具，就内置有Json魔方、Base64工具、密码工具、剪切板管理、深入信息检索等系列功能。实际上你可以直接把它看做是一个搭载了AI的综合工具箱，平时在电脑端的各项操作都可以利用它来实现快捷输出，确实就还挺便捷好用的。那今天的分享就先到这~每天一个摸鱼小技巧，跟 @一级摸鱼选手小谢 争做快乐打工人！",
        "voteup_count": 2,
        "updated_time": "2024-04-30 15:45:09",
        "question_id": 623672939,
        "user_id": "74bad1a96e0d83d68834aaa51e5431c6"
    },
    {
        "answer_id": 3318607531,
        "content": "下面这张图，一图可回顾过去开源和闭源的大语言模型。ChatGPT在2022年底发布后，在AI领域产生了巨大的变革，不论是在学术研究还是商业方面都有所体现。通过使用监督微调和从人类反馈中进行强化学习来调整大型语言模型（LLM），ChatGPT展示了模型能够在广泛的任务中回答人类问题并回答很到位。在这一成功之后，LLM的兴趣持续增长，包括在学术界和工业界频繁涌现许多新的LLM，其中许多初创公司专注于LLM。尽管闭源LLM（例如OpenAI的GPT，Anthropic的Claude）通常在性能上优于其开源对手，但后者的进展迅猛，声称在某些任务上实现了与ChatGPT相当甚至更好的表现。这不仅对研究产生了重要影响，也对业务产生了关键影响。在ChatGPT发布一周年之际，本文对这一成功进行了详尽的概述，调查了所有开源LLM声称与ChatGPT相当或更好的任务。如下图3所示。AI生成未来：ChatGPT成立一周年：开源大语言模型正在迎头赶上吗？1 引言恰好一年前，OpenAI发布了ChatGPT，这一事件在AI社区和更广泛的世界引起了轰动。首次，一款基于应用的AI聊天机器人能够通常提供有帮助、安全且详细的答案，遵循指令，甚至能够承认并纠正其先前的错误。值得注意的是，它可以执行那些传统上由预训练然后经过定制微调的语言模型执行的自然语言任务，比如摘要或问答，似乎表现得非常出色。作为首个具有这种功能的聊天机器人，ChatGPT吸引了广大公众——在推出仅两个月内就达到了1亿用户，远远快于TikTok或YouTube等其他热门应用。它还吸引了巨额商业投资，因为它有潜力降低劳动成本，自动化工作流程，甚至为客户带来新的体验。然而，由于ChatGPT没有开源，其访问受到私人公司的控制，大部分技术细节仍然未知。尽管声称ChatGPT遵循InstructGPT（也称为GPT-3.5）介绍的过程，其精确的架构、预训练数据和微调数据都是未知的。这种闭源性质带来了一些关键问题。首先，在不知道内部细节（如预训练和微调过程）的情况下，很难正确评估其对社会的潜在风险，尤其是考虑到LLMs可以惯例地生成有毒、不道德和不真实的内容。其次，据报道ChatGPT的性能随时间变化，阻碍了可重复的结果。再者，ChatGPT经历了多次停机，在2023年11月仅发生了两次重大停机，期间ChatGPT网站和其API的访问完全被阻止。最后，采用ChatGPT的企业可能会担心调用API的巨大成本、服务中断、数据所有权和隐私问题，以及像最近关于CEO Sam Altman被解雇与员工反叛董事会等戏剧性事情，以及他最终的回归这样的不可预测的事件。开源LLMs是一个有望解决或规避大多数上述问题的方向。因此，研究界一直在努力维护高性能的开源LLMs。截至目前，人们普遍认为，开源LLMs，如Llama-2或Falcon落后于其闭源对手，如OpenAI的GPT3.5（ChatGPT）和GPT-4（OpenAI，2023b），Anthropic的Claude2或Google的Bard3，其中GPT-4通常被认为在截至2023年底的时候是最强的。然而，差距越来越小，开源LLMs正在迅速赶超。实际上，正如下图1所示，对于某些任务，最好的开源LLMs已经超过了GPT-3.5-turbo。然而，这对于开源LLMs来说并非一帆风顺的挑战。LLMs的领域不断发展：闭源LLMs定期在更新的数据上进行重新训练，开源LLMs几乎每周发布一次，有大量的评估数据集和基准用于比较LLMs，使得找出最佳LLM尤其具有挑战性。在这份调查中，目标是整合最近关于开源LLMs的论文，并提供在各个领域与ChatGPT匹敌或超越它的开源LLMs的概览。贡献有三个方面：• 整合各种对开源LLMs的评估，提供对比较公正和全面的开源LLMs与ChatGPT的视角。• 系统地回顾在各种任务中超越或赶超ChatGPT的开源LLMs（如下图2所示），并进行分析。• 提供关于开源LLMs发展趋势、训练开源LLMs的良好实践以及开源LLMs可能面临的问题的见解。这份调查旨在成为研究界和商业领域的关键资源，帮助他们了解开源LLMs当前的情况和未来的潜力。对于研究人员来说，它提供了开源LLMs当前进展和不断发展趋势的详细综合，突显了未来研究的有前途的方向。对于商业领域，这份调查提供了有价值的见解和指导，协助决策者评估采用开源LLMs的适用性和益处。 在接下来的文章中，将首先介绍背景先决条件，然后深入审查在各个领域击败ChatGPT的开源LLMs，接着讨论开源LLMs的见解和问题，最后总结。2 背景2.1 训练模式「预训练」 所有LLMs都依赖于大规模的自监督预训练，使用互联网文本数据（Radford et al., 2018; Brown et al., 2020）。仅有解码器的LLMs遵循因果语言建模目标，通过该目标，模型学会在给定先前tokens序列的情况下预测下一个tokens。根据开源LLMs分享的预训练细节，文本数据的来源包括CommonCrawl5、C4、GitHub、Wikipedia、图书以及在线讨论交流，如Reddit或StackOverFlow。众所周知，扩大预训练语料库的规模可以提高模型的性能，并且与扩大模型规模相辅相成，这一现象被称为规模定律，并在（Hoffmann et al., 2022a）中进行了深入分析。现代LLMs预训练的语料库从数千亿到数万亿tokens不等。「微调」 微调旨在通过使用可用监督来适应预训练的LLM到下游任务，通常形成的数据集比用于预训练的数据集小几个数量级。T5是最早将微调框架纳入文本到文本统一框架中的模型之一，其中自然语言说明描述了每个任务。后来，通过使用自然语言说明描述的几个任务联合训练，将微调扩展为指令调整。由于指令调整能够极大地改善LLMs的零样本性能，包括在新任务上（在训练期间未见过的任务），尤其是在较大的模型规模下，因此指令调整迅速走红。标准的指令调整与多任务监督微调可能仍然无法使模型遵循人类意图，并且可以通过人类反馈强化学习（RLHF）进行改进：人类标注者对经过微调模型的输出进行排名，然后使用这些排名进行强化学习微调。最近的研究表明，人类反馈可以用LLM的反馈来替代，这个过程被称为从AI反馈中学习的强化学习（RLAIF）。一系列工作侧重于在构建多样任务的指令调整数据集时质量而不是数量：Lima在仅使用1,000个样本对Llama-65B进行微调时的性能优于GPT-3，而Alpagasus通过从52k个样本中清理其指令微调数据集，使其减少到9k个样本，从而改进了Alpaca。「持续预训练」 持续预训练是指在来自预训练LLM的模型的基础上进行另一轮预训练，通常使用比第一阶段少量的数据。这样的过程可能有助于在LLM中引出新的特性。例如，Lemur使用持续预训练来提高编码和推理能力，Llama-2-long用于扩展上下文窗口。「推理」 存在几种用LLM进行序列生成的替代方法，这些方法通过自回归解码在输出中的随机性和多样性程度而有所不同。在采样过程中增加温度会使输出更加多样化，而将其设置为0则会回到贪婪解码，这在需要确定性输出的情景中可能是必要的。采样方法top-k和top-p在每个解码步骤约束要采样的tokens池。2.2 任务领域和评估由于要执行的评估多样且广泛，因此正确评估LLMs的能力仍然是一个活跃的研究领域。问答数据集是非常受欢迎的评估基准，但最近还出现了专为LLM评估定制的新基准。在接下来的部分中，将探讨LLMs在6个主要维度上的能力： 通用能力，Agent能力，逻辑推理（包括数学和编码能力），长上下文建模，特定应用，如问答或摘要，以及可信度。3 开源LLMs vs ChatGPT3.1 通用能力基准由于每周都有大量LLMs发布，每个都声称具有卓越性能，因此要识别真正的进展和领先模型变得具有挑战性。因此，全面评估这些模型在广泛的任务领域中的性能以了解其通用能力至关重要。本节涵盖了使用基于LLM的（例如GPT-4）和传统的（例如ROUGE和BLEU）评估指标的基准。• 「MT-Bench」 旨在从八个角度测试多轮对话和遵循指令的能力，包括写作、角色扮演、提取、推理、数学、编码、知识I（STEM）和知识II（人文/社会科学）。更强大的LLMs被用作该基准的评估模型的评委。• 「AlpacaEval」 是基于AlpacaFarm评估集的LLM自动评估器，用于测试模型遵循一般用户指令的能力。它通过使用更强大的LLMs（例如GPT-4和Claude）对候选模型与Davinci-003响应进行基准测试，生成候选模型的获胜率。• 「Open LLM Leaderboad」 使用Language Model Evaluation Harness在七个关键基准上评估LLMs，包括AI2 Reasoning Challenge、HellaSwag、MMLU、TruthfulQA、Winogrande、GSM8K和DROP。该框架在零样本和少样本设置下评估LLMs在各种推理和一般知识领域的能力。• 「BIG-bench」 是一个协作基准，旨在探讨LLMs并推断它们未来的能力。它包括200多个新颖的语言任务，涵盖了各种主题和语言，这些任务对现有模型来说并非完全可解。• 「ChatEval」 是一个多Agent辩论框架，使多Agent裁判团队能够自主讨论和评估不同模型对开放式问题和传统自然语言生成任务的生成响应的质量。• 「FairEval-Vicuna」 在Vicuna Benchmark的80个问题上使用多证据校准和平衡的位置校准。FairEval-Vicuna提供了更加公正的评估结果，在采用LLMs作为评估器的范 paradigm 内与人类判断密切相关。LLMs的性能Llama-2-70B是一款杰出的开源LLM，已在包含两万亿tokens的大规模数据集上进行了预训练。它在各种通用基准测试中展现出卓越的结果。当使用指导数据进行进一步的微调时，Llama-2-chat-70B变体在一般对话任务中展示出增强的能力。特别是，Llama-2-chat-70B在AlpacaEval中实现了92.66%的胜率，超过了GPT-3.5-turbo 10.95%。然而，GPT-4仍然是所有LLMs中表现最佳的，胜率为95.28%。Zephyr-7B是另一款较小的模型，使用蒸馏的直接偏好优化，在AlpacaEval上取得了与70B LLMs相媲美的结果，胜率为90.6%。它甚至在MT-Bench上超过了Llama-2-chat-70B，得分为7.34，而Llama-2-chat-70B得分为6.86。此外，WizardLM-70B已经使用大量的指导数据进行了指导微调，涵盖了不同复杂性的任务。它在MT-Bench上以7.71的分数脱颖而出。然而，这仍然略低于GPT-3.5-turbo（7.94）和GPT-4（8.99）的分数。尽管Zephyr-7B在MT-Bench中表现出色，但在开源LLM Leaderboard上表现不佳，仅得到52.15%的分数。另一方面，GodziLLa2-70B是一款实验性模型，将来自Maya Philippines的各种专有LoRAs和Guanaco Llama 2 1K数据集与Llama-2-70B结合使用，实现了在开源LLM Leaderboard上更具竞争力的67.01%的得分。这一表现可与GPT-3.5-turbo相媲美，后者在该领域的得分为70.21%。然而，两者仍然明显落后于GPT-4，后者以85.36%的高得分领先。UltraLlama利用具有增强多样性和质量的微调数据。在其提出的基准测试中，它与GPT-3.5-turbo的性能相匹敌，并在世界和专业知识领域超过了它。3.2 Agent能力基准测试随着模型规模的不断扩大，基于LLM的Agent引起了自然语言处理社区的极大关注。鉴于此，在各种基准测试中调查了开源LLMs的Agent能力。根据所需的技能，现有的基准测试主要可以分为四类。• 使用工具：一些基准测试旨在评估LLMs的工具使用能力。「API-Bank」 专门为工具增强的LLMs设计。「ToolBench」是一个包含各种实际任务的软件工具的工具操作基准测试。「APIBench」 包含来自HuggingFace、TorchHub和TensorHub的API。「ToolAlpaca」 通过多Agent模拟环境开发了一个多样且全面的工具使用数据集。巧合的是，使用ChatGPT构建的用于工具使用的指导微调数据集也被命名为「ToolBench」。此外，「MINT」 可以评估LLMs在使用工具解决需要多轮交互的任务时的熟练程度。• 自我调试：有几个数据集可用于评估LLMs进行自我调试的能力，包括「InterCode-Bash」和「InterCode-SQL」，「MINT-MBPP」和「MINT-HumanEval」以及「RoboCodeGen」。• 遵循自然语言反馈：MINT还可以用于衡量LLMs利用自然语言反馈的能力，通过使用GPT-4模拟人类用户。• 探索环境：「ALFWorld」，「InterCode-CTF」和「WebArena」旨在评估基于LLMs的Agent是否能够从环境中收集信息并做出决策。LLMs的性能通过使用包含90B tokens的代码密集语料库对Llama-2进行预训练，并在包含30万个文本和代码样本的指导微调中，Lemur-70B-chat在探索环境或在编码任务中遵循自然语言反馈时超过了GPT-3.5-turbo的性能，如下表2所示。AgentTuning使用Llama-2在其构建的AgentInstruct数据集和通用领域指导的组合上进行指导微调，形成AgentLlama。值得注意的是，AgentLlama-70B在未见过的Agent任务上实现了与GPT-3.5-turbo相媲美的性能。通过在ToolBench上对Llama-2-7B进行微调，ToolLLaMA在工具使用评估中表现出与GPT-3.5-turbo相媲美的性能。Chen等人介绍了FireAct，可以对Llama-2-13B进行微调，以在HotpotQA上超越GPT-3.5-turbo。此外，从Llama-7B进行微调的Gorilla在编写API调用方面优于GPT-4。3.3 逻辑推理能力基准测试逻辑推理是高层次能力和技能的基本能力，例如编程、定理证明以及算术推理。为此，在本节中，将介绍以下基准测试：• 「GSM8K」 包含由人类问题作者创建的8.5K个高质量小学数学问题。这些问题需要2到8个步骤来解决，解决方案主要涉及使用基本算术运算执行一系列基本计算，以达到最终答案。• 「MATH」 是一个包含12,500个具有挑战性的竞赛数学问题的数据集。MATH中的每个问题都有一个完整的分步解决方案，可用于教模型生成答案的推导和解释。• 「TheoremQA」 是一个定理驱动的问答数据集，旨在评估AI模型将定理应用于解决具有挑战性的科学问题的能力。TheoremQA由领域专家策划，包含800个高质量问题，涵盖了数学、物理、电子工程与计算机科学以及金融领域的350个定理。• 「HumanEval」 是一组164个手写编程问题。每个问题包括一个函数签名、文档字符串、主体和几个单元测试，平均每个问题有7.7个测试。• 「MBPP」 （主要是基本编程问题）数据集包含由对Python具有基本知识的内部众包工人进行众包构建的974个短Python程序。每个问题都分配有一个解决指定问题的独立的Python函数，并包含三个测试用例，用于检查函数的语义正确性。• 「APPs」 是一个用于代码生成的基准测试，衡量模型根据任意自然语言规范生成令人满意的Python代码的能力。该基准测试包括10,000个问题，从具有简单一行解决方案到具有实质性算法挑战的问题不等。强化指导调整与传统的基于知识蒸馏的指导调整不同，Luo等人采用了Evol-Instruct构建了任务特定的高质量指导调整数据集，其中种子指导发展成知识边界或任务复杂性深度扩展的指导。此外，Luo等人还结合了PPO算法，进一步提高了生成的指导和答案的质量。在获得扩展的指导池后，通过收集来自另一个LLM（例如GPT-3.5-turbo）的响应生成新的指导调整数据集。最终，由于Query深度和宽度的发展，经过精细调整的模型的性能甚至优于GPT-3.5-turbo。例如，WizardCoder在HumanEval上表现优异，相对于GPT3.5-turbo有19.1％的绝对改进。而WizardMath相对于GPT-3.5-turbo也取得了42.9％的绝对改进。在更高质量数据上的预训练Lemur已验证了在自然语言数据和代码之间更好的混合，并使LLMs在函数调用、自动编程和Agent方面具有更强的能力。具体而言，Lemur-70B在没有任务特定的精细调整的情况下，在HumanEval和GSM8K上相对于GPT-3.5-turbo取得了显著的改进。Phi采用了不同的方法，使用教科书作为主要的预训练语料库，这使得在更小的语言模型上观察到了强大的能力。3.4 建模长上下文能力基准测试处理长序列仍然是LLMs的关键技术瓶颈之一，因为所有模型都受到有限的最大上下文窗口的限制，通常长度从2k到8k个tokens不等。对LLMs的长上下文能力进行基准测试涉及对一些自然具有长上下文的任务进行评估，例如提要或多文档QA。已经为LLMs的长上下文评估提出了以下基准测试：「SCROLLS」 是一个由7个具有自然长输入的数据集组成的流行评估基准。任务涵盖提要、问答和自然语言推理。「ZeroSCROLLS」 在SCROLLS的基础上构建（舍弃了ContractNLI，重用其他6个数据集，并添加了4个数据集），仅考虑零样本设置，评估LLMs的即插即用性。「LongBench」 设定了一个包含21个数据集跨6个任务的双语英语/中文长上下文基准。「L-Eval」 重复使用了16个现有数据集，并从头开始创建了4个数据集，形成一个多样化的、长上下文的基准，每个任务的平均长度超过4k个tokens。作者主张使用LLM评价（特别是GPT-4）而不是N-gram进行长上下文评估。「BAMBOO」 创建了一个专注于长上下文LLM评估基准，重点是通过仅收集评估数据集中的最新数据来消除预训练数据的污染。「M4LE」 引入了一个广泛的基准测试，将36个数据集分为5个理解能力：显式单跨度、语义单跨度、显式多跨度、语义多跨度和全局理解。模型在LongBench、L-Eval、BAMBOO和M4LE基准测试中，GPT-3.5-turbo或其16k版本在很大程度上优于所有开源LLMs，如Llama-2、LongChat或Vicuna；表明在长输入任务上提高开源LLMs的性能并非易事。Llama-2-long在Llama-2上使用400Btokens进行更长的预训练（从Llama-2的4k窗口增加到16k）。由此产生的Llama-2-long-chat-70B在ZeroSCROLLS上相对于GPT-3.5-turbo-16k的得分为37.7比36.7。解决长上下文任务的方法包括通过位置插值扩展上下文窗口，其中包括使用更长上下文窗口进行另一轮（短）微调；和检索增强，需要访问检索器以查找相关信息。Xu等人结合了这两种看似相反的技术，将Llama-2-70B推动到GPT-3.5-turbo-16k的平均水平上，在7个长上下文任务中（包括ZeroSCROLLS的4个数据集）上表现优于GPT-3.5-turbo-16k。3.5 应用特定能力这一部分将讨论LLMs在处理特定应用程序时所需的能力。3.5.1 Query焦点摘要「基准测试」Query焦点或基于外表的摘要需要根据一个细粒度的问题或一个方面类别生成摘要。Query焦点数据集包括AQualMuse、QMSum和SQuALITY，而基于方面的数据集包括CovidET、NEWTS、WikiAsp等。「模型」（Yang等人，2023d）发现，与ChatGPT相比，对训练数据的标准微调在性能上仍然更好，对于CovidET、NEWTS、QMSum和SQuALITY的ROUGE-1平均提高了2个点。3.5.2 开放式问答「基准测试」开放式问答有两个子类别：答案要么是短格式，要么是长格式。短格式数据集包括SQuAD 1.1、NewsQA、TriviaQA、SQuAD 2.0、NarrativeQA、Natural Question（NQ）、Quoref和DROP。长格式数据集包括ELI5和doc2dial。对于短格式和长格式数据集，评估指标是答案中的精确匹配（EM）和F1。「模型」InstructRetro在NQ、TriviaQA、SQuAD 2.0和DROP上相对于GPT-3取得了显著改进，同时与类似大小的专有GPT-instruct模型相比，在一系列短格式和长格式的开放式问答数据集上提高了7-10％。InstructRetro从预训练的GPT模型初始化，然后继续通过检索进行预训练，然后经过指导调整。3.5.3 医学「基准测试」LLMs的一个理想能力是在医学相关任务上做出贡献，以使负担得起的、高质量的医疗更容易接触到更广泛的公众。对于心理健康，IMHI基准测试是使用10个现有的心理健康分析数据集构建的，包括心理健康检测：DR、CLP、Dreaddit、孤独、SWMH和T-SID；心理健康原因检测：SAD、CAMS；心理风险因素检测：MultiWD、IRF。对于放射学，OpenI数据集和MIMIC-CXR数据集都包含具有发现和印象文本的放射学报告。「模型」对于心理健康，MentalLlama-chat-13B在IMHI训练集上对Llama-chat-13B模型进行微调。 MentalLlama-chat-13B模型在零样本提示下在IMHI的10项任务中，相对于ChatGPT的few-shot提示或零样本提示，在9项任务上表现优于ChatGPT。Liu等人提出对Llama检查点进行微调，以生成放射学报告发现的印象文本。所得的Radiology-Llama-2模型在MIMIC-CXR和OpenI数据集上相对于ChatGPT和GPT-4都取得了较大的优势。3.5.4 生成结构化响应在按照指令生成格式化响应是支持Agent能力或简化解析或翻译模型响应的手动工作的核心能力。「基准测试」Rotowire包含NBA比赛摘要及相应的比分表。Struc-Bench引入了两个数据集：Struc-Bench-Latex，其输出为Latex格式的表格，以及Struc-Bench-HTML，其输出为HTML格式的表格。「模型」Struc-Bench在结构化生成数据上对Llama-7B模型进行了微调。在上述所有基准测试中，经过微调的7B模型的性能均优于ChatGPT。3.5.5 生成评论「基准测试」LLMs的一个有趣能力是为问题的响应提供反馈或评论。为了评估这种能力，可以使用人工标注员或GPT-4作为评估器来直接评估评论。原始问题可以来自上述其他能力的任何数据集。「模型」Shepherd是一个从Llama-7B初始化的7B模型，经过社区收集的评论数据和1,317个高质量人工标注数据的训练。Shepherd在各种不同的NLP数据集上生成评论：AlpacaFarm、FairEval、CosmosQA、OBQA、PIQA、TruthfulQA和CritiqueEval。通过使用GPT-4作为评估器，Shepherd在60%以上的情况下赢得或与ChatGPT相等。在人类评估员的评估中，Shepherd几乎与ChatGPT持平。3.6 迈向可信赖的人工智能为了确保LLMs在实际应用中能够得到人类的信任，一个重要的考虑因素是它们的可靠性。例如，对于错觉（Ye＆Durrett，2022；Zhao等人，2023）和安全性（Zhiheng等人，2023b）的担忧可能降低用户对LLMs的信任，并导致在高影响应用中存在风险。3.6.1 错觉「基准测试」已经有各种基准测试，以更好地评估LLMs中的错觉。具体而言，它们包括大规模数据集、自动化度量和评估模型。• TruthfulQA是一个问答（QA）基准数据集，包含涵盖38个类别的问题。这些问题被设计成一些人由于误解而错误回答。• FactualityPrompts是一个测量开放式生成中错觉的数据集。它包含事实和非事实提示，以研究提示对LLM继续的影响。• HaluEval是一个包含生成的和人工标注的虚构样本的大型数据集。它涵盖了三个任务：问答、基于知识的对话和文本摘要。• FACTOR提出了一种可伸缩的评估语言模型事实性的方法：它自动将一个事实语料库转化为一个忠实度评估基准。该框架用于创建两个基准：Wiki-FACTOR 和 News-FACTOR。• KoLA构建了一个面向知识的语言模型评估基准（KoLA），其中包含三个关键因素：模仿人类认知以进行能力建模，使用维基百科进行数据收集，并为自动虚构评估设计对比指标。• FActScore提出了一种新的评估方法，首先将语言模型的生成分解为一系列原子事实，然后计算由可靠知识源支持的原子事实的百分比。• Vectara的错觉评估模型是一个小型语言模型，经过二进制分类器的微调，用于将摘要分类为与源文档一致（或不一致）。然后，它用于评估和基准测试各种LLMs生成的摘要的错觉。• FacTool是一个用于检测由LLMs生成的文本的事实错误的任务和领域不可知框架。除了新引入的错觉基准测试之外，以实际知识为基础的先前问答（QA）数据集也被广泛用于衡量忠实度，如HotpotQA、OpenBookQA、MedMC-QA和TriviaQA。除了数据集和自动化指标外，人工评估也被广泛采用作为忠实度的可靠度量。「模型」存在一些关于错觉的现有调查（Zhang等人，2023b；Rawte等人，2023），详细调查了潜在的方法。具体而言，超越当前GPT-3.5-turbo性能的方法可以在微调期间或仅在推理时进行。选择的性能指标显示在下表3中。在微调期间，通过提高正确性和相关性的数据质量可以导致更少的错觉模型。Lee等人（2023a）策划了一个内容过滤、以STEM领域高质量数据为重点的调整数据集。一系列LLMs在这个经过过滤的数据集上进行微调并合并。结果产生的系列，名为Platypus，与GPT-3.5-turbo相比，在TruthfulQA上实现了相当大的改进（约20%）。在推理期间，现有的技术包括特定的解码策略、外部知识增强和多Agent对话。对于解码，Dhuliawala等人（2023）介绍了Chain-of-Verification（CoVe），其中LLM起草验证问题并自我验证响应。CoVe在FactScore上相对于GPT-3.5-turbo实现了相当大的改进。对于外部知识增强，各种框架包含不同的搜索和提示技术，以当前提高GPT-3.5-turbo性能。Li等人（2023c）设计了Chain-of-Knowledge（CoK），在回答之前从异构知识源中检索。Peng等人（2023）提出了LLM-AUGMENTER，该方法使用一组即插即用的模块增强LLMs，并通过由效用函数生成的反馈迭代地修订LLM提示，以改进模型响应。Knowledge Solver（KSL）试图通过利用它们自己的强大泛化能力，教会LLMs从外部知识库中搜索基本知识。CRITIC允许LLM验证和逐渐修正其自己的输出，方式类似于人类与工具的交互。Luo等人（2023b）介绍了Parametric Knowledge Guiding（PKG）框架，该框架配备了一个知识引导模块，以访问相关知识而不更改LLMs的参数。这些推理技术然后相对于使用GPT-3.5-turbo的简单提示策略改善了答案的准确性。目前，GPT-3.5-turbo还已经整合了一个检索插件（OpenAI，2023a），以访问外部知识以减少错觉。对于多Agent对话，Cohen等人（2023）促进了生成声明的Examinee LLM与引入问题的另一个Examiner LLM之间的多轮交互。通过交叉审订过程，改善了各种QA任务的性能。Du等人（2023）要求多个语言模型实例提出和辩论他们各自的响应和推理过程，经过多轮的辩论达成共同的最终答案，从而改善了多个基准。3.6.2 安全性「基准测试」在LLMs中，安全性问题主要可以分为三个方面（Zhiheng等人，2023a）：社会偏见、模型鲁棒性和中毒问题。为了收集更好地评估上述方面的数据集，提出了几个基准测试：• SafetyBench是一个数据集，包括11,435个涵盖7个不同安全问题类别的多元选择问题。• Latent Jailbreak引入了一个基准测试，评估LLMs的安全性和鲁棒性，强调了需要采用平衡的方法。• XSTEST是一个系统地识别夸张安全行为的测试套件，例如拒绝安全提示。• RED-EVAL是一个基准测试，执行红队行动，使用基于Chain of Utterances（CoU）的提示对LLMs进行安全评估。除了自动化基准测试，安全性的一个重要度量是人工评估（Dai等人，2023），其中众包工作者将响应tokens为安全或有害。一些研究还尝试从GPT-4中收集这些标签，因为研究表明它可以取代人类评估者来评估对齐能力（Chiang＆Lee，2023）。「模型」基于当前的评估，GPT-3.5-turbo和GPT-4模型在安全性评估中仍然名列前茅。这主要归因于人工强化学习（RLHF）。RLHF首先在响应上收集人类偏好数据集，然后训练一个奖励模型来模仿人类偏好，最后使用RL来训练LLM以与人类偏好保持一致。在这个过程中，LLMs学会了展示所需的行为，排除了有害的响应，如不礼貌或有偏见的回答。然而，RLHF程序需要收集大量昂贵的人工标注，这阻碍了它在开源LLMs中的使用。为了推动LLMs的安全对齐的努力，Ji等人（2023）收集了一个人类偏好数据集，以将无害性和有用性从人类偏好分数中解开，从而为这两个度量提供独立的排名数据。实验证明，解开人类偏好可以增强安全对齐。Bai等人（2022b）试图通过来自AI反馈的RL（RLAIF）来提高安全性，其中偏好模型使用LLM生成的自我评论和修订进行训练。直接偏好优化（DPO）减少了学习奖励模型和直接使用简单的交叉熵损失从偏好中学习的需要，这在很大程度上可以减少RLHF的成本。结合和改进这些方法可能会在开源LLMs的安全性方面带来潜在的改进。4 讨论4.1 LLMs的发展趋势自从Brown等人（2020）展示了冻结的GPT-3模型在各种任务上可以实现令人印象深刻的零样本和少样本性能以来，人们已经付出了大量努力推动LLMs的发展。一方面的研究集中在扩大模型参数的规模，包括Gopher，GLaM，LaMDA，MT-NLG和PaLM，最终达到了540B参数。尽管展现出卓越的能力，但这些模型的闭源性质限制了它们的广泛应用，因此越来越多的人开始对开源LLMs的发展产生兴趣。与扩大模型规模不同，另一方面的研究探索了更好的策略或目标，以预训练较小模型，如Chinchilla和UL2。在预训练之外，人们还致力于研究LMs的指导调整，例如FLAN，T0和Flan-T5。一年前，OpenAI的ChatGPT的出现极大地改变了NLP社区的研究重点。为了赶上OpenAI，Google和Anthropic分别推出了Bard和Claude。尽管它们在许多任务上显示出与ChatGPT相媲美的性能，但它们与最新的OpenAI模型GPT-4之间仍然存在性能差距。由于这些模型的成功主要归功于人类反馈的强化学习（RLHF），研究人员已经探讨了改进RLHF的各种方式。为了促进开源LLMs的研究，Meta发布了Llama系列模型。自那以后，基于Llama的开源模型开始呈爆炸式增长。一个典型的研究方向是使用指导数据微调Llama，包括Alpaca，Vicuna，Lima和WizardLM。进行中的研究还探讨了提高Llama基于开源LLMs的Agent，逻辑推理和长上下文建模能力。此外，与基于Llama开发LLMs不同，还有许多努力致力于从头开始训练强大的LLMs，例如MPT，Falcon，XGen，Phi，Baichuan，Mistral，Grok和Yi。我们相信，开发更强大、更高效的开源LLMs，以使封闭源LLMs的能力得以民主化，应该是一个非常有前途的未来方向。4.2 结果总结就通用能力而言，Llama-2-chat-70B在一些基准测试中显示出对GPT-3.5-turbo的改进，但在大多数其他测试中仍然落后。Zephir-7B通过蒸馏直接偏好优化接近70B LLMs。WizardLM70B和GodziLLa-70B能够达到与GPT-3.5-turbo可比较的性能，展示了一个有希望的发展方向。在一些领域，开源LLMs能够超越GPT-3.5-turbo。对于基于LLM的Agent，开源LLMs通过更广泛和任务特定的预训练和微调能够超越GPT-3.5-turbo。例如，Lemur-70B-chat在探索环境和在编码任务中遵循反馈方面表现更好。AgentTuning在看不见的Agent任务上有所改进。ToolLLama能更好地掌握工具使用。Gorilla在编写API调用方面优于GPT-4。对于逻辑推理，WizardCoder和WizardMath通过增强的指导调整提高了推理能力。Lemur和Phi通过在质量较高的数据上进行预训练实现了更强大的能力。对于建模长上下文，Llama-2-long通过使用更长的tokens和更大的上下文窗口进行预训练，可以在选定的基准测试中改进。Xu等人（2023b）通过将上下文窗口扩展与位置插值和检索增强相结合，提高了7个长上下文任务的性能。对于特定应用能力，InstructRetro通过检索和指导调整进行预训练，在开放式QA方面有所改进。通过特定任务的微调，MentaLlama-chat13B在心理健康分析数据集中优于GPT-3.5-turbo。RadiologyLlama2可以提高在放射学报告上的性能。Stru-Bench，一个经过微调的7B模型，可以改善结构化响应生成，与GPT-3.5-turbo相比，这是支持Agent任务的核心能力。Shepherd，只有7B参数，可以在生成模型反馈和评论方面达到与GPT-3.5-turbo相媲美或更好的性能。对于值得信赖的AI，可以通过使用质量更高的数据进行微调，上下文感知的解码技术，外部知识增强，例如Li等人（2023c）；Yu等人（2023b）；Peng等人（2023）；Feng等人（2023），或多Agent对话来减少错觉。也有一些领域，GPT-3.5-turbo和GPT-4仍然无法匹敌，如AI安全。由于GPT模型涉及大规模的RLHF，它们以表现更安全和更具道德行为而闻名，这对于商业LLMs而言可能是比开源LLMs更重要的考虑因素。然而，随着最近在民主化RLHF过程方面的努力，可以期待在安全性方面看到更多开源LLMs的性能提升。4.3 最佳开源LLMs的配方训练LLM涉及到复杂且资源密集的实践，包括数据收集和预处理、模型设计以及训练过程。尽管有越来越多的趋势是定期发布开源LLMs，但领先模型的详细实践往往遗憾地被保密。下面列出了社区广泛认可的一些最佳实践。数据预处理预训练涉及使用数万亿数据tokens，通常来自公开可访问的来源。从伦理角度来看，排除包含个人信息的任何数据是至关重要的。与预训练数据不同，微调数据在数量上较小，但在质量上较高。使用高质量数据进行微调的LLMs表现出了改进的性能，特别是在专业领域。模型架构尽管大多数LLMs使用仅解码器的Transformer架构，但模型中采用了不同的技术来优化效率。Llama-2实施了Ghost attention以改进多轮对话控制。Mistral采用滑动窗口注意力来处理扩展上下文长度。训练使用指导调整数据进行监督微调（SFT）的过程至关重要。为了获得高质量的结果，SFT标注数以数万计就足够了，正如Llama-2所使用的27,540个标注一样。这些数据的多样性和质量至关重要。在RLHF阶段，近端策略优化（PPO）通常是更好地使模型的行为与人类偏好和指导保持一致的首选算法，对于增强LLM的安全性起着关键作用。替代PPO的方法是直接偏好优化（DPO）。例如，Zephyr-7B采用蒸馏的DPO，在各种通用基准测试中显示出与70B-LLMs相媲美的结果，甚至在AlpacaEval上超过了GPT-3.5-turbo。4.4 漏洞和潜在问题预训练期间的数据污染随着发布混淆其预训练语料库来源的基础模型，数据污染问题变得越来越明显。这种缺乏透明度可能导致对大型语言模型（LLMs）真正泛化能力的偏见。在忽略了将基准数据手动集成到训练集中的情况，且有人类专家或更大模型的标注，数据污染问题的根本原因在于基准数据的收集源已经包含在预训练语料库中。尽管这些模型并非有意使用监督数据进行预训练，它们仍然可以获得精确的知识。因此，解决检测LLMs预训练语料库的挑战，探索现有基准测试和广泛使用的预训练语料库之间的重叠，并评估对基准测试的过拟合问题变得至关重要。这些努力对于增强LLMs的忠实度和可靠性至关重要。展望未来，未来的方向可能包括建立披露预训练语料库详细信息的标准做法，并开发在整个模型开发生命周期中减轻数据污染的方法。对齐的闭源发展在社区中，使用一般偏好数据进行对齐的强化学习从人类反馈（RLHF）的应用引起了越来越多的关注。然而，由于缺乏高质量、公开可用的偏好数据集和预训练奖励模型，只有有限数量的开源LLMs已经通过RLHF进行了对齐。一些倡议试图为开源社区做出贡献。然而，仍然面临在复杂推理、编程和安全场景中缺乏多样性、高质量和可扩展的偏好数据的挑战。在基本能力上的持续改进的困难回顾本文中概述的基本能力的突破揭示了一些具有挑战性的情景：已经投入了相当大的努力来探索在预训练期间改进数据混合，以提高构建更强大基础模型的平衡性和鲁棒性。然而，相关的探索成本通常使这种方法变得不切实际。超越GPT-3.5-turbo或GPT-4的模型主要是基于从闭源模型进行知识蒸馏和额外专家标注。虽然高效，但在将这些方法扩展到教师模型时，对知识蒸馏效果的潜在问题可能会被掩盖。此外，LLMs预计将充当Agent并提供合理的解释以支持决策，然而为了使LLMs适用于现实世界的场景，标注Agent样式的数据也是昂贵且耗时的。实质上，仅通过知识蒸馏或专家标注的优化无法实现对基本能力的持续改进，并可能接近上限。未来的研究方向可能涉及探索新的方法，如无监督或自监督学习范式，以在缓解相关挑战和成本的同时实现基本LLM能力的持续改进。5 结论在这份调查中，对在ChatGPT发布一周年之际超越或赶上ChatGPT在各种任务领域中表现优异的开源LLMs进行了系统回顾。此外，提供了关于开源LLMs的见解、分析和潜在问题。这份调查为开源LLMs提供了有前途的方向，并将激发该领域更多的研究和开发，有助于缩小它们与付费同行之间的差距。参考文献[1] ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?链接：https://arxiv.org/pdf/2311.1698",
        "voteup_count": 4,
        "updated_time": "2023-12-08 21:36:39",
        "question_id": 623672939,
        "user_id": "40ba3b6fc933ec29876a5a8c8ef8d25c"
    },
    {
        "answer_id": 3483596647,
        "content": "标题：探索开源大模型的世界：AI的无限可能在人工智能的浩瀚星空中，开源大模型如同最璀璨的星辰，不仅引领着技术的革新，更激发了无数创新者的思维火花。让我们启程，深入探索这些令人瞩目的开源大模型。GPT-3：自然语言处理的巨人首先，我们不得不提及GPT-3，这个由OpenAI精心打造的自然语言处理（NLP）巨擘。它的强大能力让人惊叹，从撰写文章到编程，甚至进行哲学探讨，GPT-3似乎都能游刃有余。它的诞生，标志着AI领域的一大飞跃，为未来的语言处理技术奠定了新的里程碑。BERT：理解语言深层含义的先锋紧随其后的是BERT，谷歌开发的这款自然语言处理模型以其独特的双向理解能力而闻名。BERT的全称是Bidirectional Encoder Representations from Transformers，它能够深入理解语言的细微差别，尤其在处理复杂的语言任务时表现出色。其他值得关注的开源大模型除了GPT-3和BERT，还有许多其他模型同样值得关注。例如Facebook的RoBERTa，作为BERT的改进版本，它在性能上实现了显著提升。再如OpenAI的CLIP，这是一种创新的AI模型，能够理解图像与文本之间的关联，在艺术创作和多媒体应用领域展现出巨大的潜力。开源大模型的无限潜力这些模型仅仅是开源大模型世界的冰山一角。随着技术的持续进步，新的模型不断涌现，它们不仅推动了AI技术的发展，也为各种应用场景提供了广阔的想象空间。开源大模型的多样性与创新开源大模型的多样性是其最吸引人的特点之一。从专注于自然语言处理的模型如GPT-3和BERT，到能够处理图像和文本关系的CLIP，再到其他专门针对特定任务或数据类型的模型，开源社区的创新精神不断催生出新的工具和解决方案。这些模型的开放性意味着研究人员和开发者可以自由地访问、修改和改进它们，从而加速了AI技术的发展。技术进步对应用场景的影响随着技术的不断进步，开源大模型正在为各种应用场景提供前所未有的可能性。在医疗领域，AI模型可以帮助分析医学影像，辅助诊断；在金融行业，它们可以用于风险评估和欺诈检测；在教育领域，个性化学习工具可以根据学生的学习习惯和进度提供定制化内容。此外，开源大模型也在推动自动驾驶、智能家居、游戏开发等领域的创新。模型的可访问性与创新的民主化开源大模型的另一个重要影响是它们促进了创新的民主化。通过降低进入门槛，这些模型使得小型企业和个人开发者也能够参与到AI技术的研究和开发中来。这种开放和协作的环境不仅加快了技术的发展，还为解决全球性问题提供了更多的机会。未来展望：开源大模型与AI伦理展望未来，开源大模型将继续推动AI技术的边界。然而，随着技术的发展，我们也需要关注AI伦理和责任问题，确保这些强大的工具被用于促进社会的整体福祉。这包括确保算法的透明度、防止偏见和歧视、保护个人隐私等。开源大模型作为AI技术进步的重要驱动力，不仅展示了AI的无限可能，也为我们提供了探索未知、解决问题和创造新价值的机会。随着技术的不断发展，我们期待这些模型能够继续激发创新，同时我们也必须谨慎地处理伴随而来的伦理和社会责任问题。AI与人类：协作而非取代面对这些开源大模型的兴起，有人担忧AI是否会取代人类。然而，我的看法是，这些模型的出现实际上是在增强而非取代人类的能力。它们更像是一套强大的工具，赋予我们解决更复杂问题、实现更宏伟梦想的能力。展望未来开源大模型的涌现，无疑是AI技术发展史上的一次重大突破。它们不仅展示了AI技术的无限可能，也为人类开辟了新的可能性。展望未来，我坚信这些模型将继续引领AI技术的前进步伐，为我们的生活和工作带来更多的便捷与创新。",
        "voteup_count": 13,
        "updated_time": "2024-04-30 11:54:14",
        "question_id": 623672939,
        "user_id": "eefcdc1d60c3f6c07db77c20b7e11474"
    },
    {
        "answer_id": 3239571339,
        "content": "之前写过一些回答，但是这里：https://github.com/Mooler0410/LLMsPracticalGuide#Usage-and-Restrictions 的信息一直在更新，可以关注另外可以关注hugging face :Hugging Face – The AI community building the future.国内外的开源模型非常丰富，国外的如Meta的LLama，国内最近百川智能开源的大模型：https://github.com/baichuan-inc/Baichuan2，号称碾压LLama以上供参考。",
        "voteup_count": 3,
        "updated_time": "2023-10-07 10:04:31",
        "question_id": 623672939,
        "user_id": "646e30948d93aa29e766eb4a6b11a1ba"
    },
    {
        "answer_id": 3292369917,
        "content": "目前，已知的开源大模型有很多，如：Google的BERT、T5、GPT系列模型、OpenAI的GPT-3、Facebook的BART等等。这些模型在多种任务上表现优秀，但是需要大量的计算资源和专业知识来训练和优化。而对于大部分企业或者个人用户，可能没有足够的资源和技术能力来直接使用这些开源大模型。这时候，你可能需要一些更便捷的工具来帮助你使用这些模型，集简云就是这样一款工具。集简云是一款SaaS无代码软件连接器，可以无需开发轻松连接超过1000款软件的数据与接口，构建自动化与智能化的业务流程。通过集简云，你可以更容易地获取和分析数据，无需关心底层的数据处理和模型训练问题，只需要简单的配置，就可以构建出符合你业务需求的工作流程。这样不仅可以大大节约你的时间和精力，也能让你更专注于业务本身。所以，无论你是数据分析师，产品经理，还是企业决策者，我都强烈推荐你试用集简云，它将帮助你更好地利用现有的开源大模型，实现你的业务目标。",
        "voteup_count": 0,
        "updated_time": "2023-11-17 15:10:06",
        "question_id": 623672939,
        "user_id": "27cc62a5d4544e2de9c2147be88be3a4"
    },
    {
        "answer_id": 3301711443,
        "content": "目前已知大厂发布的大模型有如下几个，仅供参考。国内大厂序号厂商huggingface镜像下载地址1阿里通义千问https://aifasthub.com/models/Qwen2百川智能https://aifasthub.com/models/baichuan-inc3上海人工智能实验室https://aifasthub.com/models/internlm4智谱https://aifasthub.com/models/THUDM5智源人工智能研究院https://aifasthub.com/models/BAAI6FlagAlphahttps://aifasthub.com/models/FlagAlpha7零一万物https://aifasthub.com/models/01-ai海外大厂序号厂商huggingface镜像下载地1openaihttps://aifasthub.com/models/openai2googlehttps://aifasthub.com/models/google3lmsyshttps://aifasthub.com/models/lmsys4NousResearchhttps://aifasthub.com/models/NousResearch5OpenAssistanthttps://aifasthub.com/models/OpenAssistant6diffusershttps://aifasthub.com/models/diffusers7HuggingFaceH4https://aifasthub.com/models/HuggingFaceH48garage-bAIndhttps://aifasthub.com/models/garage-bAInd9microsofthttps://aifasthub.com/models/microsoft10bigcodehttps://aifasthub.com/models/bigcode",
        "voteup_count": 5,
        "updated_time": "2023-11-25 10:11:39",
        "question_id": 623672939,
        "user_id": "370e58856e50f054407ae3c804d13feb"
    },
    {
        "answer_id": 3240327543,
        "content": "文丨郝 鑫，编丨刘雨琦“OpenAI不足为惧，开源会慢慢赶上来。”彼时Hugging Face创始人Clem Delangue的一句预言，正在迅速成为现实。ChatGPT横空出世7个多月后，7月19日，Llama 2宣布开源，并且可直接商用。如今回看，这一天也成为了大模型发展的分水岭。在此之前，全世界开源的大模型不计其数，可只是停留在开发研究层面。“可商业”短短三个字，犹如一颗重磅炸弹引爆了大模型创业圈，引得傅盛连连感叹，“有的人哭晕在厕所，而有的人在梦中也能笑醒”。AI大模型圈一夜之间变了天，同时也宣告着大模型加速商业化时代的到来。自Llama 2后，开源逐渐成为主流趋势。以Llama架构为首，先掀起了一波以其为核心的开源，如Llama 2低成本训练版、Llama 2最强版、微调版等等。截至发稿前，以“LLama 2”为关键词在国外最大的AI开源社区Hugging Face检索模型，有5341条结果；在全世界最大的开源项目托管平台Github上，也有1500个词条。（图源：Hugging Face官网）（图源：Github官网）之后，创业者们的目光从解构、增强Llama 2转向了构建行业专有大模型，于是又掀起了一波Llama 2+司法、Llama 2+医疗等一系列的行业开源大模型。据不完全统计，Llama 2开源后，国内就涌现出了十几个开源行业大模型。国内头部厂商和创业公司纷纷加入开源浪潮中，阿里QWEN-7B开源一个多月下载量破100万，9月25日升级了QWEN-14B；百川智能开源的Baichuan-7B、13B两款开源大模型下载量目前已经突破500万，200多家企业申请部署开源大模型。与此形成强烈对比的是，短时间内，Llama 2对一些闭源的大模型厂商造成了致命性的打击。闭源大模型多采用调取API的方式使用，数据需要先上传至模型厂商，按照调用次数收取费用；而开源则可以在本地部署，且完全免费，可商用后产生的利润也可以收归己有。行业内人士告诉光锥智能：“在这种情况下，基于成本的考虑，已经开始有许多企业选择放弃支付上千万元的费用，转而部署和微调Llama 2”。以上种种，共同揭开了大模型开源闭源之争，发展重心的转移也让人疑惑：开源大模型是否正在“杀死”闭源？01 大模型开源，开的是什么？光锥智能梳理后发现，目前，大模型厂商和创业公司在开源和闭源的选择上，一共有三条路径：一是完全闭源，这类代表公司国外有OpenAI的GPT-3.5、GPT-4，国内有百度的文心大模型；二是先闭源再开源，这类代表公司有阿里云的通义千问，智谱AI开源GLM系列模型；三是先开源再闭源，这类代表公司有百川智能的Baichuan-7B、Baichuan-13B。现在中国市场上能够主动开源大模型，且提供商业许可的企业数量还比较有限，主要公司包括了以开源为切入的百川智能、大模型厂商代表阿里、大模型初创公司代表智谱AI以及走精调Llama 2路线的虎博科技。这从侧面也说明了一个问题，大模型开源并不是没有门槛，相反开源对一家企业的基础技术能力要求十分高，比如智谱AI的GLM-130大模型是去年亚洲唯一入选斯坦福大学评测榜的大模型；阿里通义千问大模型在IDC的“AI大模型技术能力评估测试”中获得了6项满分。如果再进一步将以上的公司分类，可以归为两类，一类是走自研大模型开源路线，一类是走Llama 2路线。这两条路线在国际上也十分典型，譬如走自研模型开源路线的Stability AI，已经陆续开源了Stable DiffusionV1、StableLM、Stable Diffusion XL（SDXL）1.0等模型，凭一己之力撑起了文生图开源领域；另一类如中东土豪研究院就死盯住Llama 2，在其基础上继续做大参数、做强性能， Llama 2开源50天后，地表最强开源模型Falcon 180B横空出世， 霸榜Hugging Face。不过，这两条路线也不是完全泾渭分明，Llama 2的开源也进一步促进了许多自研开源大模型的更新升级。8月Stability AI迅速推出类ChatGPT产品——Stable Chat，背后的大语言模型Stable Beluga就是其在两代Llama的基础上精调出来。更开放，更快迭代发展，这或许也是开源的意义。除了逆天的Falcon，目前开源模型的参数基本都控制在7B-13B左右。大模型厂商告诉光锥智能，“目前7B-13B亿参数量是一个较为合理的开源规模”。这是基于多重因素所得出的参数量规模，如计算资源限制、内存限制、开源成本考量等。阿里云CTO周靖人基于云厂商的角度考虑道：“我们希望企业和开发者，在不同的场景可以根据自己的需求选择不一样规模的模型，来真正地应用在自己的开发环境。我们提供更多可能性。”谈起为何开源大模型，周靖人强调了安全性，“我们不单单只是开源大模型，更重要的是要能够呈现出各项指标的表现效果，基于此，才能够让大家去评估其中的使用风险，更加有效地进行模型应用。”“重要的是，随着参数量的增加，模型效果提升会逐渐收敛。当模型达到一定规模后，继续增加参数对效果提升的边际效益只会下降，70-130亿参数量一般已经接近收敛状态了。”上述大模型厂商道。光锥智能发现，除了阿里云在视觉语言模型的细分领域发布了开源大模型外，其余公司皆提供的是通用能力的大模型。这或许与大模型开源仍处于非常早期阶段有关系，但考虑到开源大模型也要落地到场景中，太过于同质化的通用大模型对企业来说也容易沦为“鸡肋”。如何避免开源大模型重蹈覆辙，体现出开源的价值，回顾Meta接连祭出的“大招”，一条开源的路径似乎逐渐显现——构建开源大模型生态。2月份，Meta凭借开源的Llama大模型回到生成式AI核心阵列；5月9日，开源了新的AI 模型ImageBind，连接文本、图像 / 视频、音频、3D 测量（深度）、温度数据（热）和运动数据六种模态；5个月后，Llama 2开源可商业，含70亿、130亿和700亿三种参数规模，其中700亿参数模型能力已接近GPT-3.5；8月25日，Meta推出一款帮助开发人员自动生成代码的开源模型——Code Llama，该代码生成模型基于其开源大语言模型Llama 2；8月25日，发布全新AI模型SeamlessM4T，与一般AI翻译只能从文本到文本不同，这款翻译器还能够“从语音到文本”或者反过来“从文本到语音”地直接完成翻译；9月1日，允许开源视觉模型DINOv2商业化，同时推出视觉评估模型FACET。可以看到，Meta开源的思路是在各个AI领域遍地开花，通过发布该领域最先进的AI开源模型，吸引更多开发者的关注和使用，壮大整个AI开源生态后来反哺业务、巩固行业地位，这就如同当年的英伟达推动GPU计算的开源策略。当年英伟达推动GPU计算的开源化，不仅吸引了大量研究人员在Caffe、TensorFlow等框架上进行创新，也为自身GPU产品积累了大量优化经验，这些经验后来也帮助英伟达设计出了更适合深度学习的新型GPU架构。另一方面，GPU计算的开源生态越来越繁荣后，也为其带来了巨大的市场空间，Nvidia DGX企业级的深度学习训练平台概念应运而生，为英伟达的显卡和平台销售创造了千亿级市场。国内阿里云也在通过建设完善生态的方式，试图帮助开发者更好的用好大模型，据周靖人介绍，目前阿里云不仅有自研开源大模型，也接入了超过100个开源模型，同时打造了开源社区魔搭，更好地服务开发者和企业用户，用好、调好大模型。02 开源闭源不矛盾，是手段而非目的据外媒爆料，Meta正在加紧研发全新的开源大模型，支持免费商用，能力对标GPT-4，参数量比Llama 2还要大上数倍，计划在2024年初开始训练。国外大模型格局看似是OpenAI“一超多强”，实则是众多公司环伺，可以预见，开源大模型对闭源的围剿，越来越步步紧逼。国外一份研究报告称，大模型前期的发展创新由OpenAI、微软、谷歌等大公司闭源模型主导，但越到后期开源模型和社区的贡献值就越大。光锥智能也了解到，在国内开源大模型也成为了企业的“新卖点”，有企业甚至通过对外宣称已使用了“史上最强大模型Falcon 180B”，来展现其底层模型技术能力的强大，顶着“史上最强”的称号，又收割了“一波韭菜”。现阶段，开源大模型已经证明了几点重要的事实。首先，在非常大的数据集上进行训练，拥有几十亿个参数的大模型在性能上就可以与超大规模大模型相媲美；其次，只需要极少的预算、适量的数据以及低阶适应（Low-rank adaptation，LoRA）等技术就可以把小参数的大模型调到一个满意的效果，且将训练成本降低了上千倍。开源大模型为现在的企业提供了闭源的替代方案，低成本成为最吸引他们的地方；最后，我们也看到开源大模型的发展速度也远快于封闭生态系统。开源固然“迷人”，但更为关键的是，既不能为了开源而开源，也不能为了闭源而闭源。开源与闭源只是形式上的区别，并不矛盾，开源本身不是目的，而是手段。以开源切入大模型赛道的百川智能，在发布完Baichuan-7B、Baichuan-13B开源大模型后，王小川拿出了Baichuan-53B闭源大模型。在问到为什么没有继续开源时，王小川回答称：“模型变大之后没有走开源的这样一种方式，因为大家部署起来成本也会非常的高，就使用闭源模式让大家在网上调用API”。由此可见，是否开源或闭源并非完全没有参考，能够闭源一定是其能够提供价值。在当前，这个价值的集中体现可能是替用户完成高性能的大模型训练、推理和部署，通过调用API的方式来帮助降低门槛，这也是OpenAI闭源的思路，但因为其自身技术的绝对领先优势，使得其价值也非常得大。如果回顾红帽子公司的开源，也能探寻到同样的逻辑。过去十多年间，红帽从销售企业Linux操作系统，扩展到现在的存储、中间件、虚拟化、云计算领域，靠的就是“筛选价值”的逻辑。在最上游的开源社区，参与开源技术贡献，做大做强生态；提取开源社区中的上游技术产品，沉淀到自己小开源社区；再将其认为最有价值的技术检验、测试、打包，形成新的产品组合，完成闭源出售给客户。腾讯云数据库负责人王义成也曾对光锥智能表示：“开源的本质也是商业化，要从宏观层面看是否能满足一家公司的长期商业利益。开源的本质还是扩大生态，扩大你的影响力。开源还是要找清楚自己的定位，目标客户群。开源能否帮助产品突破，帮助公司完成阻击，还需要具体问题具体分析。”03 结尾事实上，开源还是闭源，二者并不是完全对立的关系，只是在技术发展的早期，路径选择的不同。这也并不是科技领域第一次面对这样的分叉路，参考数据库发展的路径，早期需要培育土壤，培植生态，以MySQL为主的开源数据库获得了爆发式的用户增长，但走过第一阶段后，更多企业用户发现开源数据库在面对业务时的短板，毕竟术业有专攻，谁也没办法一招打天下。为此，数据库厂商开始根据不同的企业需求针对性的研发闭源数据库，如在分布式数据库、流数据库等细分类别进行长足的创新。周靖人也认为：“未来，一定不是one size fits all”，不同的场景适配不同的参数，不同的形式，届时大模型将走过野蛮生长阶段，来到精耕细作。这也足以说明，开源还是闭源，或许只是阶段和位置的不同，但可以肯定的是，大模型时代，已经加速进入下一赛段。欢迎关注光锥智能，获取更多科技前沿知识！",
        "voteup_count": 1,
        "updated_time": "2023-10-07 19:30:05",
        "question_id": 623672939,
        "user_id": "71d43ff7c1111af75a397c711eec8104"
    },
    {
        "answer_id": 3482307873,
        "content": "不用怀疑！Llama3 开源即巅峰，霸榜githubMeta Llama 3 官方 GitHub 网站https://github.com/meta-llama/llama3此版本包括预训练和指令调整的 Llama 3 语言模型的模型权重和起始代码 - 包括 8B 到 70B 参数的大小。该存储库旨在作为加载 Llama 3 模型并运行推理的最小示例。有关更详细的示例，请参阅llama-recipes。下载要下载模型权重和分词器，请访问Meta Llama 网站并接受我们的许可证。一旦您的请求获得批准，您将通过电子邮件收到签名的 URL。然后运行 download.sh 脚本，并在提示开始下载时传递提供的 URL。先决条件：确保您已经wget安装md5sum。然后运行脚本：./download.sh。请记住，链接将在 24 小时和一定下载量后过期。如果您开始看到诸如 之类的错误403: Forbidden，您可以随时重新请求链接。获得拥抱脸我们还提供Hugging Face上的下载，包括转换器和本机llama3格式。要从 Hugging Face 下载权重，请按照以下步骤操作：访问其中一个存储库，例如meta-llama/Meta-Llama-3-8B-Instruct。阅读并接受许可证。一旦您的请求获得批准，您将有权访问所有 Llama 3 模型。请注意，处理请求最多需要一小时。要下载原始本机权重以与此存储库一起使用，请单击“文件和版本”选项卡并下载文件夹的内容original。您还可以从命令行下载它们，如果您pip install huggingface-hub：huggingface-cli download meta-llama/Meta-Llama-3-8B-Instruct --include \"original/*\" --local-dir meta-llama/Meta-Llama-3-8B-Instruct要与转换器一起使用，以下管道片段将下载并缓存权重：import transformersimport torchmodel_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"pipeline = transformers.pipeline( \"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", model_kwargs={\"torch_dtype\": torch.bfloat16}, device=\"cuda\",)快速开始您可以按照以下步骤快速启动并运行 Llama 3 模型。这些步骤将让您在本地运行快速推理。有关更多示例，请参阅Llama 食谱存储库。在具有 PyTorch / CUDA 的 conda 环境中，可以克隆并下载此存储库。在顶级目录中运行：pip install -e .访问Meta Llama 网站并注册以下载模型。注册后，您将收到一封电子邮件，其中包含下载模型的 URL。运行 download.sh 脚本时您将需要此 URL。收到电子邮件后，导航到下载的 llama 存储库并运行 download.sh 脚本。确保授予 download.sh 脚本执行权限在此过程中，系统将提示您输入电子邮件中的 URL。不要使用“复制链接”选项，而应确保手动复制电子邮件中的链接。下载所需的模型后，您可以使用以下命令在本地运行该模型：torchrun --nproc_per_node 1 example_chat_completion.py \\\n    --ckpt_dir Meta-Llama-3-8B-Instruct/ \\\n    --tokenizer_path Meta-Llama-3-8B-Instruct/tokenizer.model \\\n    --max_seq_len 512 --max_batch_size 6笔记替换 Meta-Llama-3-8B-Instruct/为检查点目录的路径和Meta-Llama-3-8B-Instruct/tokenizer.model分词器模型的路径。应将其–nproc_per_node设置为您正在使用的型号的MP值。根据需要调整max_seq_len和参数。max_batch_size此示例运行在此存储库中找到的example_chat_completion.py，但您可以将其更改为不同的 .py 文件。推理不同的模型需要不同的模型并行 (MP) 值：模型国会议员8B170B8max_seq_len所有模型都支持高达 8192 个令牌的序列长度，但我们根据和值预先分配缓存max_batch_size。因此，请根据您的硬件进行设置。预训练模型这些模型未针对聊天或问答进行微调。应该提示他们，以便预期的答案成为提示的自然延续。请example_text_completion.py参阅一些示例。为了说明这一点，请参阅下面的命令以使用 llama-3-8b 模型运行它（nproc_per_node需要设置为该MP值）：torchrun --nproc_per_node 1 example_text_completion.py \\\n    --ckpt_dir Meta-Llama-3-8B/ \\\n    --tokenizer_path Meta-Llama-3-8B/tokenizer.model \\\n    --max_seq_len 128 --max_batch_size 4\n指令调整模型经过微调的模型针对对话应用进行了训练。为了获得预期的功能和性能，ChatFormat 需要遵循中定义的特定格式：提示以<|begin_of_text|>特殊标记开始，后面跟随一条或多条消息。每条消息均以<|start_header_id|>标记、角色system或和标记开头。在双换行符之后是消息的内容。每条消息的结尾都由令牌标记。userassistant<|end_header_id|>\\n\\n<|eot_id|>您还可以部署其他分类器来过滤掉被认为不安全的输入和输出。请参阅 llama-recipes 存储库，了解如何向推理代码的输入和输出添加安全检查器的示例。使用 llama-3-8b-chat 的示例：torchrun --nproc_per_node 1 example_chat_completion.py \\\n    --ckpt_dir Meta-Llama-3-8B-Instruct/ \\\n    --tokenizer_path Meta-Llama-3-8B-Instruct/tokenizer.model \\\n    --max_seq_len 512 --max_batch_size 6\nLlama 3 是一项新技术，使用时存在潜在风险。迄今为止进行的测试尚未（也不可能）涵盖所有场景。为了帮助开发人员解决这些风险，我们创建了负责任的使用指南。问题请通过以下方式之一报告任何软件“错误”或模型的其他问题：报告模型问题：https://github.com/meta-llama/llama3/issues报告模型生成的有风险内容：http://developers.facebook.com/llama_output_feedback报告错误和安全问题：http://facebook.com/whitehat/info型号卡请参阅MODEL_CARD.md。搭建基于Llama的应用主要涉及以下步骤：一、准备工作确保你有一台运行操作系统的计算机（Windows、Mac或Linux）。确保你有一定的Python编程经验，因为Llama2通常需要通过Python环境进行搭建和配置。确保你有一个可靠的网络连接，以便下载所需的依赖库和Llama2代码。二、安装Python访问Python官方网站下载并安装适合你操作系统的Python版本。Llama2需要Python 3.5或更高版本。三、安装依赖库打开终端或命令提示符。运行pip install numpy pandas matplotlib来安装这些常用的数据科学库，它们将帮助你处理和分析数据。四、下载Llama2代码访问Llama2的GitHub页面或其他官方代码托管平台。下载最新的Llama2代码压缩包或直接克隆仓库到本地。五、配置环境解压下载的Llama2代码（如果是压缩包）。进入Llama2代码的文件夹。运行pip install -r requirements.txt来安装Llama2所需的所有依赖库。六、运行Llama2在终端或命令提示符中，确保你仍在Llama2代码的文件夹内。运行python llama2.py（或相应的启动脚本）来启动Llama2应用程序。访问指定的URL来访问Llama2的用户界面，并开始使用各种功能来处理和分析数据。此外，如果你想在Docker环境中搭建Llama2，你需要：安装Docker。创建一个Docker镜像，包含Llama2及其依赖。运行Docker容器来执行Llama2。请注意，具体的步骤和命令可能会因Llama2版本和你的操作系统而有所不同。始终建议参考Llama2的官方文档或GitHub页面上的说明以获取最新和最准确的安装指南。最后，虽然上述步骤提供了一个大致的框架来搭建基于Llama的应用，但具体的实现细节和配置可能会因你的具体需求和环境而有所不同。",
        "voteup_count": 1,
        "updated_time": "2024-04-29 09:53:14",
        "question_id": 623672939,
        "user_id": "a17c1d40f792435bb6deecb08e9817b2"
    },
    {
        "answer_id": 3409287305,
        "content": "2月22日，Google发布了轻量化开源模型Gemma：gemma (ollama.com)。Gemma 是一个轻量级、最先进的开放式模型系列，采用与创建 Gemini 模型相同的研究和技术构建。Gemma 由 Google DeepMind 和 Google 的其他团队开发，灵感来自双子座，这个名字反映了拉丁语 gemma，意思是“宝石”。此链接为Gemma的详细介绍：Gemma：Google 推出最先进的新开放模型 (blog.google)在多项基准测试中，Gemma 7B的得分，都超过了拥有130亿参数的LLAMA 13B模型。从Google提供的报告可以看出，Gemma模型在各种 Web 文档数据集上进行训练，以使其具有广泛的语言风格、主题和词汇。这包括用于学习编程语言的语法和模式的代码，以及用于掌握逻辑推理的数学文本。和大语言模型不同的是，轻量化模型可以直接在笔记本和桌面端运行，以适应不同应用场景的需要。此外Nvidia也和Google进行合作，提高Gemma的推理速度和微调能力。Nvidia提供了Gemma在线体验页面，以Gemma 2B模型为例，提出一个问题即可获得详细的答案。使用Gemma 7B模型，提出和代码相关的问题，也可以快速提供答案，或者使用中文提问，也可以输出对应的结果。Perplexity labs也在第一时间添加了Gemma模型到Playground：Perplexity Labs，选择对应的模型即可使用。下面打开网址，我做一下演示：提出一个中文问题“什么是回忆，怎么提高记忆力？”，在以gemma 7b模型输入时，从记忆技巧、注意力专注力、睡眠、药物等方面来回答如何提高记忆力。输出速度可达每秒200 TOKEN，响应速度可以达到毫秒级，回答的内容也十分准确和详细。下面尝试一个英文问题“How to improve interpersonal skills？”上图可以查看输出的回答，切换到其他模型，也可以输出非常详细的答案。如果想要在本地运行Gemma，也可以使用Olegma工具，完成一键本地部署。感兴趣的同学可以在上面网站尝试该模型。",
        "voteup_count": 2,
        "updated_time": "2024-02-26 08:22:07",
        "question_id": 623672939,
        "user_id": "19abb64e8cac1d72144c177a782c5445"
    },
    {
        "answer_id": 3391343394,
        "content": "Open-Source Language Model Pocket 春节都安好，所想皆如愿  All the best for the Spring Festival  May all your wishes come true 完整版（Full Version）: https://github.com/createmomo/Open-Source-Language-Model-Pocket微信公众号版：开源语言模型百宝袋 (Ver. 3.3)相关文章穷穷穷孩子如何体验ColossalAI SFT（Kaggle篇，Colab篇）通俗理解文本生成的常用解码策略通俗理解P-tuning (GPT Understands)通俗理解Gradient Checkpoint（附代码）千“垂”百炼：垂直领域与语言模型系列文章导语获得可用的垂直领域数据 【不限领域】利用未标注文本改进遵循指令的语言模型 (1) Instruction Backtranslation 简介【医疗/健康】ChatDoctor （解读 上 中 下）【医疗/健康】MedicalGPT-zh【医疗/健康】明医(MING)【医疗/健康】灵心(SoulChat)自动评估模型 【不限领域】用语言模型评估语言模型（1）导语【不限领域】用语言模型评估语言模型（2）PandaLM【不限领域】用语言模型评估语言模型（3）Shepherd（1 2 3 4）【医疗/健康】使用BERT-Score比较ChatDoctor与ChatGPT3.5开源模型一览 (Table of Contents)：中文友好或国内主创的开源模型（Chinese Open Source Language Models）多个领域/通用百川中文Alpaca Luotuo中文LLaMA&Alpaca大模型中文LLaMA&Alpaca大模型2流萤Firefly凤凰复旦MOSS复旦MOSS-RLHF悟道·天鹰Aquila&Aquila2雅意大模型通义千问Qwen活字AnimaBayLingBELLEBloomBiLLaBLOOMChat176BChinese-Llama-2-7b (LinkSoul-AI)GPT2 for Multiple LanguageInternLM 书生・浦语Llama2-chat-Chinese-50WLlama2-Chinese (FlagAlpha)Linly伶荔说 中文 LLaMA1-2 & OpenLLaMA & Falcon 大模型ChatRWKVChatYuanChatGLM-6BChatGLM2-6BChinese-Transformer-XLOpenKG-KnowLLMPromptCLUESkyText-Chinese-GPT3CPM-BeeTigerBotXVERSE-13BYuLan-Chat & YuLan-Chat-2Ziya-LLaMATechGPTEVAFLM-101BTinyLlamaColossal-LLaMA-2OpenBA (Encoder-Decoder)Ziya-Reader-13BFirefly-LLaMA2-ChineseMindLLMChatGLM3Skywork大模型Yi-6B/34B（零一万物）Nanbeige-16B（南北阁-16B）OrionStar-Yi-34B-Chat源2.0TechGPT2.0SUS-Chat-34BAlaya 元识OpenBuddy*【MiniGPT4Qwen】*【ChatLM-Chinese-0.2B】*【YAYI 2】*【DeepSeek LLM&MoE】*【MachineMindset(MBTI)】*【星辰语义（电信）】*【Chinese-Mixtral-8x7B】*【Baby-Llama2-Chinese】*【XVERSE-13B-256K】*【Eagle 7B（RWKV-v5）】*【iFlytekSpark-13B】*【MiniCPM】*【通义千问Qwen1.5】医疗健康本草华佗扁鹊灵心启真儿童情感陪伴大模型“巧板”OpenMEDLab 浦医明医 (MING)：中文医疗问诊大模型 (原名：MedicalGPT-zh)情感大模型PICAChinese-Vicuna-medicalMedicalGPTDISC-MedLLM （复旦）DoctorGLMChatMed-TCM&ChatMed-ConsultChatGLM-MedMeChatShenNong-TCM-LLMMindChat(漫谈): 心理大模型WiNGPTCareGPT孙思邈MolGen（药物研发）Taiyi（太一）*【MedAgents】*【Molecule Optimization】经济/金融貔貅FinMA & PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance轩辕BBT-FinCUGE-ApplicationsCornucopia-LLaMA-Fin-ChineseEcomGPTFinGLMDISC-FinLLM法律韩非 HanFei智海 录问ChatLaw 法律大模型LaWGPTLawyer LLaMALexiLawLawGPT_zh夫子•明察司法大模型DISC-LawLLMLawBench交通TransGPT · 致远教育&数学桃李EduChatchatglm-mathsAbel*【InternLM-Math】*【DeepSeekMath】表格/数据分析TableGPTData-Copilot*【Tabular LLM】自媒体&角色扮演MediaGPTCharacterGLM-6B*【Haruhi-Zero】古汉语尔雅 Erya*【荀子】编程/代码/AgentCodeShellCODEFUSION-75MDeepSeek CoderDevOps-Model（运维）MagicoderKwaiAgents*【LLaMA-Pro】*【HuixiangDou】天文/海洋/地球科学/科学星语StarWhisperOceanGPT*【K2&GeoGalactica】*【SciGLM】可参考的其它开源模型CerebrasMPT-7BChatDoctorOpenGPTCode Llama (Meta AI)OrcaDolly 1&2OpenChatKitFinGPTOpen-AssistantFalconPlatypusFacebook/Meta LLaMA/LLaMA2MedLLaMA-13B & PMC-LLaMA: Continue Training LLaMA on Medical PapersGiraffeRedPajamaGALACTICASQLCoder (Defog)Goar-7B for Arithmetic TasksStableLMHuggingChatStableVicunaKoala: A Dialogue Model for Academic ResearchStanford AlpacaLongLLaMAUltraLM-13BLLaMA复刻版OpenLLaMAVicuna: An Open-Source Chatbot Impressing GPT-4 with 90% ChatGPT QualityLlama-X: Open Academic Research on Improving LLaMA to SOTA LLMWombatLit-LLaMA ️WizardMathMammoTHXGen-7BMistral 7BXwin-LMLLaMA 2 LongUltraLM-13B (UltraFeedback)Llemma: An Open Language Model For MathematicsMistral-Trismegistus-7B （神秘学/玄学/灵性）Memory-GPT(MemGPT)MetaMathChipNeMo (芯片设计)Zephyrneural-chat-7b-v3-1（Intel）SteerLMLlama CoderMeditronRankZephyrStableLM Zephyr 3BOrca 2Mixtral 7b 8 ExpertPhiLLM360（Amber,CrystalCoder,Diamond）MambaSOLARNexusRaven（function calling LLM）LLaMA-MoE*【TinyLlama】*【Nous-Hermes-2 Mixtral 8x7B】*【AlphaGeometry】*【MoE-Mamba】*【StarCoder】*【OLMo】*【H2O-Danube-1.8B】训练/推理Alpaca-LoRAllama2.mojoAlpacaFarmLightLLMColossalAIMedusaChatLLaMAMegatron-LLaMAChinese-GuanacoMeZO: Fine-Tuning Language Models with Just Forward PassesDPO (Direct Preference Optimization)MLC LLMDialogADV：Evaluate What You Can't Evaluate: Unassessable Generated Responses QualityPKU-Beaver 河狸 (Safe RLHF)DeepSpeed-ChatPaLM + RLHF (Pytorch)FlexGenRL4LMsFlagAI and FlagDataReinforcement Learning with Language ModelGuanaco & QloRASpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight CompressionGPT4AllScikit-LLM: Sklearn Meets Large Language ModelsHugNLPTransformer Reinforcement LearningINSTRUCTEVALTrain_Transformers_with_INT4LOw-Memory Optimization (LOMO)Transformer Reinforcement Learning Xllama.cppvLLMllama2.cLongLoRARLLTE: Long-Term Evolution Project of Reinforcement LearningFlashAttentionExecuTorchTensorRT-LLMBPO（Black-Box Prompt Optimization）S-LoRASoRAXuanCe(玄策): 开源的深度强化学习(DRL)库EasyLM（JAX/Flax）FATE-LLM - Federated Learning for LLMsDeepSpeed-FastGenNVIDIA NeMo-AlignerRLAIF: Scaling Reinforcement Learning from Human Feedback with AI FeedbackMLXOpenRLHFCoLLiE: Collaborative Training of Large Language Models in an Efficient WaySuperalignmentLLMLingua: Compressing Prompts for Accelerated Inference of Large Language ModelsLarge Language Model UnlearningPowerInferm-LoRA*【LASER】*【StripedHyena-7B】*【SwiftInfer】*【SPIN（Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models）】*【Self-Rewarding Language Models】*【OPO（On-the-fly Preference Optimization）】*【ASPIRE】*【The Impact of Reasoning Step Length on Large Language Models】*【SliceGPT】*【FuseLLM】*【Tree of Thoughts】*【CogGPT】*【KTO（Kahneman-Tversky Optimisation）】*【Aligner】*【RPO（Robust Prompt Optimization）】*【Inference-Time Training Helps Long Text Generation】*【LiPO】*【ChatLLM.cpp】*【Self-Discover】评价天秤（FlagEval）獬豸（Xiezhi）BenchmarkC-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation ModelsHaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language ModelsKoLA: Carefully Benchmarking World Knowledge of Large Language ModelsLucyEval—中文大语言模型成熟度评测CMB: A Comprehensive Medical Benchmark in ChineseMultiscale Positive-Unlabeled Detection of AI-Generated TextsPandaLMAuto-JCLEVA: Chinese Language Models EVAluation PlatformALCUNA: Large Language Models Meet New KnowledgeHalluQA：Evaluating Hallucinations in Chinese Large Language ModelsGLoRE: Evaluating Logical Reasoning of Large Language ModelsHelpSteerAlignBench: 多维度中文对齐评测基准UHGEvalPurple Llama (Meta)OMGEvalSciGuard&SciMT-Safety*【HaluEval 2.0, The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models】*【DebugBench: Evaluating Debugging Capability of Large Language Models】*【GenMedicalEval】*【R-Judge】*【TravelPlanner】*【EasyJailbreak】*【AgentBench】文本向量*【Matryoshka Representation Learning】*【Jina Embeddings】*【BGE-M3】*【Nomic Embed】*【Moka Massive Mixed Embedding（M3E）】AgentAuto-GPTToolBench&ToolLLMHuggingGPTCAMEL:Communicative Agents for “Mind” Exploration of Large Scale Language Model SocietyAgentLM (AgentTuning, AgentInstruct)XAgentOpenAgents*【Personal LLM Agents - Survey】*【AUTOACT】*【MetaGPT】*【Multi-LLM-Agent】其它Alpaca-CoTSelf-InstructChatPiXiuWanda (Pruning by Weights and activations)GorillaStreaming LLMSheared LLAMA (Structured Pruning)gpu_poorLLMPruner：大语言模型裁剪工具QA-LoRALLM-Pruner: On the Structural Pruning of Large Language ModelsLLM for Recommendation SystemsTransformer Index for GEnerative Recommenders (TIGER)KnowPATAuthentiGPT: Detecting Machine-Generated TextCuriosity-driven Red-teaming for Large Language Models*【TinyGSM】*【MathPile】*【Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM】*【Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding】*【QAnything】*【Meta-Prompting】*【Lepton Search】*【RLMRec】“ 持续更新中 (Continuously Updated)... ”",
        "voteup_count": 8,
        "updated_time": "2024-02-08 20:50:54",
        "question_id": 623672939,
        "user_id": "0349dc6ac1dd9a4bcf7d3711da8d9bd3"
    },
    {
        "answer_id": 3483635246,
        "content": "截至2024年4月30日，已知的开源大模型主要有以下几个：LaMDA：由谷歌AI开发，是迄今为止最大的开源语言模型之一，拥有1370亿个参数，在各种自然语言处理任务上表现出卓越的性能。https://blog.google/technology/ai/lamda/Megatron-Turing NLG：由NVIDIA和微软合作开发，拥有5300亿个参数，是世界上最大的神经语言模型之一，在文本生成、机器翻译和问答等任务上表现出强大的能力。https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/WuDao 2.0：由北京智源人工智能研究院开发，拥有1.75万亿个参数，是世界上最大的中文语言模型之一，在中文理解和生成方面表现出最先进的水平。https://gpt3demo.com/apps/wu-dao-20BARD：由百度文心工作室开发，拥有10万亿个参数，是世界上最大的中文-英文双语大模型之一，在跨语言理解和生成方面表现出强大的能力。http://research.baidu.com/Blog/index-view?id=165OPT：由OpenAI开发，拥有1750亿个参数，是一个通用的大型语言模型，在各种自然语言处理任务上表现出良好的性能。https://openai.com/Bloom：由Hugging Face和BigScience合作开发，拥有1760亿个参数，是一个多模态的大型语言模型，可以处理文本、图像和代码等多种信息类型。https://huggingface.co/docs/transformers/en/model_doc/bloomJurassic-1 Jumbo：由AI21 Labs开发，拥有1780亿个参数，是一个大型语言模型，在各种自然语言处理任务上表现出良好的性能。https://www.ai21.com/blog/announcing-ai21-studio-and-jurassic-1Eleuther：由EleutherAI开发，拥有1370亿个参数，是一个大型语言模型，在各种自然语言处理任务上表现出良好的性能。https://www.eleuther.ai/ 除了上述列出的模型之外，还有一些其他的开源大模型正在开发中，例如由Google AI开发的Pathways System 2和由微软开发的DeepSpeed Megatron-Turing NLG。 开源大模型的出现为研究人员和开发人员提供了宝贵的资源，可以促进自然语言处理技术的进步，并推动新的应用和创新。 ",
        "voteup_count": 0,
        "updated_time": "2024-04-30 12:34:54",
        "question_id": 623672939,
        "user_id": "6c6d72e9f948dcf69f0034872c71f291"
    },
    {
        "answer_id": 3235241868,
        "content": "之前写过一篇文章，供参考。目前市面上有大量的开源大模型，而且还在不断涌现。一方面我们要关注最新的发展态势，同时大模型也存在良莠不齐的情况，特别是国内有些大模型只是简单套壳，这存粹是浪费我们的时间。当然我们也不可能什么模型都去碰，以下根据我所了解的信息以及结合试用模型的一些情况，挑选出文本生成大模型以及文生图大模型的一些典型代表，其中也包括国内三家典型的文本生成模型代表，如果我们要学习大模型，可以从如下这些开源大模型入手，比如文本生成模型挑选1到2个，如Llama2或者Mistral，或者挑一个国内的大模型，如ChatGLM，当然如果你要挑选文生图，你可以挑选1个进行学习，如runwayml的stable diffusion 1.5或者StabilityAI的SDXL 1.0，各位可以根据自己的喜好或者目标挑选如下相应的开源大模型：GPT-2：GPT-2 (Generative Pre-trained Transformer 2) 是由 OpenAI 开发的大型自然语言处理模型，在2019年2月发布，是当时最好的开源大模型，可以算是大模型发展的一大里程碑，由于GPT-3和GPT-4不再开源，所以这是唯一能获得OpenAI开源的文本生成大模型。它基于 Transformer 架构，具有 1.5 亿个参数。这个模型是为了处理各种自然语言处理任务而设计的，包括但不限于文本生成、翻译、问答以及摘要生成。Falcon180B：Falcon180B 是由阿布扎比的全球领先技术研究中心 TII 发布的世界顶级开源大模型。此模型在 3.5 万亿 token 的训练下，拥有 1800 亿参数，性能超过了之前的开源模型 Llama2 70B和GPT 3.5 ，甚至接近了OpenAI的 GPT-4​。可惜大部分玩家玩不起。Llama 2：Llama 2 是由 Meta AI 开发的下一代大型开源语言模型，可免费用于研究和商业用途。于 2023 年 7 月 18 日发布。Llama 2 包括预训练和微调的大型语言模型，模型参数范围从 70 亿到 700 亿不等​​。预训练模型是在 2 万亿个令牌上训练的，具有比 Llama 1 两倍的上下文长度。微调模型在超过 100 万个人类注释上进行了训练。目前Llama2已经成为文本生成模型的标杆，大部分模型都会拿它做基准对比，国内很多开源大模型更是在它的基础进行微调和训练。Mistral-7B：Mistral-7B 是 Mistral AI 开发的大型语言模型，定位为 OpenAI 的 GPT-3 和 GPT-4 等高容量模型的替代选择，根据Apache 2.0 许可证发布，允许在各种环境中无限制地使用和部署。它拥有70亿个参数。是目前小模型的佼佼者。Mistral-7B 在多个指标上显著优于 Llama 2 13B，并在某些领域与 Llama 34B 相当。基准测试涵盖了包括常识推理、世界知识、阅读理解、数学和编码等多个主题。值得注意的是，Mistral-7B 的表现相当于一个比其大3倍的假想 Llama 2 模型，显示了其设计的高效率。BLOOM：BLOOM 是一个大型多语言模型，由 BigScience 项目创建，这个项目是一个开放的合作项目，有来自全球的数百名研究人员和机构参与​​。超过 1,000 名 AI 研究人员参与了 BLOOM 的创建，目的是为了提供一个可以大规模公开访问的大型语言模型。BLOOM 模型拥有 1760 亿个参数，能够在 46 种自然语言和 13 种编程语言中生成文本。对于其中的绝大多数语言，例如西班牙语、法语和阿拉伯语，BLOOM 是第一个拥有超过 1000 亿参数的语言模型​。StarCoder：StarCoder 是一个为代码而设计的大型语言模型（LLM），由 Hugging Face 和 ServiceNow 合作开发了 StarCoder。StarCoder 和 StarCoderBase 模型都具有大约 155 亿的参数，能处理长达 8,000 个令牌的文本序列​。这些模型是在来自 GitHub 的许可许可数据上训练的，包括 80 多种编程语言、Git 提交、GitHub 问题和 Jupyter 笔记本​​。StarCoder 现已可用于 Visual Studio Code，作为 GitHub Copilot 的替代品，用于代码生成。Watsonx.ai Geospatial Foundation Model：Watsonx.ai Geospatial Foundation Model 是 IBM 与 NASA 合作开发的一个大型开源地理空间人工智能模型，这是目前Hugging Face最大的开源地理模型。这个模型代表了气候科学和地球研究的重要进展，旨在促进 AI 的民主化和加速这些关键领域的创新​。这模型是基于 NASA 的卫星数据建立，目的是将卫星数据转换为高分辨率的洪水、火灾和其他景观变化的地图，以揭示我们的星球的过去并预示其未来​。ChatGLM-6B：ChatGLM-6B 是一个开源的、支持中英双语问答的对话语言模型，基于 General Language Model (GLM) 架构，具有 62 亿参数。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）。ChatGLM-6B 使用了和 ChatGLM 相同的技术，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62 亿参数的 ChatGLM-6B 已经能生成相当符合人类偏好的回答。 ChatGLM-6B 权重对学术研究完全开放，在填写问卷进行登记后亦允许免费商业使用。Baichuan-7B：Baichuan-7B是由百川智能开发的一个开源的大规模预训练模型。基于Transformer结构，在大约1.2万亿tokens上训练的70亿参数模型，支持中英双语，上下文窗口长度为4096。在标准的中文和英文权威benchmark（C-EVAL/MMLU）上均取得同尺寸最好的效果。Baichuan-7B使用宽松的开源协议，允许用于商业目的。Qwen-7B：通义千问-7B（Qwen-7B）是阿里云研发的通义千问大模型系列的70亿参数规模的模型。Qwen-7B是基于Transformer的大语言模型, 在超大规模的预训练数据上进行训练得到。预训练数据类型多样，覆盖广泛，包括大量网络文本、专业书籍、代码等。同时，在Qwen-7B的基础上，我们使用对齐机制打造了基于大语言模型的AI助手Qwen-7B-Chat。相较于最初开源的Qwen-7B模型，我们现已将预训练模型和Chat模型更新到效果更优的版本。runwayml/stable-diffusion-v1-5：RunwayML 的 Stable-Diffusion-v1-5 是一个基于深度学习的大型模型，专门用于文本到图像的生成。Stable-Diffusion-v1-5 是基于 Stable-Diffusion-v1-2 的权重初始化，随后在 \"laion-aesthetics v2 5+\" 数据集上以 512x512 的分辨率进行了 595k 步的微调，同时减少了 10% 的文本条件化以改善无分类器的引导采样。该模型可以与 Diffusers 库一起使用​。是目前最受欢迎的文生图的开源模型之一。stabilityai/stable-diffusion-2-1：Stable-Diffusion-2-1 是一个继 Stable-Diffusion-2 后微调过的模型，由 Stability AI 开发，并专注于从文本生成图像，能够生成详细和逼真的图像，具有改善的组成和逼真的美学，这些图像是基于较短的文本提示生成的。此模型首先从 Stable-Diffusion-2（768-v-ema.ckpt）微调，通过在相同的数据集上添加额外的 55k 步（带有 punsafe=0.1），然后进行了另外 155k 额外步骤的微调（带有 punsafe=0.98）。它也是目前最受欢迎的文生图的开源模型之一。stabilityai/stable-diffusion-xl-base-1.0：Stability AI的Stable Diffusion XL 1.0 模型，也被称为SDXL 1.0，是一个基于文本到图像的生成模型，包含一个名为\"ensemble of experts\"的流程，用于潜在扩散。首先，基础模型用于生成（带噪声的）潜在变量，然后通过专门用于最终去噪步骤的细化模型进一步处理这些变量。SDXL 1.0是文本到图像生成模型的下一个迭代，Stability AI描述Stable Diffusion XL 1.0为其迄今为止\"最先进\"的文本到图像模型。根据评估结果，SDXL 1.0 基础模型的性能显著优于 Stable Diffusion 1.5 和 2.1 的变体。当与细化模块结合时，SDXL 1.0 实现了最佳的整体性能.",
        "voteup_count": 3,
        "updated_time": "2023-10-03 15:15:48",
        "question_id": 623672939,
        "user_id": "17674eb41cf5ef88cdc1e69d5169a8d7"
    },
    {
        "answer_id": 3484694399,
        "content": "目前已知的开源大模型包括但不限于以下几种：Skywork-13B系列：由昆仑万维开源，包含基础模型和数学模型，专为中文优化，具有130亿参数。GROK：3140亿参数的混合专家模型，是目前为止参数量最大的开源LLM。Gemma：谷歌开源的模型，提供了不同大小的版本，部分版本可免费商用。Mixtral：Mistral AI的模型，宣称超越GPT3.5，重新定义AI性能和多样性。LLama2：OpenAI开源的模型，提供了不同规模的版本，从70亿到650亿参数不等。WizardLM：微软发布的模型，使用ChatGPT对指令进行复杂度进化微调LLama2。Falcon：阿联酋技术研究所推出的模型，训练数据量达3.5万亿token。Vicuna：基于LLama13B使用ShareGPT指令微调的模型。OpenSora：高效复现类Sora视频生成的完全开源方案。Baichuan：百川智能开源的7B大模型，可商用。Qwen：通义千问升级的模型，支持32K上文。这些模型在不同的评测基准上表现优异，覆盖了从基础的语言建模到特定领域的应用，如数学问题解决、编程、企业智能等。开源大模型的可用性为研究人员和开发者提供了丰富的资源，以进行各种NLP任务和探索新的应用场景。",
        "voteup_count": 1,
        "updated_time": "2024-05-01 15:29:12",
        "question_id": 623672939,
        "user_id": "9b5deb65baf04849cac5e85a4e71e715"
    },
    {
        "answer_id": 3307279764,
        "content": "",
        "voteup_count": 0,
        "updated_time": "2023-11-29 19:30:03",
        "question_id": 623672939,
        "user_id": "0e4a13661c2a1112787cd9381a3b90bc"
    },
    {
        "answer_id": 3461546757,
        "content": "一、本地部署1.1 下载客户端官网地址：https://gpt4all.io/index.html，这里根据我们电脑的操作系统选择对应的客户端。1.2 客户端安装和我们平时安装软件一样，这里目录优先选择磁盘空间大的位置，新建一个文件夹用于存放这个软件数据1.3 安装完毕基本设置匿名使用分析，以改进GPT4AI：是 / 否匿名分享聊天到GPT4AI数据湖：是 / 否上面有个内容都勾选否就行，简单来说就是是否需要联网查询1.4 模型选择我们先选择一个评分高的1.5 模型下载这里我们点击download models热点：可以接入gpt4，只是需要电脑配置和API来承接等待下载完毕我们可以多下几个如果客户端下载缓慢，我们直接在网页端下载下载之后导入到对应的文件目录二、模型下载完毕2.1 导入模型特别说明：路径别改，我上面的步骤改了，但是模型导进去不生效，识别不了，大家保持默认就行2.2 效果测试唯一的缺点，没有算力，响应很慢",
        "voteup_count": 2,
        "updated_time": "2024-04-10 23:14:20",
        "question_id": 623672939,
        "user_id": "913f689d53c9bd16c78307c16037942b"
    },
    {
        "answer_id": 3484216477,
        "content": "Meta的LLaMA系列由Meta开源，包含不同规模的模型，从7亿到650亿参数不等，适合在多种设备上运行。官方论文和介绍：LLaMA-2 PaperHugging Face博客介绍：LLaMA-2 Hugging Face BlogGitHub仓库：meta-llama/llama3BLOOMHugging Face提供的1760亿参数模型，支持多语言处理任务。Hugging Face模型页面：BLOOM-176BBLOOM项目介绍：BigScience BLOOM论文：BLOOM Paper斯坦福大学的Stanford Alpaca基于LLaMA 7B进行指令调优的模型，提供了训练数据和代码的开源。官方介绍：Stanford CRFM AlpacaGitHub仓库：tatsu-lab/stanford_alpaca清华大学的ChatGLM支持中英双语的对话语言模型，基于General Language Model（GLM）架构，针对中文进行了优化。清华大学开源：ChatGLM GitHubStableLM由Stability AI开发的语言模型，具有不同参数规模的模型，可以生成文本和代码。Stability AI的模型：StableLM GitHub",
        "voteup_count": 0,
        "updated_time": "2024-04-30 23:31:54",
        "question_id": 623672939,
        "user_id": "2e324bb81939567cdbc283591bd53991"
    },
    {
        "answer_id": 3484391918,
        "content": "推荐一个大模型的评测榜单，llm arena leaderboard。上面列举了大部分的开源、闭源的大模型。主流的都在那里。这个网站通过人工对比模型生成结果的方式来确定模型之间的好坏，并且最终用游戏elo分数的方式对模型进行排名。所以除了你可以了解到有哪些模型，模型是否开源、是否可以商用，还可以看到模型的性能怎么样。目前开源里比较强的模型是command r+、llama3、mistral、qwen、yi。",
        "voteup_count": 1,
        "updated_time": "2024-05-01 08:36:41",
        "question_id": 623672939,
        "user_id": "d93bc893ade3e64f1f05070490a2effe"
    },
    {
        "answer_id": 3486114603,
        "content": "我已经整理了关于国内外30个热门大型模型的相关资料，请您查看我的回答：全球热门大模型架构配置信息汇总国内外 30 个热门大模型的架构的图文解析汇总",
        "voteup_count": 1,
        "updated_time": "2024-05-03 10:37:59",
        "question_id": 623672939,
        "user_id": "18dd8357bb937263823209ff8d4d299a"
    },
    {
        "answer_id": 3485623411,
        "content": "国外开源大模型：1. LLaMA 2：由Meta（Facebook的母公司）开源，这个模型系列提供了不同大小的变体，从7亿到700亿参数，支持多种自然语言处理任务。2. Grok：由AI21 Labs开源，这是一个3140亿参数的模型，专注于文本生成和理解任务，尽管对计算资源要求较高。3. Falcon：由阿拉伯技术创新研究所开源，提供了不同规模的模型，以处理各种自然语言理解和生成任务。国内开源大模型：1. 书生・浦语（InternLM）：由上海人工智能实验室与商汤科技等联合发布，提供了7B和104B参数规模的模型，支持广泛的自然语言处理任务，并提供商用许可。2. ChatGLM2：由智谱AI发布，是一个6B参数的中英双语对话模型，支持对话生成和文本理解任务。3. 盘古α：“盘古α”是由鹏城实验室联合北京大学等机构发布的业界首个全开源2000亿参数中文预训练语言模型，具备很强的小样本学习能力，适用于多种文本生成领域。开源大模型为研究人员和开发者提供了强大的工具，有助于推动自然语言处理技术的发展和应用。",
        "voteup_count": 1,
        "updated_time": "2024-05-02 18:20:19",
        "question_id": 623672939,
        "user_id": "d7e47105949f70cd7081e4fa608e0198"
    },
    {
        "answer_id": 3483742873,
        "content": "开源大模型近年来在人工智能领域发展迅速，吸引了众多研究者和开发者关注。简单整理了一些知名的开源大模型，涵盖了自然语言处理、计算机视觉、多模态等领域：### 自然语言处理（NLP）1. **BERT（Bidirectional Encoder Representations from Transformers）**   - 来源：Google   - 开源链接：[GitHub](https://github.com/google-research/bert)   - 特点：基于Transformer的双向预训练模型，广泛应用于各种NLP任务。2. **RoBERTa（A Robustly Optimized BERT Pretraining Approach）**   - 来源：Facebook AI Research (FAIR)   - 开源链接：[GitHub](https://github.com/pytorch/fairseq/tree/master/examples/roberta)   - 特点：对BERT进行了优化，通过更大规模的训练数据和更长的训练时间，提升了性能。3. **GPT系列（Generative Pre-trained Transformer）**   - 来源：OpenAI   - 开源版本示例：[EleutherAI的GPT-Neo](https://github.com/EleutherAI/gpt-neo) 和 [GPT-J](https://github.com/eleutherai/jumbo)   - 特点：强大的生成能力，尤其是GPT-3展示了惊人的文本生成和任务适应性，但GPT-3本身并未完全开源，只有部分变体和研究者复现的版本公开。4. **T5（Text-to-Text Transfer Transformer）**   - 来源：Google   - 开源链接：[GitHub](https://github.com/google-research/text-to-text-transfer-transformer)   - 特点：统一的文本到文本框架，适用于多种NLP任务，通过改变输入输出格式即可适应不同任务。5. **ALBERT（A Lite BERT）**   - 来源：Google   - 开源链接：[GitHub](https://github.com/google-research/albert)   - 特点：在BERT基础上进行了参数量减少的优化，使得模型更轻量，同时保持了强大的性能。### 计算机视觉（CV）1. **YOLO（You Only Look Once）系列**   - 来源：Joseph Redmon等人   - 开源链接：[YOLOv5](https://github.com/ultralytics/yolov5) （非官方升级版）   - 特点：实时目标检测模型，速度快且准确，适用于物体识别和定位。2. **DETR（DEtection TRansformer）**   - 来源：Facebook AI Research (FAIR)   - 开源链接：[GitHub](https://github.com/facebookresearch/detr)   - 特点：首次将Transformer架构应用于端到端的目标检测任务，引入了新的训练范式。### 多模态1. **M6（M6: A Large-Scale Multimodal Pretrained Model）**   - 来源： DAMO Academy   - 开源链接：[GitHub](https://github.com/damo-cv/M6)   - 特点：一个大规模的多模态预训练模型，整合了文本、图像、语音等多种模态信息。2. **CLIP（Contrastive Language-Image Pre-training）**   - 来源：OpenAI   - 开源链接：[GitHub](https://github.com/openai/CLIP)   - 特点：通过对比学习，在图像和文本之间建立关联，实现了图像和文本的零样本迁移学习。这些模型代表了当前大模型研究的前沿，不仅推动了自然语言处理、计算机视觉等领域的进步，也促进了跨模态理解和生成技术的发展。值得注意的是，随着技术的快速迭代，新的开源大模型也在不断涌现。",
        "voteup_count": 1,
        "updated_time": "2024-04-30 14:23:55",
        "question_id": 623672939,
        "user_id": "b4dd5831f5fff8cbeac46aacfc66353a"
    },
    {
        "answer_id": 3478224789,
        "content": "大型语言模型（LLMs）无疑是此次AI革命的关键，它们基于Transformer架构，通过预训练大量文本数据，获得惊人的对话和任务处理能力。尽管如此，目前备受欢迎的大模型，诸如ChatGPT和Bard，都建立在专有的闭源基础之上，这无疑限制了它们的使用，并导致技术信息的透明度不足。然而，开源AI大模型（LLMs）正逐渐崭露头角，它们不仅增强了数据的安全性和隐私保护，还为用户节省了成本，减少了对外部依赖，实现了代码的透明性和模型的个性化定制。更重要的是，开源LLMs积极支持社区的发展，推动着整个领域的创新和发展！本文将详细介绍10个顶级开源LLMs大模型！记得提前收藏！1. LLaMA 3来源网络近期，Meta 重磅发布发布两款开源Llama 3 8B与Llama 3 70B模型，供外部开发者免费使用。Meta表示，Llama 3 8B和Llama 3 70B是目前同体量下，性能最好的开源模型。LLaMA 无疑是开源模型的顶流，国内好多大模型都是基于它实现的！它通过人类反馈的强化学习 （RLHF） 进行了微调。它是一种生成文本模型，可以用作聊天机器人，并且可以适应各种自然语言生成任务，包括编程任务。从其分享的基准测试可以看出，Llama 3 400B+ 的实力几乎媲美 Claude 超大杯以及 新版 GPT-4 Turbo，虽然仍有一定的差距，但足以证明其在顶尖大模型中占有一席之地。模型下载链接：https://llama.meta.com/llama-downloads/GitHub项目地址：https://github.com/meta-llama/llama32. Phi-3Phi 是由微软 AI 研究院最新开发的一个开源「小型语言模型」，可商用，卖点是小，需要的资源少。模型包括 Phi-3-Mini、Phi-3-Small 和 Phi-3-Medium。其中，Phi-3-Mini 最小，只有 3.8B 的参数，但在重要的基准测试中的表现可与大型模型如 Mixtral 8x7B 和 GPT-3.5 媲美而更大的 Small 和 Medium ，在扩展的数据集的加持下就更牛逼了。来源网络《Phi-3 技术报告：一个能跑在手机上的大模型》：https://arxiv.org/abs/2404.14219链接：https://huggingface.co/microsoft（待上线）3. BERT来源网络BERT是早期大型语言模型的代表作，底层技术基于Transformer架构。谷歌于2017年开发并在《注意力是你所需要的一切》中介绍了它。作为测试Transformer潜力的首批实验之一，BERT在2018年开源后迅速在自然语言处理任务中取得先进性能。由于其创新和开源性质，BERT成为最受欢迎的LLMs之一，有数千种开源、免费和预训练的模型用于各种用例。不可否认的是，近年来谷歌对开源大模型的态度变得较为冷漠。链接：https://github.com/google-research/bert4. Falcon 180B来源网络Falcon 40B已在开源LLM社区获得高度评价，位列Hugging Face排行榜首位。新推出的Falcon 180B展示了专有与开源LLM间差距正快速缩小。2023年9月，阿联酋技术创新研究所宣布，Falcon 180B正在接受1800亿参数的训练，其计算能力令人瞩目，已在多种NLP任务中超越LLaMA 2和GPT-3.5。尽管免费供商业和研究使用，但运行Falcon 180B需大量计算资源。5. BLOOM来源网络BLOOM 于 2022 年推出，经过与来自 70+ 个国家的志愿者和 Hugging Face 的研究人员为期一年的合作项目，BLOOM 是一种自回归LLM训练，可使用工业规模的计算资源在大量文本数据上继续从提示开始文本。BLOOM 的发布标志着生成式 AI 开源化的重要里程碑。BLOOM 拥有 1760 亿个参数，是最强大的开源之一LLMs，能够以 46 种语言和 13 种编程语言提供连贯准确的文本。其透明度是其核心特点，源代码和训练数据均可访问，方便运行、研究和改进。此外，BLOOM可通过Hugging Face生态系统免费使用。链接：http://bigscience.huggingface.co6. XGen-7B来源网络多家公司纷纷参与LLM竞赛，Salesforce于2023年7月推出XGen-7BLLM。多数开源LLMs提供有限信息的大答案，而XGen-7B支持更长的上下文窗口。其高级版本XGen-7B-8K-base拥有8K上下文窗口。虽然XGen使用7B参数进行训练，远少于其他强大LLMs，但效率颇高。尽管尺寸较小，XGen表现卓越，适用于商业和研究，但XGen-7B-{4K，8K}-inst版本除外，该版本经教学数据和RLHF训练，以非商业许可发布。7. GPT-NeoX 和 GPT-J来源网络GPT-NeoX和GPT-J是由EleutherAI实验室开发的GPT的开源替代品，分别拥有200亿和60亿参数。尽管与其他超过1000亿参数的LLMs相比，它们依然能提供高精度结果。它们经过22个高质量数据集的训练，适用于多个领域和用例。与GPT-3不同，GPT-NeoX和GPT-J未接受RLHF训练。可用于各种自然语言处理任务，包括文本生成、情感分析以及研究和营销活动开发，并可通过NLP Cloud API免费获取。8. Vicuna13-B图源网络Vicuna-13B是一个开源对话模型，基于LLaMa 13B进行微调，训练数据来自ShareGPT收集的用户共享对话。作为一款智能聊天机器人，Vicuna-13B在多个行业有广泛应用，如客户服务、医疗保健、教育、金融和旅游/酒店。初步评估表明，Vicuna-13B在90%以上的案例中优于LLaMa2和Alpaca等其他模型。9. Mistral 7B来源官网Mistral 7B v0.2 是 Mistral-7B-Instruct-v0.2 的基础预训练模型，属于「Mistral Tiny」系列。此次更新主要提升上下文长度至32K，Rope Theta设为1e6，并取消滑动窗口。链接：https://mistral.ai/10. 零一万物Yi系列模型是01.AI推出的下一代开源大型语言模型，旨在成为双语语言模型领域的佼佼者。该模型利用3T多语言语料库进行训练，具备出色的语言理解、常识推理和阅读理解等能力。据2024年1月数据显示，Yi-34B-Chat模型在AlpacaEval排行榜上位列第二，仅次于GPT-4 Turbo，且优于其他LLM如GPT-4、Mixtral、Claude。此外，Yi-34B模型在Hugging Face Open LLM Leaderboard和C-Eval等各种基准测试中，均排名第一，超越所有现有开源模型，如Falcon-180B、Llama-70B、Claude。这些成绩使Yi系列模型成为全球最强大的LLM模型之一，展现出广阔的应用前景。论文：https://arxiv.org/abs/2403.04652链接：https://github.com/01-ai/Yi如何选择适合您需求的开源LLM开源LLM领域正迅速扩大，全球开发人员合作升级和优化LLMs版本，性能差距有望缩小。选择开源LLM时，需考虑以下因素，找到最适合您需求的LLM：您的目标是什么？注意许可限制，选择适合商业用途的LLM。为什么需要LLM？考虑是否有必要使用LLM实现您的想法，避免不必要的花费。您需要的精度如何？大型LLMs通常更准确。如需要高精度，可选择如LLaMA或Falcon的大型模型。您愿意投入多少资金？大型模型需要更多资源。考虑基础设施和云提供商成本。能否使用预训练模型？如适合预训练模型的使用场景，可节省时间和金钱。结语IT行业的历史告诉我们，开源是软件领域里的一大潮流，它推动了应用生态的繁荣。但自从GPT3出现后，Open AI却选择了闭源，这使得开源大模型的发展似乎停滞在了GPT3.5的阶段。不过，业界还是有一些口碑不错的前沿开源大模型，比如Meta的LLaMA3、Mistral的Mistral 8x7B和零一万物的Yi-34B等。虽然开源模式在构建生态方面很给力，但因为算力和算法等方面的限制，它在大模型领域的发展还充满了不确定性。甚至有人担心，开源模型会逐渐落后。好在Llama 3的出现，给开源模型带来了一线希望。这场关于开源与闭源的辩论还在继续，咱们就拭目以待，看看开源和闭源将如何共同塑造AI的未来吧！",
        "voteup_count": 1,
        "updated_time": "2024-04-25 14:47:21",
        "question_id": 623672939,
        "user_id": "2e20e9051b943f64b52e979069aff153"
    },
    {
        "answer_id": 3485942071,
        "content": "AI21实验室的Jurassic-2亚马逊的AlexaTMAnthropic的Claude百度的Ernie 3.0 Titan彭博的BloombergGPTCerebras的Cerebras-GPTDatabricks&Mosaic ML的DBRX谷歌DeepMind的Gopher、Chinchilla、Gemini、GemmaEleutherAI的GPT-J和GPT-NeoX谷歌的BERT、T5、XLNet、GLaM、LaMDA、PaLM、Minerva华为的PanGu-ΣIndependent的Neuro-samaLAION的OpenAssistantHugging Face的BLOOMMeta的OPT、Galactica、LLaMA微软的Phi-2微软跟英伟达合作的Megatron-Turing NLGMistral AI的Mistral 7B、Mixtral 8x7BOpenAI的GPT1~4RWKV的Eagle 7BTechnology Innovation Institute的Falcon、Falcon 180BxAI的Grok-1Yandex的YaLM 100B",
        "voteup_count": 0,
        "updated_time": "2024-05-03 04:24:01",
        "question_id": 623672939,
        "user_id": "f8751bbee13af508a95bf9228305eb11"
    },
    {
        "answer_id": 3483037709,
        "content": "我说一个：羊驼3",
        "voteup_count": 0,
        "updated_time": "2024-04-29 21:28:58",
        "question_id": 623672939,
        "user_id": "7e2468fe0b1f140b0115478bb11598b0"
    },
    {
        "answer_id": 3480841829,
        "content": "上次写过一篇《本地部署搭建自己的ChatGPT》，主要是使用套壳软件+APIKey搭建本地大模型。这次主要是利用开源大模型在本地搭建聊天平台（或者本地知识库）。优点：私密；免费；灵活（各个模型随便用）。缺点：速度慢；非最新；中文效果不一定好。背景需求1.本人没有GPU，只能在CPU电脑上搭建，所以速度比较慢，仅作为练手或体验。2.使用的linux（麒麟）操作系统，基本上是源码部署。3.局域网使用，没法连接互联网。（声明：网上有比较多的教程、这里更多的是记录本人搭建过程）本人使用Ollama、LMStudio都体验过，这里以Ollama+AnythingLLM实现。一、安装Ollama（其实官网中有详细介绍）项目地址：https://ollama.com/download/1.在联网电脑中：sudo curl -L https://ollama.com/download/ollama-linux-amd64 -o /usr/bin/ollama相当于把ollama下载到/usr/bin/ollama中2.将ollama拷贝到离线电脑相同位置，并sudo chmod +x /usr/bin/ollama赋予相应权限3.添加Ollama作为一个系统服务：创建用户：sudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama创建服务（ollama.service），放在/etc/systemd/system/下：[Unit]Description=Ollama ServiceAfter=network-online.target[Service]ExecStart=/usr/bin/ollama serveUser=ollamaGroup=ollamaRestart=alwaysRestartSec=3[Install]WantedBy=default.target启动服务：sudo systemctl daemon-reloadsudo systemctl enable ollamasudo systemctl start ollama4.在命令行窗口中输入：ollama，如果出现下图表示安装成功(显示了一些几本命令，如create、run、pull等)二、使用Ollama1.使用官网的模型（library）ModelParametersSizeDownloadLlama 38B4.7GBollama run llama3Llama 370B40GBollama run llama3:70bPhi-33,8B2.3GBollama run phi3Mistral7B4.1GBollama run mistralNeural Chat7B4.1GBollama run neural-chatStarling7B4.1GBollama run starling-lmCode Llama7B3.8GBollama run codellamaLlama 2 Uncensored7B3.8GBollama run llama2-uncensoredLLaVA7B4.5GBollama run llavaGemma2B1.4GBollama run gemma:2bGemma7B4.8GBollama run gemma:7bSolar10.7B6.1GBollama run solar直接在命令行窗口输入 ollama run 模型名（如llama3），下图是使用qwen0.5b的界面在这个界面就可以进行问答了。若要退出，直接输入/bye即可2.使用其他模型（这里主要使用GGUF格式的模型）下载GGUF模型，下载地址：国内下载大模型的极速通道：替代 Huggingface 的优选方案下载GGUF格式文件后，如下载Llama3-8B-Chinese-Chat.q4_k_m.GGUF，在这个文件的同目录下新建Modelfile文件，内容如下：FROM ./Llama3-8B-Chinese-Chat.q4_k_m.GGUFTEMPLATE \"\"\"{{ if .System }}<|start_header_id|>system<|end_header_id|>{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>{{ .Response }}<|eot_id|>\"\"\"PARAMETER stop \"<|start_header_id|>\"PARAMETER stop \"<|end_header_id|>\"PARAMETER stop \"<|eot_id|>\"PARAMETER stop \"<|reserved_special_token\"在命令行窗口，使用ollama  create  example  -f  Modelfile（example替换为你想使用的名字）使用ollama list可以查看刚才创建的模型了使用ollama run example可以对话了三、使用AnythingLLM界面看到下图的界面总是不舒服，能不能也有ChatGPT一样的界面呢，答案是肯定的，且不止一种。AnythingLLM的部署运行可以参考上一篇文章（江南水乡：本地部署搭建自己的ChatGPT）要在AnythingLLM中使用ollama，只需要作如下设置：注意：Ollama Base URL:http://127.0.0.1:11434，选择刚才自己创建的模型（如qwen:0.5b）就可以使用了。",
        "voteup_count": 0,
        "updated_time": "2024-04-27 22:47:32",
        "question_id": 623672939,
        "user_id": "f5d276802f5b52872235b2171b77e0b7"
    },
    {
        "answer_id": 3262478457,
        "content": "# Top 1: langchain-ai/langchain作者：langchain-ai简介：⚡ Building applications with <em>LLMs</em> through composability ⚡关注人数：65618标签：Python链接：https://www.github.com/langchain-ai/langchain# Top 2: nomic-ai/gpt4all作者：nomic-ai简介：gpt4all: open-source <em>LLM</em> chatbots that you can run anywhere关注人数：53467标签：llm-inference、C++链接：https://www.github.com/nomic-ai/gpt4all# Top 3: binary-husky/gpt_academic作者：binary-husky简介：为ChatGPT/GLM提供实用化交互界面，特别优化论文阅读/润色/写作体验，模块化设计，支持自定义快捷按钮&amp;函数插件，支持Python和C++等项目剖析&amp;自译解功能，PDF/LaTex论文翻译&amp;总结功能，支持并行问询多种<em>LLM</em>模型，支持chatglm2等本地模型。兼容文心…关注人数：44263标签：chatglm-6b、chatgpt、Python、academic、gpt-4、large-language-models链接：https://www.github.com/binary-husky/gpt_academic",
        "voteup_count": 0,
        "updated_time": "2023-10-24 14:48:47",
        "question_id": 623672939,
        "user_id": "70a71f14ca46c4ec7bb94385833ea1f5"
    },
    {
        "answer_id": 3486331624,
        "content": "开源大模型，目前全球有不少，但目前最强的语言大模型应该是Mate在4月18日推出的Llama 3。国内外都各自有不少优秀的开源模型，让我们为这些开源贡献者鼓掌，正是有了他们的无私开源，才有了互联网和人工智能语言大模型的不断进步和追赶。随着人工智能技术的不断入发展，大模型逐渐成为各大科技企业竞争中的前沿赛道。让我们盘点下截止至2024年5月，分别有什么开源的大模型吧！一、开源大模型是什么在当前的生成式人工智能(Generative AI)的科技浪潮中，大型语言模型(Large Language Models, 简称LLM)扮演着不可或缺的角色。LLM是基础上的变革性推动者，其技术核心是Transformers架构，这是一种在我们理解、生成和处理人类语言方面特别强大的神经网络技术。我用AI绘画工具生成的配图这些模型的“大型”之名来源于其庞大的参数数量，它们包含了数亿，甚至是数以万亿计的参数，而这些参数都预先通过大量的文本数据集进行训练所得到。而开发出来的大模型，由于开发成本巨大，因此分别有了开源和闭源两种做法，开源即是将制作内容公开，并放在网络平台可供公众下载至本地，闭源则不公开。业界对开源大模型看法不一。百度CEO李彦宏曾公开表示，开源模型会越来越落后。他认为，“大家以前用开源觉得开源便宜，其实在大模型场景下，开源是最贵的。所以开源模型会越来越落后。”我用AI绘画工具生成的配图而360集团的创始人周鸿祎却表达了不同看法。周鸿祎认为，开源是科技发展的重要推动力。没有开源就没有Linux，而没有Linux就没有今天的互联网。他鼓励企业和开发者们充分利用开源资源，共同推动科技进步。二、国外最新开源大模型1.Llama 32024年4月18日，Meta在官网上宣布旗下最新开源大模型Llama 3发布。目前，Llama 3已经开放了80亿(8B)和700亿(70B)两个小参数版本。根据Meta的官方解读，Llama 3借由在其定制的两个24k GPU集群上，利用超过15TB的数据进行训练，这一数据容量相当于其前身Llama 2使用数据的七倍之多，并且包含的代码量是后者的四倍。此外，Llama 3还增强了上下文处理能力，支持的上下文长度达到8K，是Llama 2处理能力的两倍。Meta还发布了一套全新的、高质量的人类评估数据集。该评估集综合了1800个提示，覆盖了12个核心用途，囊括了从寻求建议和头脑风暴到更复杂的任务，如分类、封闭式和开放式问答、编码、创意写作、信息提取、角色塑造、推理、重写和总结等。在将Llama 3与同期竞争的大型语言模型进行比较时，Meta进行了全面的人类评估研究。这些比较包括了如Claude Sonnet、Mistral Medium以及广泛认可的GPT-3.5等模型。评估者基于构建的评估集进行了细致的偏好排名。结果显示，在模拟真实世界应用的场景下，Llama 3的性能非常出色，赢得了至少52.9%的偏好率。Meta已在GitHub、Hugging Face、Replicate上开源其Llama 3模型，开发人员可使用工具对Llama 3进行定制和微调，以适应特定的用例和需求，感兴趣的开发者可以查看官方的入门指南并前往下载部署。Github地址：https://github.com/meta-llama/llama3/2.Grok-1在2024年3月，马斯克宣布开源的Grok-1，是一个由 xAI 从头训练的模型，拥有高达314亿个参数，采用了混合专家（MoE）层结构。xAI 发布了大型语言模型 Grok-1 的基本模型权重和网络架构，使用了 Apache-2.0 许可证。 根据介绍，Grok 的架构是在 2023 年 10 月使用自定义训练堆栈在 JAX 和 Rust 上开发的，采用了创新的神经网络设计方法。Grok-1有314B的大小，需要有足够 GPU 内存的机器，从网友的推算来看，可能需要一台拥有 628 GB GPU 内存的机器，大概是8个H100（每个 80GB），才有可能使用示例的代码来测试模型。Github地址：https://github.com/xai-org/grok-13. Gemma谷歌在2024年2月，推出了全新的开源模型系列「Gemma」。相比 Gemini，Gemma 更加轻量，同时保持免费可用，模型权重也一并开源了，且允许商用。这次的发布包含两种权重规模的模型：Gemma 2B 和 Gemma 7B。每种规模都有预训练和指令微调版本。想使用的人可以通过 Kaggle、谷歌的 Colab Notebook 或通过 Google Cloud 访问。谷歌在推出Gemma时采取了一个与之前Gemini模型截然不同的策略。通过将Gemma直接推向开源生态系统，谷歌打破了自己之前倾向于保持核心技术封闭的做法。Gemma的开源特性使其与Gemini形成了鲜明对比。如果要使用Gemini，开发者只能通过特定的接口或者在谷歌自家的Vertex AI平台上工作，相比之下，Gemma的开源不仅降低了使用门槛，让开发者可以直接获取模型的源代码和权重，进行自由修改和再训练以适应特定需求。官方地址：https://ai.google.dev/gemma/4.Mistral LargeMistral AI被称作“法国版 OpenAI”，Mistral Large 是 Mistral AI 的最新旗舰版，特点是具备顶级的的推理能力，可用于处理复杂的多语言推理任务，涵盖文本理解、转换和代码生成等。作为一款欧洲制作的大模型，Mistral Large支持英语、法语、西班牙语、德语和意大利语，据称它能够提供对这些语言的深层次理解，不仅能精准把握各语言的复杂语法规则，还能细腻捕捉文化背景中的微妙差异。该模型配置了32,000个token的上下文窗口，确保在分析长达约24,000英文单词的文档时，能精确抽取每一个核心信息点，无遗漏。在功能设计上，Mistral Large强调指令的精确执行能力，使开发者能轻松制定并实施个性化的审核与管理策略。此外，它原生集成了函数调用功能与输出模式的自定义限制，极大地促进了应用程序的扩展能力，简化了技术集成过程，加速了开发团队技术栈的现代化迭代。官方地址：Mistral AI | Frontier AI in your hands三、国内最新开源大模型1.Qwen 2024年2月份，通义千问大模型（Qwen）的 1.5 版上线了。新版大模型包括六个型号尺寸：0.5B、1.8B、4B、7B、14B 和 72B，其中最强版本的性能超越了 GPT 3.5、Mistral-Medium，包括 Base 模型和 Chat 模型，且有多语言支持。Qwen 1.5 的发布还有如下一些重点：支持 32K 上下文长度；开放了 Base + Chat 模型的 checkpoint；可与 Transformers 一起本地运行；同时发布了 GPTQ Int-4 / Int8、AWQ 和 GGUF 权重。借助更先进的大模型作为评委，通义千问团队在两个广泛使用的基准 MT-Bench 和 Alpaca-Eval 上对 Qwen1.5 进行了初步评估，评估结果如下：在不同模型尺寸下，Qwen1.5 都在评估基准中表现出强大的性能，72B 的版本在所有基准测试中都超越了 Llama2-70B，展示了其在语言理解、推理和数学方面的能力。GITHUB地址：https://github.com/QwenLM/Qwen1.52.ChatGLM智谱AI推出的ChatGLM系列，作为一套高性能的大规模语言模型，凭借其卓越的效能与开放源代码政策，在国内外的大模型领域均享有高度声誉。ChatGLM系列是国产大语言模型中最强大最著名的模型之一。在第一代ChatGLM-6B在2023年3月份推出，开源推出之后不久就获得了很多的关注和使用。3个月后的2023年6月份，ChatGLM2发布，再次引起了广泛的关注。2023年10月27日，智谱AI携手清华大学再度发力，发布第三代基础大语言模型ChatGLM3系列。这次发布的第三代模型共包含3个：基础大语言模型ChatGLM3-6B-Base、对话调优大语言模型ChatGLM3-6B和长文本对话大语言模型ChatGLM3-6B-32K。如下所示：模型版本模型介绍上下文长度开源情况DataLearner模型信息卡地址ChatGLM3-6B-Base基础大语言模型，预训练结果8K免费商用授权https://www.datalearner.com/ai-models/pretrained-models/ChatGLM3-6B-BaseChatGLM3-6B基础大语言模型针对对话微调调优，适合对话8K免费商用授权https://www.datalearner.com/ai-models/pretrained-models/ChatGLM3-6BChatGLM3-6B-32K对话调优的大语言模型，但是支持32K上下文32K免费商用授权https://www.datalearner.com/ai-models/pretrained-models/ChatGLM3-6B-32KGITHUB地址：https://github.com/THUDM/ChatGLM33.SkyworkSkywork-13B是一系列基于Transformer架构的大语言模型（LLMs），由昆仑万维开源。这些模型在超过3.2万亿个标记的多语言数据集上进行了预训练，主要涵盖中文和英文文本12。Skywork-13B系列包括两个主要模型：Skywork-13B-Base和Skywork-13B-Math。Skywork-13B系列在多个领域展现出了性能跃升，尤其是在文本生成、创意写作及数学逻辑推理等任务上表现卓越。同时它还慷慨分享了其模型评估标准、数据配置策略以及训练基础设施的优化方案，极大地促进了知识共享与技术创新。其开源模式尤为友好——用户仅需下载模型并同意《Skywork模型社区许可协议》，无须额外的商业授权步骤即可直接应用于商业场景，同时摒弃了对使用者的行业背景、公司规模及用户基数的限制，展现了极高的开放性。GITHUB地址：https://github.com/SkyworkAI/Skywork4.书生书生开源大模型是由上海人工智能实验室联合商汤科技等机构共同开发的一系列多模态、多任务通用大模型。这些模型包括但不限于书生·多模态、书生·浦语和书生·天际等基础模型，以及针对特定领域如视觉、语言处理的模型。书生开源大模型的特点在于其开源性质，旨在为学术界和产业界提供全链条的研发与应用支持，从而推动人工智能技术的发展和应用。其中，书生·多模态包含200亿参数，由80亿海量多模态样本训练而成，支持350万语义标签的识别和理解，覆盖开放世界常见的类别和概念。书生·多模态模型挑战AI传统局限，迈向真实世界的复杂性，掌握超350万种开放语义，涵盖日常各类物体、动作及字符，实现从预设任务到开放场景的跨越。模型展示了从图像到文本的创造性转化能力，以名画《湖山清夏图》为灵感创作七言绝句，融合深厚的文化底蕴与艺术解析力。通过分析画作元素、构思诗意并遵循诗词格律，其创作过程体现了对跨模态联合学习的成功应用，及对中国古典文学的借鉴，如借鉴韦庄诗意构造佳句。GITHUB地址：https://github.com/InternLM/InternLM结语通过以上的介绍，你是否了解目前的主要开源大模型有哪些呢？希望我的介绍对你有价值。我是德里克文，一个对AI绘画，人工智能有强烈兴趣，从业多年的室内设计师！如果对我的文章内容感兴趣，请帮忙关注点赞收藏，谢谢！",
        "voteup_count": 17,
        "updated_time": "2024-05-03 19:11:32",
        "question_id": 623672939,
        "user_id": "a1fa12f6fb4bcbf17af64a52cda69b86"
    },
    {
        "answer_id": 3482263122,
        "content": "开源模型的发展实在是太快了，Gemma，Mixtral，LLama3。。。“最强开源模型一夜易主”这样的新闻标题，几个月换换主角就发一遍，都快用烂了。不同的基座模型加上徒子徒孙们，数是数不完的，所以还是按着谱系缕会更清晰一些。文中提到的一些宝藏合集、工具和资源链接我会附在文章末尾，Here we go。开源大模型谱系常见底座模型概览：转自：Awesome Chinese LLM不同的底座模型各有特色和优势。比如，ChatGLM作为一款可商用的模型，其参数大小和训练token数都非常可观，适合于生成式对话等应用场景（尤其擅长中文）。LLaMA以其多样的参数大小和训练最大长度，提供了更多的选择空间，但是在商用方面是有一定限制的。而DeepSeek则在参数大小上提供了更多的选择，可以适应不同的应用需求。这些开源基座衍生出无数分支与变体，共同构筑了一个生机勃勃、枝繁叶茂的开源项目森林。转自：Awesome Chinese LLM常见的开源大模型介绍Llama 3基于超过15万亿token的数据集进行训练，是Llama2数据集的7倍还多，为模型提供了更丰富的信息基础。 支持8K长文本，配备了改进的tokenizer，词汇量高达128K，性能表现优异。缺点是会有飙英文的现象，建议试试开源社区里的中文微调版本。ChatGLM3智谱AI和清华大学 KEG 实验室联合发布的新一代对话预训练模型，国产之光。ChatGLM3在保留了前两代模型对话流畅、部署门槛低等众多优秀特性的基础上，采用了更多样的训练数据、更充分的训练步数和更合理的训练策略。ChatGLM3系列原生支持工具调用、代码执行和Agent任务等复杂场景，为用户提供了更为全面和高效的功能支持。Qwen通义千问阿里云研发的通义千问大模型系列模型，包括参数规模为18亿（1.8B）、70亿（7B）、140亿（14B）和720亿（72B）。拥有广泛的知识领域和强大的分析能力，可应用于智能搜索、问答系统等领域，为用户提供便捷、高效的智能交互体验，能有效调用插件以及升级为Agent。Mistral 7B由Mistral AI开发的70亿参数LLM。它在所有基准测试中都优于Llama 2 13B，在许多基准测试中超过了Llama 1 34B，并使用滑动窗口注意力（SWA）来优化模型的注意力过程，显著提高了速度。不过国外的都有个共同的伤，更擅长英语任务。Yi 模型李开复团队零一万物发布的开源预训练大模型。能够一次处理40万汉字，曾成功登顶Hugging Face开源模型榜首的中文模型。中英均表现优异，具有强大的语言理解和生成能力，适用于多种自然语言处理任务。在本地搭建部署开源模型从个人学习的角度来说，其实还是有点门槛的，硬件资源就是一关。建议从参数规模较小的7B左右开源模型开启尝试。比如清华的GLM系列，meta的llama系列都可以。推荐使用Ollama，Ollama是在Docker容器中部署和管理大型语言模型（LLM）的工具，可以大大简化部署过程。安装好ollama后，通过命令就可以直接下载大模型。Ollama官网有列出它支持的模型库，可以看到连Llama3都包括了，还是很香的：不过Ollama是命令行交互，不太方便，可以通过安装一个OpenWebUI来在浏览器使用。如果嫌自己摸索太麻烦，正好最近有一个知乎知学堂联合AGI课堂推出的程序员的AI大模型进阶之旅免费公开课。这个课正是为了适应当下AI大模型的发展而推出的，由几位业内大佬主讲，主要面向的就是想系统性学习AI大模型的同学。会体系化地介绍开源、闭源大模型的技术路径，以及在此基础之上的应用开发技术栈。像如何fine-tune模型、Langchain技术等等都会有全面涉及。重要的是有 AI 大模型学习资源包，以及好用的 AI工具等。感兴趣的可以关注一下 ↓ ↓ ↓线上体验ShirtAI如果想先从线上进行体验，可以试下ShirtAI。ShirtAI是一款集成全球大模型全方位AI产品，问答、绘画、PDF对话解析、AI TTS语音等功能都有了，非常省心。主流的开源、闭源模型都可以选择。网页版：https://www.myshirtai.com/IOS版本：https://apps.apple.com/us/app/shirtai/id6474819973huggingface chat有科学上网工具的，可以到huggingface 的体验页面直接体验，支持很多开源模型：https://huggingface.co/chat/如何选择开源模型选择开源 LLM ，一定要从实际情况出发。任务目标：每种模型都有自己的优势和短板，在模型选择时，一定要结合自己的任务需求，寻找适合的模型。成本：开源模型看似免费，但是需要考虑计算资源、存储资源、能源消耗、技术储备、后期维护等一系列成本。毋庸置疑的是，LMM 规模越大、越复杂，成本越高。精度：可以通过对模型在类似任务上的性能基准进行考察，比较不同的 LLMs 执行你所需的任务类型的准确程度。数据安全：尤其在商业领域，数据安全是重要的考虑因素。 可以从隐私保护、合规等方面进行综合考虑。社区和支持：一个活跃的开源社区可以提供技术支持、更新和改进，良完善的文档和教程可以帮助你更快地掌握模型的使用。参考资料：非常宝藏的中文LLM大合集：https://www.github-zh.com/projects/643916827-awesome-chinese-llmHuggingFace的开源大模型排行榜Open LLM Leaderboard - a Hugging Face Space by HuggingFaceH4集成多个开源模型的聊天页面Huggingchat：https://huggingface.co/chat/网页体验Llama3：Chat with Meta Llama 3 on Replicate (llama2.ai)Ollama：GitHub - ollama/ollama: Get up and running with Llama 3, Mistral, Gemma, and other large language models.OpenWebUI：GitHub - open-webui/open-webui: User-friendly WebUI for LLMs (Formerly Ollama WebUI)",
        "voteup_count": 133,
        "updated_time": "2024-04-29 09:42:40",
        "question_id": 623672939,
        "user_id": "6736256b9f1cc119e59101b6562cc6c5"
    },
    {
        "answer_id": 3439756668,
        "content": "GPT-4的爆火，彻底掀起了学术界对于多模态大模型的研究热潮；而开源大模型更是霸占了模型界的半边天；看基本上大家介绍得都七七八八了，那我就就着最近几个热度比较高的开源模型介绍一下吧；① Gemma北京时间2月21日晚21点，美国科技巨头谷歌宣布推出全球性能最强大、轻量级的开源模型系列Gemma；分为2B（20亿参数）和7B（70亿）两种尺寸版本，2B版本甚至可直接在笔记本电脑上运行，7B性能远远超越Llama 2 13B！Google DeepMind CEO Demis Hassabis表示，轻量开源的Gemma是同类尺寸中性能最佳的模型！如今，谷歌不仅将对手瞄向OpenAI，而且打算独占鳌头，新模型将Llama-2碾在脚底，遥遥领先。② Grok-13月18日，马斯克旗下AI初创企业xAI今天发布新闻稿，宣布正在开源3140亿参数的混合专家模型 Grok-1，该模型遵循 Apache 2.0 协议开放模型权重和架构，号称是“迄今为止全球参数量最大的开源大语言模型”。Grok-1是一个混合专家（Mixture-of-Experts，MOE）大模型，这种MOE架构重点在于提高大模型的训练和推理效率，这个技术将大模型划分为不同的子专家模型，每次推理只选择部分专家网络进行推理，在降低成本的同时保证模型的效果；Grok-1包含了8个专家，总参数量高达314B（3140亿），处理Token时，其中的两个专家会被激活，激活参数量为86B；并且在马斯克数次嘲讽Open AI是“Close AI”之后，xAI现已将Grok-1的权重和架构在GitHub上开源，似乎有股想要“身体力行”敦促Open AI恢复开源的意味了~大部分模型在多种任务上表现优秀，但是仍需要大量的计算资源和专业知识来训练和优化，对于普通人来说，其实许多现成的落地产品，就已经足够解决日常需求；✨ 智能表格处理：Sheet+Sheet+是一个AI驱动的Excel和Google Sheets工具，可以从文本生成Google Sheets和Excel公式，将公式转换为简单的解释，调试公式等，帮助我们节省时间，简化电子表格工作；Sheet+是一个创新的AI-powered工具，它可以将Google表格和Excel公式编写速度提高10倍，抛弃繁琐的公式编写，将公式转换为简单的解释、调试公式等~Sheet+最大的特点就是它的易用性，它与Excel和Google Sheets兼容，使我们可以在任何自己喜欢的平台上使用，即使你不是Excel或Google Sheets的专家，你也仍然可以使用Sheet+轻松创建复杂的公式~✨ 灵感创意生成：AI创意生成家“创意生成家”大概就是所有使用过这个工具的人对它的印象了！“创造力”绝对是这款软件的最大亮点！集合了AI写作、AI绘画、AI聊天等AI功能于一体，通过AI让我们可以将更多的精力投入到创造性的任务中，释放出更多的创意和想象力！软件AI写作功能，能够支持到工作总结、培训方案、岗位事迹、论文开题报告、商业计划书、活动策划、视频脚本等等文章内容的方向，都属于非常实用的写作类型！而且其它功能表现也不俗！譬如AI绘画，功能能够支持“图生图”、“文生图”两大主流AI绘画模式，并且简洁干净的UI界面，也让许多新手小白表示能够快速上手；各种类型的模板对于一些毫无基础的用户非常友好，完美地契合这些用户的眼光和需求，只需要选择到钟意的风格，简单输入自己的需求，调整设置后即可生成输出！当然，软件中的其它各种AI配音、AI特效、AI转换等等功能，输出创意也都非常不错！✨ 视频智能合成：Runway Gen-2曾经AI视频界的王者，仅突破几秒内的连贯性就搏得业内欢呼，而如今却被Sora的光芒完全掩盖，但大鹏依然觉得值得推荐！Gen-2是一个多模态的人工智能系统，可以生成带有文本、图像或视频剪辑的新颖视频，8种视频模式自由选择，只需要打字或上传图像就能生成逼真的合成视频~Gen-2的操作方法非常简单，在屏幕中间点击上传一段视频，完成上传后，左侧可以预览视频，右上方可以选择各种不同风格，下方可以调整参数，自定义程度非常高~大模型开源与闭源之争是人工智能领域的一个热门话题，开源和闭源各有优缺点，应根据不同的应用场景选择合适的模式；未来，即便发展方向再怎么变，开源成了主流，但闭源也永远不会沉没。又是干货满满的分享，我看谁还没点赞收藏喜欢，有什么意见也可以在评论区直说，@视频编辑助手绝对欢迎！",
        "voteup_count": 13,
        "updated_time": "2024-03-22 16:19:59",
        "question_id": 623672939,
        "user_id": "63417af9dec64a7241e1465f2ecadf08"
    },
    {
        "answer_id": 3225832217,
        "content": "据我所知，目前广泛可用性的开源模型是Llama-2-70B，可以根据提示词，写出不错的创意性文章，影评等主观性比较强的文本，内容丰富，情绪饱满，强于ChatGPT 3.5，ChatGPT 3.5的英语版本的创意表现，也比不过Llama270。但问答逊于ChatGPT 3.5，有时候，会比ChatGPT更好，稳定性不如ChatGPT。Llama-13B也不错。你可以用ChatGPT（问答)+Llama 270（创意），搭配出一个不错的免费体验。其他的开源模型，都在爬各种测试榜，我感觉实际可用性不大。测试榜有大的水分，针对性优化，有极强的迷惑性。大模型，并不是容易的事。我认为中国还没有掌握大模型的核心技巧，虽然他们声明自己可以在榜单上做得有多好。",
        "voteup_count": 8,
        "updated_time": "2023-09-25 14:27:52",
        "question_id": 623672939,
        "user_id": "bea84fe95aee12a6939ee59dde16d1b3"
    },
    {
        "answer_id": 3239611227,
        "content": "现在开源大模型挺多，感觉新闻一会一个，容易花多眼乱。不过，其实只要捋一捋，关系就清晰了。怎么捋呢？关键在于谱系。开源模型虽然多，但大多数都是从Meta的Llama派生出来的。简单来说，就是用了Llama作为基模型，然后选用其它不同的训练方法微调。所以，记住Llama，就记住了大多数开源大模型。比如说Alpaca。Alpaca同样也是一款具有里程碑意义的开源大模型，听说的人很多，但这款模型其实也是Llama的派生模型，也就是说，其底层结构和Llama是完全一致的，不同之处在于微调。Alpaca使用Llama作为基模型，然后使用了自建的数据集进行有监督学习。自建数据集的过程比较有意思，研究人员首先人工编写了175条提示，输入openai的text-davinci-003模型（因为ChatGPT当时还没提供API），然后得到52K条instruction-following的样本，形成数据集。最后用来给Llama-7B做有监督学习微调，得到的新模型就是Alpaca。类似的做法很多，另一个很有名的比如Vicuna，用的是http://ShareGPT.com的数据。后来又出现了一些中文Llama，复杂一点，因为Llama原生不怎么支持中文，具体来说就是词表只有少量中文词元，需要对词表做二次处理。但模型结构也是一致的。Llama还在不断迭代，现在已经出到2了，meta还推出了Code Llama，不过底层模型还是Llama，在代码生成上做了优化。国外的开源大模型知名的还有Falcon，有一段时间说很厉害，远超Llama，后来又说是指标计算问题，还是Llama强一点。国内也有一些自研的开源大模型，不是中文Llama而是底层结构也自己设计自己训练，一个字，有钱。名气大的有三个，阿里通义千问的开源版Qwen模型，我用过觉得不错的ChatGLM，现在出了2，还有王小川的Baichuan，现在也出了2。不过，从名气从能力从生态来说，Llama都是开源大模型的扛把子。总之，记住Llama，就记住了大多数开源大模型。上面说的是开源大语言模型，也就是LLM。开源AI模型就更多了，比如说AI绘画的SD，还有一些搞多模态的。",
        "voteup_count": 18,
        "updated_time": "2023-10-07 10:29:38",
        "question_id": 623672939,
        "user_id": "92c8c683498d310e028124afff90fe6b"
    },
    {
        "answer_id": 3286312505,
        "content": "本文是开源 LLM 发展史系列文章的第三部分。此前，第一部分《开源语言大模型演进史：早期革新》回顾了创建开源 LLM 的最初尝试。第二部分《开源语言大模型演进史：高质量基础模型竞赛》研究了目前可用的最受欢迎的开源基础模型（即已进行预训练但尚未微调或对齐的语言模型）。本文将介绍如何通过微调/对齐那些更出色的LLaMA-2等开源模型来提升它们的效果，并缩小开源和私有LLM之间的差距。​（本文作者为Rebuy公司AI总监、深度学习博士Cameron R. Wolfe。以下内容经授权后由OneFlow编译发布，转载请联系授权。原文：https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-imitation）作者 | Cameron R. WolfeOneFlow编译翻译｜杨婷、宛子琳（引自[1,2,12]）在开源语言大模型之前的研究中，大部分关注点都集中在创建预训练基础模型上。然而，由于这些模型没有经过微调，缺乏对齐，它们的质量无法匹敌顶级的闭源 LLM (如 ChatGPT 或 Claude )。付费模型通常使用 SFT（监督微调）和 RLHF（人类反馈强化学习）等技术进行了全面对齐，这极大地提高了模型的可用性。相比之下，开源模型通常只使用较小的公开数据集进行小规模微调。本文将着眼于最近的研究，通过更广泛的微调和对齐来提高开源 LLM 的质量。（引自[17,18]）对齐过程。本文将研究开源 LLM 的微调和对齐过程。在进入正题之前，我们需要了解什么是对齐以及如何对齐。语言模型的训练通常可分为几部分，如上图所示，首先我们需要对模型进行预训练，然后进行多个步骤的微调。预训练之后，LLM 能够准确预测下一个词元，但其输出的内容可能会比较重复和单调。因此，需要对模型进行微调以改善其对齐能力，让模型生成与人类用户期望相一致的文本（如遵循指令、避免有害输出、防止虚假陈述，生成有趣或富有创造性的输出等）。（引自[17]）监督微调（SFT）。对齐可通过两种微调技术实现：监督微调和从人类反馈中进行强化学习。可参考上图获取更多详细信息。SFT 对模型进行简单微调，使用标准的语言建模目标，在高质量的提示和响应示例上进行训练。LLM 可以从这些示例中学习到如何进行适当的回应！SFT 非常简单有效，但需要精心策划一个能够捕捉到“正确”行为的数据集。RLHF。直接使用来自人类标注者的反馈对 LLM 进行训练，人类标注者会识别出他们喜欢的输出，然后 LLM 再学习如何生成更多类似的输出。为实现这一目标，我们首先需要一组提示，并在每个提示上由 LLM 生成多个不同的输出。之后，让人类标注者根据输出的质量对每个生成的回应进行评分。这些评分可用来训练奖励模型（即带有额外回归头的微调版本的 LLM ），用于预测每个回应的分数。接着，RLHF 通过 PPO 强化学习算法对模型进行微调，将得分最大化。通常，高质量的 LLM 会通过 SFT 和 RLHF（使用大量人类反馈）按顺序进行对齐。1 模仿学习（引自[16]）在 LLaMA [3]发布后，开源研究社区终于能够使用强大的基础 LLM 进行各种不同应用的微调或对齐，这使得开源 LLM 的研究经历了爆发式增长，从业者们争相在自己选择的任务上对 LLaMA 模型进行微调。有趣的是，在此期间，研究中最常见的方向之一是模仿学习。模仿学习在某种程度上可以看作是一种对齐方法，它通过对另一个更强大的 LLM 的输出进行微调来优化 LLM。这种方法受到了知识蒸馏的启发。具体细节可参考上图。“模型模仿的前提是，一旦通过 API 提供了专有 LM，就可以收集 API 的输出数据集，并使用该数据集对开源 LLM 进行微调。” – 引自 [6]开源模仿学习研究提出的问题很简单：我们是否可以通过仅对 ChatGPT 或 GPT-4 生成的响应进行微调，创建出与它们一样强大的模型？对此，我们可采用以下的简单方法进行预测： 收集这些模型的对话示例（如使用 OpenAI API） 在这些数据上进行（监督）微调（使用正常的语言建模目标）研究社区对于模仿学习是否为一种有价值的学习方法进行了长时间的激烈讨论。最终，我们发现这种方法在实践中能够奏效，但只在特定条件下才能发挥出良好效果。2 模仿学习的初步努力LLaMA 催生了众多的模仿模型（引自[7,8,9,10]）LLaMA 发布之后，研究人员使用 ChatGPT 的派生对话，迅速发布了各类模仿模型。通常，用于训练这些模型的数据（禁止用于商业用途）可以从 OpenAI API 或类似 ShareGPT 处获得。下面按时间顺序总结了一些广为人知的模仿模型。Alpaca [7] 通过 self-instruct [11]框架，在 GPT-3.5（即 text-davinci-003 ）中自动收集微调数据集，对 LLaMA-7B 进行微调。收集数据和对 Alpaca 进行微调仅需600美元。Vicuna [8] 使用 ChatGPT（从 ShareGPT 派生）的 70,000 个对话示例对 LLaMA-13B 进行微调。有趣的是，Vicuna 的整个微调过程仅需 300 美元。Koala [9] 在来自 Alpaca 的微调数据集和其他来源（如 ShareGPT、HC3、OIG、Anthropic HH 和 OpenAI WebGPT/Summarization）的大量对话示例上对 LLaMA-13B 进行了微调。与之前的模仿模型相比，Koala 在更大的数据集上进行了微调，并进行了更全面的评估。GPT4ALL [16] 在来自 GPT-3.5-turbo 的80万个聊天补全（chat completions）上对 LLaMA-7B 进行了微调。作者还发布了训练/推理代码和量化模型权重，可以在计算资源较少的情况下进行推理（如使用笔记本电脑）。（引自[8,9]）模仿的影响。这些模仿模型相继发布，并声称其质量可与 ChatGPT 和 GPT-4 等顶级专有模型相媲美。例如，Vicuna 能达到相当于 GPT-4 模型质量的 92%，Koala 在多数情况下能达到或超过 ChatGPT 模型表现，可参考上图。上述结果表明，通过模仿学习，我们可以提取任一专有模型的能力，并将其转化为一个更小的开源 LLM 。假如情况果真如此，那么人们将能够轻松复制最优秀的专有 LLM ，从而导致专有 LLM 丧失其优势。“开源模型具有快速、可定制、更私密、更高质量等特点。开源社区仅需 100 美元和 13B 参数，就能做到谷歌需花费 1000 万美元和 540B 个参数才能勉强完成的任务。此外，谷歌为了实现这些目标需要几个月的时间，而开源模型只需几周便可完成。” ——引自[9]模仿模型的爆炸式增长使开源模型首次被真正视为闭源 LLM 的潜在替代品之一，自 GPT-3 提出以来，闭源 LLM 就主导了这一领域。尽管使用付费 API 逐渐成为了标准，但模仿模型的出色表现为开源 LLM 带来了希望。3 模仿模型是否现实可行？（引自[6]）尽管模仿模型的出色表现给人们带来了希望，但 [6] 表明，我们忽略了一些重要因素，即需要对这些模型进行更具针对性的评估，经过评估，我们发现模仿模型的表现远不及 ChatGPT 和 GPT-4等顶级专有 LLM 。事实上，在大多数情况下，通过模仿来微调基础模型并不能弥合开源模型与专有模型在效果上的巨大差距。相反，由此产生的模型仅能更好地处理在微调集中大量呈现的任务，但也更容易出现幻觉。（引自[6]）实验设置：为评估模仿学习的效果，[6]中的作者从 ChatGPT 中筛选了约 130000 个不同的对话示例，并构建了一个数据集。然后，他们对不同规模的语言模型使用不同数量的模仿数据进行微调，并衡量了它们的效果。上述实验可得出以下几个有趣的观察结果：在人类评估试验中，用于微调的模仿数据量并不能提高模型质量。模仿模型在标准化基准测试中的表现通常不如基础模型（当使用更多模仿数据时，质量甚至会下降）。增加基础模型的大小能逐步提升模仿模型的质量。其中到底发生了什么？当在更广泛的自然语言评估基准上对模仿模型进行评估时，我们发现模仿模型的质量与其对应的基础 LLM 相当或稍差。换句话说，模仿模型实际上并不能与 ChatGPT 等模型的质量相匹敌。与专有 LLM 相比，这些模型的知识基础较为有限，这一点可以从更大的基础模型的质量改进中得到印证。“在我们看来，相比模仿专有系统，改进开源模型的最佳策略是解决核心难题，即开发出更优秀的基础 LLM 。”——引自[6]关于这一点，我们首先需要考虑：为什么这些模型的表现如此出色？在[6]中我们看到，模仿模型学会了模仿 ChatGPT 等模型的风格。因此，即使模型更频繁地生成事实错误信息（这些信息更难以进行简单的检查或验证），人类工作者仍会受蒙蔽，认为模型输出的是优质内容。4 模仿学习是否真的有用？“研究表明，从逐步解释中学习（无论这些解释由人类生成还是由更先进的 AI 模型生成）是改进模型能力和技能的一种有效途径。”——引自[1]在[6]中发现，模仿模型的表现并不像最初预计的那样出色，研究界对于模仿模型的真正价值感到困惑。然而，值得注意的是，[6]的分析中指出，局部模仿——即学习模仿模型在特定任务上的行为，而非模仿其整体行为——是相当有效的。然而，这并不意味着模仿模型可大致与专有模型的质量相匹配。为了整体提升模仿模型的质量，[6]的作者提出了两个途径：生成一个更大、更全面的模仿数据集创建一个更好的基础模型用于模仿学习有趣的是，随后的研究对这两条途径进行了深入探索，结果表明二者都能产生积极效果。（引自[12]）Orca [12] 是基于 LLaMA-13B 的一种模仿模型。然而，与之前的模仿学习工作相比，Orca 是在从 ChatGPT 和 GPT-4 收集的质量更高、更详细、更全面的数据集上进行训练的。之前用于模仿学习的数据集是一种“浅层”数据，只包含了 ChatGPT 等模型生成的提示和响应示例，可参考上图。“我们得出的结论是，如果要纯粹通过模仿来广泛匹配 ChatGPT，就需要集中精力收集大量的模仿数据集，并收集比当前可用数据更加多样、质量更高的模仿数据。”——引自[6]为改进浅层模仿，Orca 尝试通过以下方式增加由 ChatGPT 或 GPT-4 等模型生成的模仿数据集：解释跟踪逐步思考过程复杂指令为此，私有LLM被提示通过指令或系统信息提供其响应的详细解释。通过向模仿模型可见的数据增加额外的有用信息，这种方法超越了简单的提示——响应对。当从 ChatGPT 等强大模型中进行学习时，Orca 不仅只看到模型的响应，还可以从详细解释和思考过程中学习，这些解释和思考过程与模型的响应在复杂提示上共同生成。下面是一个示例以作说明。（引自[12]）在使用这种详细的模仿数据集（来自 ChatGPT 的 500 万个示例和来自 GPT-4 的 100 万个示例）进行大规模微调之后，与先前的模仿模型相比，Orca 的表现非常出色，可参考下图。虽然 Orca 显著缩小了开源模仿模型与私有 LLM 之间的差距，但我们仍然可以从下表中看到，该模型在质量上始终不及 GPT-4。不幸的是，即便改进了模仿方法，开源模仿模型也无法完全匹敌顶级专有模型的质量。然而，Orca 的出色表现表明，模仿学习是一种有价值的微调策略，可显著改善高质量基础 LLM 的质量。进一步的研究发现，在[12]中，成功利用模仿学习有两个重要要求：一个大规模、全面的模仿数据集每个响应都包含详细的解释跟踪更好的基础 LLM。尽管[6]中的作者认为，收集足够大且多样化的模仿学习数据集非常困难，但 Orca 的例子证明了这种做法的可行性。此外，随后的研究还探索了[6]中的另一个途径：创建更强大的（开源）基础模型。虽然最初开源预训练 LLM 的表现不佳，但我们最近看到了各种强大的预训练 LLM 的发布，如 LLaMA [3]、MPT [14，15]和Falcon [13]。鉴于模型预训练是后续微调（如模仿学习、SFT、RLHF 等）的起点，预训练更强大的基础模型会改善下游模仿模型的质量！本系列文章的第二部分涵盖了所有最优秀的开源预训练语言模型。5 对齐开源 LLM（引自[5]）模仿学习试图通过对专有语言大模型（LLM）的响应（及其解释轨迹）进行训练，从而提升开源基础模型的质量。尽管在一些特定情况下，这种方式取得了可喜成果，但（显然）这并不是顶级专有模型接受训练的方式——模仿只是快速构建强大开源模型的捷径。如果我们期望开源 LLM 能与专有模型的表现相媲美，就需要在对齐方面加大投入。“这些闭源的产品级 LLM 都经过了大规模微调，使其更符合人类偏好，从而极大地提升了模型的易用性和安全性。然而，这个步骤可能需要投入大量的计算资源和人工标注，且往往缺乏透明度，难以被复现。\" ——引自 [1]对齐的阻碍是什么？对齐开源模仿模型的想法看似很简单，既然已经有了出色的基础模型，为什么不直接复制 GPT-4 这样的模型的对齐过程呢？问题在于，对齐过程需要大量的计算和人工标注资源，并且严重依赖专有数据，这限制了透明度，使得复现结果变得十分困难。因此，一段时间以来，开源模型在对齐研究方面一直滞后于其对应的专有模型。接下来我们将探讨两项最近的研究 —— LIMA [2] 和 LLaMA-2 [1]，它们通过更好的对齐显著提高了开源 LLM 的质量。6 开源对齐的前期工作在介绍 LIMA 和 LLaMA-2 之前，值得注意的是，开源研究社区并没有完全回避对齐预训练模型。例如，Falco-40B-Instruct [13] 对来自 Baize（Chatbot） 的 1.5 亿个词元进行了结构微调（SFT）。类似地，还发布了许多经微调的 MPT-7B [14] 和 MPT-30B [15] 的变体，其中包括 Chat/Instruct 变体，在公共数据集上经过 SFT 处理，以及 StoryWriter 变体，在上下文更长的的数据上进行了微调。（引自开源LLM排行榜）此外，如果我们简单观察一下开源 LLM 排行榜（如上图所示），会发现不同模型都在各类数据集上通过 SFT 进行了微调，开源 LLM 并没有完全回避对齐过程。然而，顶级的专有模型则同时使用了 SFT 和 RLHF，在大规模高质量的对话和人类反馈数据集上进行对齐；与之相比，大多数开源模型仅使用质量较低且缺乏多样性的公共数据集进行对齐。因此，要想真正与专有模型的质量相媲美，开源 LLM 需要尝试复现其对齐过程。7 LIMA：数据高效对齐[2]“模型的知识和能力几乎完全来自于预训练阶段的学习，对齐过程则教会了模型在与用户交互时应使用哪个子分布格式。”——引自 [2]如上文所述，长期以来，开源 LLM 主要通过在公共数据集上进行 SFT 来对齐。考虑到对 SFT 的重视，[2]中的作者对预训练 LLM 中 SFT 的作用进行了深入探究。目标在于揭示预训练和通过 SFT 对齐在创建高质量 LLM 中的相对重要性，并揭示在经过 SFT 后最大化模型性能的最佳实践。数据集：为实现这一目标，[2]中的作者构建了一个包含 1000 个对话示例的小型数据集用于SFT。尽管数据量看起来不大，但这个数据集中的示例都经过精心筛选，通过提示的多样性和输出风格或语气的统一性，确保了数据集的质量。详见下图。（引自 [2]）尽管用于训练 LIMA 的 SFT 数据集规模较小，但其质量非常出色。有趣的是，在 [2] 中我们可以看到，LIMA 经过该数据集的微调后表现异常出色，甚至接近于 GPT-4 和 Claude 等 SOTA LLM 的表现，详见下图。（引自 [2]）这一结果表明，语言模型可以通过少量精心筛选的示例有效对齐。尽管 LIMA 的表现仍然不及 GPT-4，但能够用如此少的数据实现如此高质量的对齐，这样的结果既出乎意料又令人印象深刻。这一发现告诉我们，在进行 SFT 时，数据质量似乎是最重要的因素。我们能够学到什么？我们从 LIMA 中学到了许多有价值的经验。首先，对于 SFT 而言，仅增加数据量是不够的，数据质量也至关重要，这一点在上图中有详细说明。此外，在[2]中，关于\"表面对齐假设\"提出了一个独特的全新视角来解决对齐问题。简而言之，这个假设认为大多数 LLM 的核心知识是在预训练过程中学习的，而对齐的关键则是找到适当的格式或风格来展现这些知识。因此，对齐能够以一种数据高效的方式进行学习。8 LLaMA-2：提高对齐研究的透明度 [1]“Llama 2-Chat 是数月研究和对齐技术迭代应用的结果，其中包括指令调整和RLHF，需要大量的计算和标注资源。”——引自 [1]近期发布的 LLaMA-2[1] 套件由几个开源模型组成，其参数大小在 70 亿到 700 亿之间。与其前身 LLaMA-1 相比，LLaMA-2 的预训练数据要多 40%（2 万亿个词元），具有更长的上下文长度，并采用了针对快速推理进行优化的架构（分组查询注意力[4]）。LLaMA-2 成为了开源模型的 SOTA。然而，LLaMA-2 套件中不仅包含预训练的 LLM，还通过大规模对话数据和人类反馈对每个模型进行了微调（同时使用 SFT 和 RLHF），并投入了大量精力进行对齐。由此得到的模型被称为LLaMA-2-Chat。（引自 [5]）LLaMA-2 这些经优化的版本表现出色，并在弥合开源和专有 LLM 之间的对齐差距迈出了重要一步。LLaMA-2 的对齐过程强调两个关键的行为特性：1. 有用性：模型满足用户的请求并提供所需信息。2. 安全性：模型避免回复“不安全”的内容。为确保对齐后的模型既有用且安全，根据以上原则，对为 SFT 和 RLHF 提供的数据进行了筛选、收集和标注。（引自 [1]）SFT：LLaMA-2 对齐过程的第一步是使用 SFT 进行微调。与其他开源 LLM 类似，LLaMA-2 首先在公开可用的指令调优数据上进行微调。然而，这样的数据往往缺乏多样性，质量欠佳，正如LIMA [2]中所示，这会严重影响模型效果。因此，[1]中的作者集中收集了一小部分高质量数据用于 SFT。这些数据来源多样，包括手动创建或标注的示例，以及从公开来源获取并经过质量过滤的数据。最终，LLaMA-2 通过使用 27540 个高质量对话示例进行第二阶段的微调。具体示例可参考上图。“令人惊讶的是，我们发现经过 SFT 模型采样的输出往往可与人工标注的 SFT 数据相媲美，这表明我们可以重新调整优先级，将更多的标注工作投入到基于偏好的 RLHF 的标注中。” ——引自[1]有趣的是，[1]中的作者观察到，对于 SFT 的收集数据而言，超过 27K 个高质量示例所带来的效益是递减的。这些发现与 LIMA [2]的实证分析结果相一致。我们并不需要大量的数据进行 SFT，但这些数据必须是高质量的。同时，[1]中的作者还注意到，经过 SFT 的 LLaMA-2 模型似乎能够自动生成 SFT 数据。RLHF：LLaMA-2 进一步使用超 1百万 个人类反馈示例的 RLHF 进行微调。为收集这些反馈，采用了二进制协议，要求人工标注者编写一个提示，并从 LLM 生成的两个响应中择优选择其中之一，进而根据有用性和安全性标准，收集人类偏好数据。例如，注重安全性的人类偏好标注可能会鼓励标注者设计一个可能会引发不安全响应的对抗性提示。然后，人类标注者可以标注哪个响应更可取、更安全（如果有的话）。“在其他条件相同的情况下，奖励模型的改进可以直接转化为对 LLaMA 2-Chat 的改进。” ——引自[1]人类反馈数据按批次收集，而 LLaMA-2 在每个批次之间通过 RLHF 进行微调。因此，在每次 RLHF 试验后，每个 LLaMA-2-Chat 模型将迭代创建多个版本，总共有五个版本。在[1]中，我们看到每次收集新的人类偏好数据时，都会训练一个新的奖励模型用于 RLHF，以确保奖励模型准确捕捉到最新模型的人类偏好。此外，出乎意料的是，生成的奖励模型的质量能总体上预测 LLaMA-2-Chat 的模型质量。总之，在整个迭代 RLHF 的过程中，LLaMA-2 会利用超一百万次的人类反馈示例来进行微调。（引自 [1])如上图所示，LLaMA-2-Chat 的质量（考虑有用性和安全性）在与 SFT 和 RLHF 对齐的多次迭代中平稳提升。这个可视化清晰地展示了每种技术对结果模型质量的影响程度。也就是说，单独使用 SFT 的效果有限。但即便仅应用 SFT，每个 RLHF 阶段的执行都显著提高了模型的对齐水平。Open LLM 排行榜上的前五名模型均基于 LLaMA-2（来自 OpenLLM 排行榜）质量：如上述 Open LLM 排行榜所示，LLaMA-2-Chat 模型目前是开源 LLM 领域的 SOTA。相比[1]中其他流行 LLM ，我们可以看到 LLaMA-2-Chat 模型在有用性和安全性方面远远优于其他开源模型。详见下图。（引自 [1]）此外，LLaMA-2 在有用性和安全性方面的表现甚至可与 ChatGPT 这样的顶级专有模型相媲美。简而言之，以上结果充分表明 LLaMA-2-Chat 模型的对齐质量很高，生成的模型能够准确捕捉并遵守人类期望的有用性和安全性标准。“[对齐]可能需要大量的计算和人工标注成本，且通常不够透明，难以复现，这阻碍了社区推动 AI 对齐研究的进展。\" ——引自[1]。LLaMA-2 的重要性：对开源 LLM 研究而言，LLaMA-2 不仅树立了新的质量标杆，还采用了与之前工作根本不同的方法。通过参考[2]，我们了解到专有 LLM 通常依赖大量专门标注的数据进行对齐，而在开源研究中，这个过程更加难以复现。尽管之前的开源模型主要利用 SFT 和公开的对话数据源，但 LLaMA-2 是最早在开源 LLM 中广泛投入对齐过程的模型之一，它精心策划了大量高质量的对话和人类偏好，用于 SFT 和 RLHF。这使得 LLaMA-2 不仅在质量上有所突破，也在方法上为开源 LLM 研究做出了重要贡献。9 结语本系列文章全面探讨了从 OPT 到 LLaMA-2 的整个开源语言模型的发展历程。尽管在这两个模型之间进行了大量研究，但它们的提出仅相隔一年！开源人工智能研究社区的发展十分迅速，跟进该领域的研究非常令人兴奋，这一过程有趣且有益。LLaMA-2-Chat 等强大的模型令人敬畏。作为从业者和研究人员，我们能够使用这些模型，从中学习，并深入了解它们的工作原理，这样的机会独一无二，理应珍惜。特别对于 LLM 来说，开源研究真的非常酷！参考文献（请上下滑动）[1] Touvron, Hugo, et al. \"Llama 2: Open Foundation and Fine-Tuned Chat Models.\" arXiv preprint arXiv:2307.09288 (2023).  [2] Zhou, Chunting, et al. \"Lima: Less is more for alignment.\" arXiv preprint arXiv:2305.11206 (2023). [3] Touvron, Hugo, et al. \"Llama: Open and efficient foundation language models.\" arXiv preprint arXiv:2302.13971 (2023). [4] Ainslie, Joshua, et al. \"GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.\" arXiv preprint arXiv:2305.13245 (2023). [5] “Introducing Llama2: The next generation of our open source large language model”, Meta, https://ai.meta.com/llama/. [6] Gudibande, Arnav, et al. \"The false promise of imitating proprietary llms.\" arXiv preprint arXiv:2305.15717 (2023). [7] Taori,  Rohan et al. “Stanford Alpaca: An Instruction-following LLaMA model.” (2023). [8] Chiang, Wei-Lin et al. “Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality.” (2023). [9] Geng, Xinyang et al. “Koala: A Dialogue Model for Academic Research.” (2023). [10] Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar. GPT4All: Training an assistant-style chatbot with large scale data distillation from GPT-3.5-Turbo, 2023. [11] Wang, Yizhong, et al. \"Self-instruct: Aligning language model with self generated instructions.\" arXiv preprint arXiv:2212.10560 (2022). [12] Mukherjee, Subhabrata, et al. \"Orca: Progressive Learning from Complex Explanation Traces of GPT-4.\" arXiv preprint arXiv:2306.02707 (2023). [13] “Introducing Falcon LLM”, Technology Innovation Institute, https://falconllm.tii.ae/. [14] “Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable Llms.” MosaicML, http://www.mosaicml.com/blog/mpt-7b. [15] “MPT-30B: Raising the Bar for Open-Source Foundation Models.” MosaicML, http://www.mosaicml.com/blog/mpt-30b. [16] Gou, Jianping, et al. \"Knowledge distillation: A survey.\" International Journal of Computer Vision 129 (2021): 1789-1819. [17] Ouyang, Long, et al. \"Training language models to follow instructions with human feedback.\" Advances in Neural Information Processing Systems 35 (2022): 27730-27744. [18] Glaese, Amelia, et al. \"Improving alignment of dialogue agents via targeted human judgements.\" arXiv preprint arXiv:2209.14375 (2022).注释1.暂时先写到这里！我确信在继续开源 LLM 研究之后，会写另一篇文章进行分享。2.这个“配方”——通常被称为三步技术——是由InstructGPT（ChatGPT 的姐妹模型）提出的，自提出以来已被许多强大的 LLM 所采用！3.我不确定模仿学习是否算作对齐。它与 SFT 非常相似，我们从现有的强大 LLM（例如 GPT-4）中选择对话示例进行 SFT。人们可以将模仿学习看作是一种通用微调，甚至是一种指令微调的变体。4.这个指标是通过使用 GPT-4 作为评判者进行自动评估获得的。5.Orca 使用 FLAN 收集的提示生成模仿数据集，由于 OpenAI API 的速率/词元限制，这个过程需要几周时间。6.有趣的是，[1]中的作者采用了两种不同的 RLHF 方法，包括典型的 PPO， RLHF 变体和一种拒绝采样微调变体：i) 从模型中采样 K 个输出；ii) 选择最佳输出；iii) 在该示例上进行微调。值得注意的是，这两种方法都基于强化学习。7.正如模仿学习一样，这些公开数据甚至可以来自其他的强大 LLM。例如，我们可以参考 ShareGPT 提供的对话数据。其他人都在看GPU架构与计算入门指南为什么开源大模型终将胜出OpenAI规模经济与第二护城河微调语言大模型选LoRA还是全参数全面对比GPT-3.5与LLaMA 2微调语言大模型推理性能工程：最佳实践开源LLM演进史：高质量基础模型竞赛试用OneFlow: GitHub - Oneflow-Inc/oneflow: OneFlow is a deep learning framework designed to be user-friendly, scalable and efficient.",
        "voteup_count": 4,
        "updated_time": "2023-11-12 20:24:03",
        "question_id": 623672939,
        "user_id": "c3e085905d3d937577852b893d204cc7"
    },
    {
        "answer_id": 3486021492,
        "content": "ChatGLM 绝对是固步自封，最后被市场策略击败。",
        "voteup_count": 4,
        "updated_time": "2024-05-03 08:40:54",
        "question_id": 623672939,
        "user_id": "a18b203fce545918c5a60a0a99aa2991"
    },
    {
        "answer_id": 3484407526,
        "content": "最近，Meta公司再次引起了技术界的广泛关注，他们推出了新一代开源大模型——Llama 3，这个模型不仅提高了技术的可访问性，也为AI的未来开辟了新的道路。然而，虽然这一大模型听起来很美，但对于中文用户来说，Llama 3似乎还有一段距离要走。今天给大家分享一个Llama 3 8B中文优化版整合包。Llama 3简介Llama 3在发布时包括了两个版本，8B和70B，这两个模型的规模巨大，性能强大，分别对标业界的顶尖AI模型。但是，如果你用中文向它提问，可能会收到英文或中英混合的答复，这显然不利于国内用户的体验。因此，针对这一点，需要对模型进行适当的微调才能更好地服务于中文语境。不过，幸好我们有一群充满才华和热情的开发者正在努力解决这个问题。在Github和HuggingFace平台上，已经出现了一些专门针对中文优化的Llama 3项目。其中，“llama3-Chinese-chat”项目尤为引人注目。“llama3-Chinese-chat”是一个由开发者Ke Bai创建的GitHub项，这个项目基于Meta的Llama-3-8B-Instruct模型进行了微调，专门优化了中文处理的效果。这个版本的Llama 3显著减少了中文问题英文回复的现象，同时也优化了中英文混合回复的问题，使得答案更加贴近中文用户的使用习惯。离线懒人包来了而为了让大家更容易上手，我出手为该项目推出了一款离线整合包。这个懒人包极大简化了部署过程，用户只需下载并解压，然后双击“一键启动”，即可在本地部署并运行这一强大的AI模型。①双击“一键启动.exe”。②双击一键启动程序后，会打开一个命令提示窗口，项目会自动运行。加载成功后，项目会自动打开浏览器加载主界面。记得点点关注不迷路哦，后续还有更多酷炫的AI项目分享~③打开页面后，可以看到项目主界面。加载完模型后，就可以在本地快速使用啦~使用很简单:输入提示词：跟使用ChatGPT一样，输入提示词即可。点击提交：可以跟Llama 3 8B大模型在本地无限免费聊天了。这种本地化的AI应用不仅方便用户探索和利用最新的AI技术，还极大地推动了开源模型在中国的应用和发展。随着越来越多的开发者加入优化和本地化的行列，未来Llama 3在中文处理方面的表现只会越来越好。这不仅是Meta的胜利，更是全球AI社区共同努力的成果。喜欢尝鲜的你，不妨试试这款离线整合包，体验一下在家与AI聊天的乐趣吧！注意事项：①该项目建议使用英伟达显卡运行，建议8G显存以上②请确保安装路径不包含中文，不然可能会引起奇怪的适配问题懒人包球友专享下载链接：（本文首发于最强开源大模型Llama-3中文特别版来了！）",
        "voteup_count": 2,
        "updated_time": "2024-05-01 08:58:32",
        "question_id": 623672939,
        "user_id": "9b68b2c43555437ca8bd565753c6035b"
    },
    {
        "answer_id": 3229464690,
        "content": "国内开源的大模型：ChatGLM-6B、ChatGLM2-6BBaichuan-13B、Baichuan2-13BQianwen-7B、Qianwen-14BXVERSE-7B国外大模型Phi-1.5Llama2系列模型Falcon180BCodeLlama-34B",
        "voteup_count": 13,
        "updated_time": "2023-09-28 06:38:21",
        "question_id": 623672939,
        "user_id": "11a1ee87d7a260edff9ef6c1fc84f2d4"
    },
    {
        "answer_id": 3483759456,
        "content": "本周，开源领域迎来多项技术成果更新：开源大语言模型迎来 Meta Llama3 和微软的 WizardLM 2，CodeQwen1.5-7B 加入开源代码领域，Mistral-22b-v0.2 在开源中探索 MOE 与稠密模型的转换，Mini-Gemini 和 Hugging Face 开源的视觉语言模型 Idefics2 则是在开源多模态模型中不断演进。除了技术演进外，商业领域裁员与融资并存。之前占据融资热点的 AI 明星企业 Stability AI 和 Tome 相继宣布裁员计划，与之相对的则是大模型领域动辄数亿美元的融资。这也为诸多公司敲下了警钟，在应用淘汰赛中，如何在可控成本下，找寻能赚取稳定现金流的场景，是 AI 产品能否持续运营的关键。并且随着市场竞争的加剧，这一淘汰赛正在迅速展开。具体内容大模型持续更新4 月 12 日，知识管理厂商印象笔记宣布其自研大语言模型被正式命名为「印象大模型」，并已根据《生成式人工智能服务管理暂行办法》及相关法律法规完成模型备案，其 AI 产品印象 AI 也迎来多项功能更新，未来将为更多用户提供包含阅读、总结在内的多项智能化知识管理服务。4 月 14 日，OpenAI 在官宣日本办事处的同时，宣布推出针对日语优化 GPT-4 定制模型。Open AI 表示，以 Speak 为代表的本地企业已经可以使用自定义模型，该模型在翻译和总结日语文本方面提供了更高的性能。最重要的是，其运行速度比 GPT-4 Turbo 快三倍，这样的成本效益将成为满足当地各种需求的合适选择。4 月 17 日，MiniMax 稀宇科技 正式发布其 MoE 模型 abab 6.5 系列，该系列包含 abab 6.5 和 abab 6.5s，其中 abab 6.5 包含万亿参数，并支持 200k tokens 的上下文长度，abab 6.5s 同样支持 200k tokens 的上下文长度，但更高效，可以在 1 秒内处理近 3 万字的文本。多模态领域4 月 13 日，xAI 在其官网推文中宣布推出多模态模型 Grok-1.5 Vision，这也意味着，除了文本信息，Grok 现在还可以处理各种包含图表、表格、截图和照片在内的视觉信息，并将于近期邀请现有的 Grok 用户进行测试。4 月 15 日，香港中文大学终身教授贾佳亚团队提出的开源多模态模型 Mini-Gemini 宣布其 130 亿参数的 demo 上线 Hugging Face。此前于 3 月 28 日，Mini-Gemini 即宣布其代码、模型、数据已经全部开源。4 月 16 日，Hugging Face 更新了其视觉语言模型 Idefics2。该模型能够理解和生成基于图像和文本的文字回复，并且在 OCR 识别能力方面显著增强。开源领域4 月 13 日，Mistral AI 在发布 Mistral-22b-v0.1 仅仅两天之后，宣布开源 Mistral-22b-v0.2。该模型实现了从 MOE 到稠密（Dense）模型的转换，并且其训练数据是 v0.1 的 8 倍。相较于 v0.1， v0.2 在数学才能和编程能力获得明显提升，并且在多轮对话中也能保持高度的对话流畅性。Mistral AI 同时宣布 v0.3 已经在训练过程中，并将有更多 220 亿参数的模型发布，直到其找到将 MOE 压缩的最佳成果。4 月 15 日，微软发布并开源其新一代大语言模型系列 WizardLM 2，此系列包括三个模型，分别是 WizardLM-2 8x22B（MOE）、WizardLM-2 70B 和 WizardLM-2 7B。但 4 月 16 日，微软宣布因为其不熟悉新模型的发布流程，未能对 WizardLM 2 进行毒性测试（toxicity testing），并已将代码文件从 Github 以及 Hugging Face 上删除，在完成测试后会尽快重新发布。4 月 16 日，通义千问团队开源了基于 Qwen1.5 的代码模型 CodeQwen1.5-7B 及其对话模型。CodeQwen1.5-7B 支持 92 种编程语言，并且能够处理最长 64 K 的上下文输入，并展现出了优秀的代码生成、长序列建模、代码修改等能力。4 月 17 日，AGI 公司 Zyphra Technologies 宣布推出其新一代开源基础模型 Zamba-7B。这个 70 亿参数的模型定位于 AI 设备的装载上，并声称在基准测试中优于 LLaMA 1、LLaMA 2-7B。同时其模型权重也即将开源，以供大家判断实际效果。4 月 17 日，昆仑万维宣布其基座大模型——天工 3.0 开启公测。天工 3.0 拥有 4000 亿参数，是目前全球最大的开源 MoE 大模型（但目前在 Github 和 ModelScope 未见其开源项目）。同时，天工 3.0 新增了图表对比生成、研究模式、增强模式、扩图修图等功能。4 月 18 日，Meta 正式发布 Llama3，目前已经上架官网和 Hugging Face。此次开源的 Llama3 共包括 2 个模型，Meta-Llama-3-8B 和 Meta-Llama-3-70B。在 MMLU、GPQA、HumanEval、GSM-8K、MATH 这五个评测集的表现上，不仅超过了 Mistral 7B，甚至部分评测集中，Meta-Llama-3-8B 模型的得分超过了 Meta-Llama-2-70B。而且在未来几个月内，Meta 还会推出更多的版本。应用探索新产品新功能 / 插件4 月 16 日，Poe 宣布推出其 3.0 版本，并新增多机器人聊天功能，用户可以通过 @指令，在不同任务场景下调用多个大模型进行对话，以发挥不同大模型的优势。Poe 致力成为对话 AI 应用商店，提供变现工具和企业服务。4 月 16 日，Adobe 宣布推出一款适配 Adobe Acrobat Reader 和 Adobe Acrobat 的 AI 助手——Adobe Acrobat AI Assistant，以帮助用户快速处理、检索、阅读和总结吸收 PDF 文档中的内容。目前，该功能仅支持英文，预计未来还会扩展至更多语言。4 月 16 日，Adobe 宣布了 Premiere Pro 全新版本的更新计划，本次更新中包含了为第三方 AI 视频生成模型添加插件。这意味着在 Adobe Firefly 自身的能力之外，用户即将可以直接通过 Adobe 工具体系调用 OpenAI Sora、Runway Gen-2 和 Pika。4 月 17 日，昆仑万维宣布基于天工 3.0 打造的天工 SkyMusic 登录天工 APP ，并开启全面公测。天工 SkyMusic 可以生成 80 秒 44100Hz 采样率双声道立体声歌曲，支持生成说唱、民谣、放克、古风、电子等多种音乐风格，还能学习颤音、歌剧、吟唱、男女对唱、自动和声等歌唱技巧。同时，也支持参考音乐与方言歌曲两种生成方式。4 月 18 日，钉钉 AI 助理市场（AI Agent Store）正式上线，首批将推出超过 200 个 AI 助理，覆盖企业服务、效率工具、财税法务、教育学习等类别。根据钉钉披露，截至 2024 年 3 月底，钉钉 AI 已超过 220 万家企业使用，月活跃企业超过 170 万家。终端 AI4 月 12 日，蔚来宣布端云多模态大模型 NOMI GPT 正式启动推送。NOMI GPT 内置的认知中枢、情感引擎和端侧多模态感知架构赋予了 NOMI 与用户进行开放式问答的交互能力。本次升级后，用户可在车内体验到大模型百科、无限趣聊、魔法氛围、趣玩表情、用车问答、AI 场景生成在内的多项全新交互体验。4 月 17 日，Rewind 宣布推出一款可穿戴 AI 设备 Limitless。Limitless 可以记录用户的日常对话内容，并利用 AI 进行会议准备、实时传译、记录和总结。这款产品预计在 2024 年 8 月份发货，预计售价为 99 美元。4 月 18 日，联想在 TechWorld 2024 上发布了内嵌个性化 AI 智能体「联想小天」的 AI PC 系列产品，价格从 5999 到 17999 元不等，目前已开启预购。其他4 月 12 日，已发布大模型安全基座和 AI 生成内容检测基座的瑞莱智慧在其公众号宣布，已经完成新一轮战略融资。本轮融资由光源资本担任独家财务顾问，投资方包括北京市人工智能产业投资基金等。4 月 13 日，估值 3 亿美元的 AI 初创公司 Tome 解雇了 12 名员工，在解雇之前该团队拥有 59 名员工。Tome 产品专注于 AI 生成 PPT，截至 4 月初，Tome 付费专业版每月收入约为 30 万美元。4 月 15 日，微软在其官网宣布，其将向阿联酋 AI 公司 G42 投资 15 亿美元，并持有少数股权和董事会席位。G42 将在微软云计算平台 Azure 上运行其人工智能应用和服务，来为中东地区、中亚和非洲国家的各行各业提供先进的 AI 解决方案。4 月 16 日，根据媒体消息，由王小川创立的百川智能正在进行新一轮数亿美元的融资，本轮融资也将成为今年以来国内 AI 领域最大的融资之一。4 月 18 日，根据内部电子邮件，Stability AI 新任命的联席 CEO Shan Shan Wong 和 Christian Laforte 宣布，Stability AI 裁员 20 多名员工，这涉及这个 200 人 团队的 10%。此前于 3 月 23 日，Stability AI 宣布其 CRO Emad Mostaque 离职，并退出董事会。",
        "voteup_count": 0,
        "updated_time": "2024-04-30 14:39:00",
        "question_id": 623672939,
        "user_id": "dd0cf3f9b8809033760b59336b599f6d"
    },
    {
        "answer_id": 3471719929,
        "content": "1、导读3D高斯泼溅最近已经成为新视角合成的一种高效表示方法。本工作研究了其编辑能力，特别是着重于补全任务，旨在为不完整的3D场景补充高斯，以实现视觉上更好的渲染效果。与2D图像补全任务相比，补全3D高斯模型的关键是要确定新增点的相关高斯属性，这些属性的优化很大程度上受益于它们初始的3D位置。为此，我们提出使用一个图像指导的深度补全模型来指导点的初始化，该模型基于2D图像直接恢复深度图。这样的设计使我们的模型能够以与原始深度对齐的比例填充深度值，并且利用大规模扩散模型的强大先验。得益于更精确的深度补全，我们的方法，称为InFusion，在各种复杂场景下以足够更好的视觉保真度和效率（约快20倍）超越现有的替代方案。并且具有符合用户指定纹理或插入新颖物体的补全能力。原文链接：最新开源 | 又快又好的扩散模型助力3D高斯场景补全(a) InFusion 能够无缝删除 3D 对象，以用户友好的方式进行纹理编辑和对象插入。(b) InFusion 通过扩散先验学习深度补全，显着提高深度修复质量。下面一起来阅读一下这项工作~2、论文信息标题：InFusion: Inpainting 3D Gaussians via Learning Depth Completion from Diffusion Prior作者：Zhiheng Liu等人机构单位: 中科大，港科大，蚂蚁，阿里巴巴项目主页地址: https://johanan528.github.io/Infusion/Github仓库: https://github.com/ali-vilab/infusion3、背景3D高斯作为新视角合成的一种重要方法，因能够以惊人的渲染速度制作出具有真实感的图像而受到重视。3D高斯提供了明确的表示能力和实时处理的可能性，大大提高了编辑3D场景的实用性。特别是对于虚拟现实（VR）和增强现实（AR）等互动式下游应用，研究如何编辑3D高斯变得越来越重要。我们的研究关注于3D高斯的补全任务，这对于3d场景编辑至关重要，有效填补了确实部分，并为进一步的移动物体，增加新物体，改变纹理等编辑方式奠定基础。现有方法对3D高斯补全的初步探索通常是使用对不同角度的渲染图象进行图像层次的补全，迭代的使用修复后的2D多视图图像作为新的训练数据。但是，这种方法往往会因生成过程中的不一致而产生模糊的纹理，且速度缓慢。值得注意的是，当初始点在3D场景中精确地定位时，高斯模型的训练质量会显著提高。因此一个实际的解决方案是将需要补全位置的高斯设置到正确的初始点，从而简化整个训练过程。因此，在为需补全高斯分配初始高斯点时，进行深度补全是关键的，将修复后的深度图投影回3D场景能够实现向3D空间的无缝过渡。因此，我们引入了InFusion，一种创新的3D高斯补全方法，我们利用了预训练扩散模型先验，训练了一个深度补全模型。我们的方法表明，Infusion可以准确确定初始点的位置，显著提高了3D高斯图像修复的保真度和效率。该模型在与未修复区域的对齐以及重构物体深度方面展现了显著的优越性。这种增强的对齐能力确保了补全高斯和原3D场景的无缝合成。此外，为了应对涉及大面积遮挡的挑战性场景， InFusion可以通过渐进的补全方式，体现了它解决此类复杂案例的能力。4、方法如上图InFusion技术方案的核心是一个以输入的RGB图像为条件的深度补全模型。这个模型能够根据观测到的单视图图像来预测和修复缺失的深度信息。它利用了预训练的潜在扩散模型先验，这些模型在大规模图像数据集上进行训练，从而具备了强大的生成能力和泛化性。整体流程如下：场景编辑初始化：首先，根据编辑需求和提供的掩码，在训练3d高斯场景的过程中，利用预先标记的掩码，构造残缺的高斯场景。深度补全：总体来说，选择一个参考视图，并对该视角渲染得到的单张RGB图像利用图像修复模型如（Stable Diffusion XL Inpainting ）进行修复。再利用深度补全模型基于观测图像预测出缺失区域的深度信息，生成补全后的深度图。具体来说，深度补全模型接受三个输入：从3D高斯渲染得到的深度图、相应的修复后彩色图像和一个掩码，其中掩码定义了需要补全的区域。先使用变分自编码器（VAE）将深度图和彩色图像编码到潜在空间中。其中通过将深度图重复使其适合VAE的输入要求，并应用线性归一化，使得深度值主要位于[-1,1]区间内。后将编码后的深度图加噪得到的近高斯噪声，将掩码区域设置为0的编码后的深度图，编码后的RGB指导图像，以及掩码图像，在channel维度进行连接，输入到U-Net网络进行去噪，逐步从噪声中恢复出干净的深度潜在表示。再次通过VAE解码得到补全后的深度图。3D点云构建：使用补全后的深度图和对应的彩色图像，通过3D空间中的反投影操作，将2D图像点转换为3D点云，这些点云随后与原始的3D高斯体集 合合并。Gaussian模型优化：合并后的3D点云通过进一步很少迭代次数的优化过程进行调整，以确保新补全的高斯体与原始场景在视觉上的一致性和平滑过渡。5、实验结果与过往方法对比，Infusion表现出保持 3D 连贯性的清晰纹理，而基线方法通常会产生模糊的纹理，尤其是复杂场景下。在更具有挑战性的场景下，包括具有多对象遮挡的场景，Infusion相比于其他方法也能够产生令人满意的效果同时通过与广泛使用的其他基线方法的比较，以及相应的点云可视化。比较清楚地表明，我们的方法成功地能够补出与现有几何形状对齐的正确形状。Infusion可以通过迭代的方式，对复杂的残缺gaussian进行补全。得益于Infusion补全3d高斯点的空间准确性，用户可以修改补全区域的外观和纹理。通过编辑单个图像，用户可以将物体投影到真实的三维场景中。此过程将虚拟对象无缝集成到物理环境中，为场景定制提供直观的工具。7、结论本文提出的方法InFusion，为3D高斯场景提供了高质量且高效的补全能力。此外，我们证明了结合扩散先验能够显著增强了我们的深度图像修复模型。这个改进的深度补全模型对于各种3D应用，特别是在新视角合成领域有着很大的应用前景。我们的方法为潜在扩散模型（LDM）与3D场景编辑之间建立了联系。这种协同作用对于未来的进一步发展和优化具有重大潜力。移步公众号「3D视觉工坊」第一时间获取工业3D视觉、自动驾驶、SLAM、三维重建、最新最前沿论文和科技动态。推荐阅读1、基于NeRF/Gaussian的全新SLAM算法2、移动机器人规划控制入门与实践：基于Navigation23、自动驾驶的未来：BEV与Occupancy网络全景解析与实战4、面向三维视觉的Python从入门到实战5、工业深度学习异常缺陷检测实战6、[第二期]基于面结构光三维重建高阶班实战",
        "voteup_count": 2,
        "updated_time": "2024-04-19 17:38:07",
        "question_id": 623672939,
        "user_id": "019de10470299bb171ad42ae0293aa68"
    },
    {
        "answer_id": 3348162505,
        "content": "开源大模型现在很多，但是能力参差不齐，你可在huggingface中查询，并得到模型最新的排名！但是开源模型的能力一定会超越GPT-4，这只是时间问题！最近看到了网友的一篇分享，网友认为2024年开源模型能力一定会追赶上并超越GPT-4。下面分享下网友内容：是的，开源将会在今年超越GPT-4！下面是理由：1、人才方面：来自斯坦福大学、卡内基梅隆大学等顶尖学府的数千名研究人员正在研发开源人工智能模型；一些非常聪明且灵活的公司和开源开发者也在致力于改善开源人工智能。2、技术方面：从2023年取得的巨大进步来看，我们将开发出更快、更有效的训练、微调和优化方法；换言之，我们不再需要庞大的计算资源。3、大科技公司的支持：像Meta这样的重要大科技公司，以及可能的AWS，都在支持开源，并为开源作出了重大贡献。4、数据方面：已经多次证明，高质量的数据对于培养优秀的LLMs至关重要。手工精选的合成数据集比大量私有数据更有用；事实上，在发明GPT-4时，OAI并没有数据优势。5、基础设施/计算方面：2023年，风险投资者向开发开源LLMs投入了超过10亿美元。这个数字还会继续增长，并足够购买GPU。6、进展方面：这是开源最有说服力的理由！目前的结果显示，开源已经排在第三位，仅次于OAI和表现最佳的Claude模型；如果我们在2023年取得同样的进展，我们将得到一个GPT-4级别的模型。开源人工智能的趋势正在受到支持；到2024年底，我们将接近GPT-4。我愿意为此下注！",
        "voteup_count": 0,
        "updated_time": "2024-01-03 13:50:46",
        "question_id": 623672939,
        "user_id": "21d380338cafa98ae77093f0d88d73eb"
    },
    {
        "answer_id": 3240358390,
        "content": "一般大模型主要指的是大语言模型，如chatgpt等，除了语言模型，还有视觉任务，也出现了大模型，比如sam。1.大语言模型大语言模型开源的很多，列出2个代表模型：1.1 llama2llama2是meta最新开源的语言大模型，训练数据集2万亿token，上下文长度是由llama的2048扩展到4096，可以理解和生成更长的文本，包括7B、13B和70B三个模型，在各种基准集的测试上表现突出，最重要的是，该模型可用于研究和商业用途。关于llama2，具体可以参考以下文章：yeyan：【llm大语言模型】一文看懂llama2(原理,模型,训练)yeyan：【llm大语言模型】code llama详解与应用1.2 通义千问通义千问是阿里开源的大语言模型，参数规模为70亿（7B）和140亿（14B）。开源包括基础模型Qwen，即Qwen-7B和Qwen-14B，以及对话模型Qwen-Chat，即Qwen-7B-Chat和Qwen-14B-Chat。具体参考官方文档【千问中文文档】。同时，千问还开源了多模态大模型，具体看【继70亿参数大模型后，阿里云又开源通义千问「多模态大模型」，如何看待这一举措？将对行业产生哪些影响？】2.视觉大模型视觉大模型主要提供视觉的基础模型，目前比较有代表的就是sam。2.1 sam在网络数据集上预训练的大语言模型具有强大的zero-shot(零样本)和few-shot(少样本)的泛化能力，这些\"基础模型\"可以推广到超出训练过程中的任务和数据分布。sam就是利用sa1b数据集，实现了无监督的分割图像的基础模型，分割效果非常好。具体参考文章：yeyan：【论文解读】MetaAi SAM(Segment Anything) 分割一切yeyan：Segment Anything(sam)项目整理汇总[2023.9.2]2.2 dinov2dinov2是一种自监督方法，基于其LVD-142M数据集，通过在大型数据集上预训练图像编码器，获得具有语义的通用视觉特征，这些特征可用于广泛的视觉任务，不用微调，就可以获得与有监督模型相当的性能，dinov2结合一个简单的线型分类器，就能在分割任务达到很好的效果,算是对sam的一个补充。支持深度估计、语义分割、图片检索等任务。深度估计语义分割实例检索具体参考【如何评价Meta最新发布的DINOv2：无需监督学习稳健的视觉特征？】2.3 InternImage-书生通用视觉模型书生通用视觉大模型，参数高达30亿，图像分类标杆数据集ImageNet 90.1% Top1准确率，开源模型中准确度最高，物体检测标杆数据集COCO 65.5 mAP。各视觉任务的指标如下图，基本都是sota或者接近sota。参考链接：书生大模型中文文档",
        "voteup_count": 5,
        "updated_time": "2023-11-06 19:54:56",
        "question_id": 623672939,
        "user_id": "3ecbd82685cc5122ac602c81f72fd742"
    },
    {
        "answer_id": 3240314843,
        "content": "国外大模型：llama2 7,13,70b调试好的：xwin70bsynthia-13b还有合成模型：https://huggingface.co/Undi95/MLewd-ReMM-L2-Chat-20B-GGUFTheBloke/Speechless-Llama2-Hermes-Orca-Platypus-WizardLM-13B-GGUFfalcon180b:还没调试好的，如果调试好那么接近gpt4也是可能的mistral-7b调试好的:mistral-openorca开源大模型每天都有进展，过一两周就会有惊喜。",
        "voteup_count": 9,
        "updated_time": "2023-10-07 19:16:58",
        "question_id": 623672939,
        "user_id": "4d70177af48d36b23d5c66459f1dbfa8"
    },
    {
        "answer_id": 3483832985,
        "content": "国内外开源的大模型那可太多了！好比国外马斯克家的GROK、谷歌家的Gemma、Meta家的LLama3就都比较有名；国内提到开源大模型的话，那智谱的ChatGLM3、百川智能的Baichuan2、元象的XVERSE也都榜上有名~像最近刚刚新鲜出炉且比较有看头的还有微软家开源的生成式AI模型Phi-3系列。小而精的这一开源AI生成式模型不仅具备“苗条的身材”，同时也有着良好的性能~作为当下开源模型中最小规模的AI模型系列，其中最小参数量的Phi-3 mini在各项专业基准测试、跑分性能中，更是成功超越了Mixtral8x7B和GPT-3.5等业界知名模型。可见这两年来，开源大模型的发展和突破可丝毫不输给闭源赛道~不过不管是开源也好，闭源也好，终究还是得落到实处才是有用的~伴随着开源大模型的稳步前行，国内外也在此基础上衍生推出了许多实用AI落地工具，像下面提到的这4个就是很好的实例：「1」AI写作宝✨多功能型AI综合百宝袋入门AI工具不知道选什么，那这一堪称“六边形战士”的免费国产AI百宝袋就值得闭眼冲。写文创作、绘图生成、对话问答在这软件中就能一并得到实现~每日刷新登录即可GET到免费的使用机会，既不用付费，也不要求网络环境，简直不要太香！既然它都叫AI写作宝了，那“AI写作”必然是它的拿手好戏~支持辅助创作的文本类型有很多，活动策划、毕业论文、述职报告、种草笔记、视频脚本等等，多个领域的使用需求都能一键安排到位。「2」功夫量化✨专业级别的数据分析金融人士值得码住收藏的一个AI数据分析工具，不需要懂得编程也照样可以通过简单的交互问答实现数据的快速分析与决策。不仅是专业金融分析师可以利用它来进行更加高效的数据分析与处理，就连没什么经验的普通投资者也可以借助它快速捕获市场信息、做出更为明智的投资决策。Windows、macOS、Linux以及手机小程序多个端口均可打开使用，登录上账号还能云同步多个平台上的历史信息~「3」花生图像✨AI商品设计&抠图去背景有点AI版PS那味的一个在线AI图片编辑器，诸如PS可实现的抠图、消除笔、图片设计、文本添加等操作它一样可以满足，而且操作起来还要更加的简单好上手~绝大部分的功能选项都能一步到位，几乎不怎么需要手动去处理，对于美工设计岗的小伙伴而言，它属实是个提升效率不可或缺的得力助手~简化修图流程、优化图像处理效果、提升设计效率它可是专业的！「4」小微助手✨桌面端AI效率工具现在的智能手机自带有各种智能语音助手，电脑端自然也不落后~出自微信家族的这一桌面端AI交互效率工具，就内置有Json魔方、Base64工具、密码工具、剪切板管理、深入信息检索等系列功能。实际上你可以直接把它看做是一个搭载了AI的综合工具箱，平时在电脑端的各项操作都可以利用它来实现快捷输出，确实就还挺便捷好用的。那今天的分享就先到这~每天一个摸鱼小技巧，跟 @一级摸鱼选手小谢 争做快乐打工人！",
        "voteup_count": 2,
        "updated_time": "2024-04-30 15:45:09",
        "question_id": 623672939,
        "user_id": "74bad1a96e0d83d68834aaa51e5431c6"
    },
    {
        "answer_id": 3318607531,
        "content": "下面这张图，一图可回顾过去开源和闭源的大语言模型。ChatGPT在2022年底发布后，在AI领域产生了巨大的变革，不论是在学术研究还是商业方面都有所体现。通过使用监督微调和从人类反馈中进行强化学习来调整大型语言模型（LLM），ChatGPT展示了模型能够在广泛的任务中回答人类问题并回答很到位。在这一成功之后，LLM的兴趣持续增长，包括在学术界和工业界频繁涌现许多新的LLM，其中许多初创公司专注于LLM。尽管闭源LLM（例如OpenAI的GPT，Anthropic的Claude）通常在性能上优于其开源对手，但后者的进展迅猛，声称在某些任务上实现了与ChatGPT相当甚至更好的表现。这不仅对研究产生了重要影响，也对业务产生了关键影响。在ChatGPT发布一周年之际，本文对这一成功进行了详尽的概述，调查了所有开源LLM声称与ChatGPT相当或更好的任务。如下图3所示。AI生成未来：ChatGPT成立一周年：开源大语言模型正在迎头赶上吗？1 引言恰好一年前，OpenAI发布了ChatGPT，这一事件在AI社区和更广泛的世界引起了轰动。首次，一款基于应用的AI聊天机器人能够通常提供有帮助、安全且详细的答案，遵循指令，甚至能够承认并纠正其先前的错误。值得注意的是，它可以执行那些传统上由预训练然后经过定制微调的语言模型执行的自然语言任务，比如摘要或问答，似乎表现得非常出色。作为首个具有这种功能的聊天机器人，ChatGPT吸引了广大公众——在推出仅两个月内就达到了1亿用户，远远快于TikTok或YouTube等其他热门应用。它还吸引了巨额商业投资，因为它有潜力降低劳动成本，自动化工作流程，甚至为客户带来新的体验。然而，由于ChatGPT没有开源，其访问受到私人公司的控制，大部分技术细节仍然未知。尽管声称ChatGPT遵循InstructGPT（也称为GPT-3.5）介绍的过程，其精确的架构、预训练数据和微调数据都是未知的。这种闭源性质带来了一些关键问题。首先，在不知道内部细节（如预训练和微调过程）的情况下，很难正确评估其对社会的潜在风险，尤其是考虑到LLMs可以惯例地生成有毒、不道德和不真实的内容。其次，据报道ChatGPT的性能随时间变化，阻碍了可重复的结果。再者，ChatGPT经历了多次停机，在2023年11月仅发生了两次重大停机，期间ChatGPT网站和其API的访问完全被阻止。最后，采用ChatGPT的企业可能会担心调用API的巨大成本、服务中断、数据所有权和隐私问题，以及像最近关于CEO Sam Altman被解雇与员工反叛董事会等戏剧性事情，以及他最终的回归这样的不可预测的事件。开源LLMs是一个有望解决或规避大多数上述问题的方向。因此，研究界一直在努力维护高性能的开源LLMs。截至目前，人们普遍认为，开源LLMs，如Llama-2或Falcon落后于其闭源对手，如OpenAI的GPT3.5（ChatGPT）和GPT-4（OpenAI，2023b），Anthropic的Claude2或Google的Bard3，其中GPT-4通常被认为在截至2023年底的时候是最强的。然而，差距越来越小，开源LLMs正在迅速赶超。实际上，正如下图1所示，对于某些任务，最好的开源LLMs已经超过了GPT-3.5-turbo。然而，这对于开源LLMs来说并非一帆风顺的挑战。LLMs的领域不断发展：闭源LLMs定期在更新的数据上进行重新训练，开源LLMs几乎每周发布一次，有大量的评估数据集和基准用于比较LLMs，使得找出最佳LLM尤其具有挑战性。在这份调查中，目标是整合最近关于开源LLMs的论文，并提供在各个领域与ChatGPT匹敌或超越它的开源LLMs的概览。贡献有三个方面：• 整合各种对开源LLMs的评估，提供对比较公正和全面的开源LLMs与ChatGPT的视角。• 系统地回顾在各种任务中超越或赶超ChatGPT的开源LLMs（如下图2所示），并进行分析。• 提供关于开源LLMs发展趋势、训练开源LLMs的良好实践以及开源LLMs可能面临的问题的见解。这份调查旨在成为研究界和商业领域的关键资源，帮助他们了解开源LLMs当前的情况和未来的潜力。对于研究人员来说，它提供了开源LLMs当前进展和不断发展趋势的详细综合，突显了未来研究的有前途的方向。对于商业领域，这份调查提供了有价值的见解和指导，协助决策者评估采用开源LLMs的适用性和益处。 在接下来的文章中，将首先介绍背景先决条件，然后深入审查在各个领域击败ChatGPT的开源LLMs，接着讨论开源LLMs的见解和问题，最后总结。2 背景2.1 训练模式「预训练」 所有LLMs都依赖于大规模的自监督预训练，使用互联网文本数据（Radford et al., 2018; Brown et al., 2020）。仅有解码器的LLMs遵循因果语言建模目标，通过该目标，模型学会在给定先前tokens序列的情况下预测下一个tokens。根据开源LLMs分享的预训练细节，文本数据的来源包括CommonCrawl5、C4、GitHub、Wikipedia、图书以及在线讨论交流，如Reddit或StackOverFlow。众所周知，扩大预训练语料库的规模可以提高模型的性能，并且与扩大模型规模相辅相成，这一现象被称为规模定律，并在（Hoffmann et al., 2022a）中进行了深入分析。现代LLMs预训练的语料库从数千亿到数万亿tokens不等。「微调」 微调旨在通过使用可用监督来适应预训练的LLM到下游任务，通常形成的数据集比用于预训练的数据集小几个数量级。T5是最早将微调框架纳入文本到文本统一框架中的模型之一，其中自然语言说明描述了每个任务。后来，通过使用自然语言说明描述的几个任务联合训练，将微调扩展为指令调整。由于指令调整能够极大地改善LLMs的零样本性能，包括在新任务上（在训练期间未见过的任务），尤其是在较大的模型规模下，因此指令调整迅速走红。标准的指令调整与多任务监督微调可能仍然无法使模型遵循人类意图，并且可以通过人类反馈强化学习（RLHF）进行改进：人类标注者对经过微调模型的输出进行排名，然后使用这些排名进行强化学习微调。最近的研究表明，人类反馈可以用LLM的反馈来替代，这个过程被称为从AI反馈中学习的强化学习（RLAIF）。一系列工作侧重于在构建多样任务的指令调整数据集时质量而不是数量：Lima在仅使用1,000个样本对Llama-65B进行微调时的性能优于GPT-3，而Alpagasus通过从52k个样本中清理其指令微调数据集，使其减少到9k个样本，从而改进了Alpaca。「持续预训练」 持续预训练是指在来自预训练LLM的模型的基础上进行另一轮预训练，通常使用比第一阶段少量的数据。这样的过程可能有助于在LLM中引出新的特性。例如，Lemur使用持续预训练来提高编码和推理能力，Llama-2-long用于扩展上下文窗口。「推理」 存在几种用LLM进行序列生成的替代方法，这些方法通过自回归解码在输出中的随机性和多样性程度而有所不同。在采样过程中增加温度会使输出更加多样化，而将其设置为0则会回到贪婪解码，这在需要确定性输出的情景中可能是必要的。采样方法top-k和top-p在每个解码步骤约束要采样的tokens池。2.2 任务领域和评估由于要执行的评估多样且广泛，因此正确评估LLMs的能力仍然是一个活跃的研究领域。问答数据集是非常受欢迎的评估基准，但最近还出现了专为LLM评估定制的新基准。在接下来的部分中，将探讨LLMs在6个主要维度上的能力： 通用能力，Agent能力，逻辑推理（包括数学和编码能力），长上下文建模，特定应用，如问答或摘要，以及可信度。3 开源LLMs vs ChatGPT3.1 通用能力基准由于每周都有大量LLMs发布，每个都声称具有卓越性能，因此要识别真正的进展和领先模型变得具有挑战性。因此，全面评估这些模型在广泛的任务领域中的性能以了解其通用能力至关重要。本节涵盖了使用基于LLM的（例如GPT-4）和传统的（例如ROUGE和BLEU）评估指标的基准。• 「MT-Bench」 旨在从八个角度测试多轮对话和遵循指令的能力，包括写作、角色扮演、提取、推理、数学、编码、知识I（STEM）和知识II（人文/社会科学）。更强大的LLMs被用作该基准的评估模型的评委。• 「AlpacaEval」 是基于AlpacaFarm评估集的LLM自动评估器，用于测试模型遵循一般用户指令的能力。它通过使用更强大的LLMs（例如GPT-4和Claude）对候选模型与Davinci-003响应进行基准测试，生成候选模型的获胜率。• 「Open LLM Leaderboad」 使用Language Model Evaluation Harness在七个关键基准上评估LLMs，包括AI2 Reasoning Challenge、HellaSwag、MMLU、TruthfulQA、Winogrande、GSM8K和DROP。该框架在零样本和少样本设置下评估LLMs在各种推理和一般知识领域的能力。• 「BIG-bench」 是一个协作基准，旨在探讨LLMs并推断它们未来的能力。它包括200多个新颖的语言任务，涵盖了各种主题和语言，这些任务对现有模型来说并非完全可解。• 「ChatEval」 是一个多Agent辩论框架，使多Agent裁判团队能够自主讨论和评估不同模型对开放式问题和传统自然语言生成任务的生成响应的质量。• 「FairEval-Vicuna」 在Vicuna Benchmark的80个问题上使用多证据校准和平衡的位置校准。FairEval-Vicuna提供了更加公正的评估结果，在采用LLMs作为评估器的范 paradigm 内与人类判断密切相关。LLMs的性能Llama-2-70B是一款杰出的开源LLM，已在包含两万亿tokens的大规模数据集上进行了预训练。它在各种通用基准测试中展现出卓越的结果。当使用指导数据进行进一步的微调时，Llama-2-chat-70B变体在一般对话任务中展示出增强的能力。特别是，Llama-2-chat-70B在AlpacaEval中实现了92.66%的胜率，超过了GPT-3.5-turbo 10.95%。然而，GPT-4仍然是所有LLMs中表现最佳的，胜率为95.28%。Zephyr-7B是另一款较小的模型，使用蒸馏的直接偏好优化，在AlpacaEval上取得了与70B LLMs相媲美的结果，胜率为90.6%。它甚至在MT-Bench上超过了Llama-2-chat-70B，得分为7.34，而Llama-2-chat-70B得分为6.86。此外，WizardLM-70B已经使用大量的指导数据进行了指导微调，涵盖了不同复杂性的任务。它在MT-Bench上以7.71的分数脱颖而出。然而，这仍然略低于GPT-3.5-turbo（7.94）和GPT-4（8.99）的分数。尽管Zephyr-7B在MT-Bench中表现出色，但在开源LLM Leaderboard上表现不佳，仅得到52.15%的分数。另一方面，GodziLLa2-70B是一款实验性模型，将来自Maya Philippines的各种专有LoRAs和Guanaco Llama 2 1K数据集与Llama-2-70B结合使用，实现了在开源LLM Leaderboard上更具竞争力的67.01%的得分。这一表现可与GPT-3.5-turbo相媲美，后者在该领域的得分为70.21%。然而，两者仍然明显落后于GPT-4，后者以85.36%的高得分领先。UltraLlama利用具有增强多样性和质量的微调数据。在其提出的基准测试中，它与GPT-3.5-turbo的性能相匹敌，并在世界和专业知识领域超过了它。3.2 Agent能力基准测试随着模型规模的不断扩大，基于LLM的Agent引起了自然语言处理社区的极大关注。鉴于此，在各种基准测试中调查了开源LLMs的Agent能力。根据所需的技能，现有的基准测试主要可以分为四类。• 使用工具：一些基准测试旨在评估LLMs的工具使用能力。「API-Bank」 专门为工具增强的LLMs设计。「ToolBench」是一个包含各种实际任务的软件工具的工具操作基准测试。「APIBench」 包含来自HuggingFace、TorchHub和TensorHub的API。「ToolAlpaca」 通过多Agent模拟环境开发了一个多样且全面的工具使用数据集。巧合的是，使用ChatGPT构建的用于工具使用的指导微调数据集也被命名为「ToolBench」。此外，「MINT」 可以评估LLMs在使用工具解决需要多轮交互的任务时的熟练程度。• 自我调试：有几个数据集可用于评估LLMs进行自我调试的能力，包括「InterCode-Bash」和「InterCode-SQL」，「MINT-MBPP」和「MINT-HumanEval」以及「RoboCodeGen」。• 遵循自然语言反馈：MINT还可以用于衡量LLMs利用自然语言反馈的能力，通过使用GPT-4模拟人类用户。• 探索环境：「ALFWorld」，「InterCode-CTF」和「WebArena」旨在评估基于LLMs的Agent是否能够从环境中收集信息并做出决策。LLMs的性能通过使用包含90B tokens的代码密集语料库对Llama-2进行预训练，并在包含30万个文本和代码样本的指导微调中，Lemur-70B-chat在探索环境或在编码任务中遵循自然语言反馈时超过了GPT-3.5-turbo的性能，如下表2所示。AgentTuning使用Llama-2在其构建的AgentInstruct数据集和通用领域指导的组合上进行指导微调，形成AgentLlama。值得注意的是，AgentLlama-70B在未见过的Agent任务上实现了与GPT-3.5-turbo相媲美的性能。通过在ToolBench上对Llama-2-7B进行微调，ToolLLaMA在工具使用评估中表现出与GPT-3.5-turbo相媲美的性能。Chen等人介绍了FireAct，可以对Llama-2-13B进行微调，以在HotpotQA上超越GPT-3.5-turbo。此外，从Llama-7B进行微调的Gorilla在编写API调用方面优于GPT-4。3.3 逻辑推理能力基准测试逻辑推理是高层次能力和技能的基本能力，例如编程、定理证明以及算术推理。为此，在本节中，将介绍以下基准测试：• 「GSM8K」 包含由人类问题作者创建的8.5K个高质量小学数学问题。这些问题需要2到8个步骤来解决，解决方案主要涉及使用基本算术运算执行一系列基本计算，以达到最终答案。• 「MATH」 是一个包含12,500个具有挑战性的竞赛数学问题的数据集。MATH中的每个问题都有一个完整的分步解决方案，可用于教模型生成答案的推导和解释。• 「TheoremQA」 是一个定理驱动的问答数据集，旨在评估AI模型将定理应用于解决具有挑战性的科学问题的能力。TheoremQA由领域专家策划，包含800个高质量问题，涵盖了数学、物理、电子工程与计算机科学以及金融领域的350个定理。• 「HumanEval」 是一组164个手写编程问题。每个问题包括一个函数签名、文档字符串、主体和几个单元测试，平均每个问题有7.7个测试。• 「MBPP」 （主要是基本编程问题）数据集包含由对Python具有基本知识的内部众包工人进行众包构建的974个短Python程序。每个问题都分配有一个解决指定问题的独立的Python函数，并包含三个测试用例，用于检查函数的语义正确性。• 「APPs」 是一个用于代码生成的基准测试，衡量模型根据任意自然语言规范生成令人满意的Python代码的能力。该基准测试包括10,000个问题，从具有简单一行解决方案到具有实质性算法挑战的问题不等。强化指导调整与传统的基于知识蒸馏的指导调整不同，Luo等人采用了Evol-Instruct构建了任务特定的高质量指导调整数据集，其中种子指导发展成知识边界或任务复杂性深度扩展的指导。此外，Luo等人还结合了PPO算法，进一步提高了生成的指导和答案的质量。在获得扩展的指导池后，通过收集来自另一个LLM（例如GPT-3.5-turbo）的响应生成新的指导调整数据集。最终，由于Query深度和宽度的发展，经过精细调整的模型的性能甚至优于GPT-3.5-turbo。例如，WizardCoder在HumanEval上表现优异，相对于GPT3.5-turbo有19.1％的绝对改进。而WizardMath相对于GPT-3.5-turbo也取得了42.9％的绝对改进。在更高质量数据上的预训练Lemur已验证了在自然语言数据和代码之间更好的混合，并使LLMs在函数调用、自动编程和Agent方面具有更强的能力。具体而言，Lemur-70B在没有任务特定的精细调整的情况下，在HumanEval和GSM8K上相对于GPT-3.5-turbo取得了显著的改进。Phi采用了不同的方法，使用教科书作为主要的预训练语料库，这使得在更小的语言模型上观察到了强大的能力。3.4 建模长上下文能力基准测试处理长序列仍然是LLMs的关键技术瓶颈之一，因为所有模型都受到有限的最大上下文窗口的限制，通常长度从2k到8k个tokens不等。对LLMs的长上下文能力进行基准测试涉及对一些自然具有长上下文的任务进行评估，例如提要或多文档QA。已经为LLMs的长上下文评估提出了以下基准测试：「SCROLLS」 是一个由7个具有自然长输入的数据集组成的流行评估基准。任务涵盖提要、问答和自然语言推理。「ZeroSCROLLS」 在SCROLLS的基础上构建（舍弃了ContractNLI，重用其他6个数据集，并添加了4个数据集），仅考虑零样本设置，评估LLMs的即插即用性。「LongBench」 设定了一个包含21个数据集跨6个任务的双语英语/中文长上下文基准。「L-Eval」 重复使用了16个现有数据集，并从头开始创建了4个数据集，形成一个多样化的、长上下文的基准，每个任务的平均长度超过4k个tokens。作者主张使用LLM评价（特别是GPT-4）而不是N-gram进行长上下文评估。「BAMBOO」 创建了一个专注于长上下文LLM评估基准，重点是通过仅收集评估数据集中的最新数据来消除预训练数据的污染。「M4LE」 引入了一个广泛的基准测试，将36个数据集分为5个理解能力：显式单跨度、语义单跨度、显式多跨度、语义多跨度和全局理解。模型在LongBench、L-Eval、BAMBOO和M4LE基准测试中，GPT-3.5-turbo或其16k版本在很大程度上优于所有开源LLMs，如Llama-2、LongChat或Vicuna；表明在长输入任务上提高开源LLMs的性能并非易事。Llama-2-long在Llama-2上使用400Btokens进行更长的预训练（从Llama-2的4k窗口增加到16k）。由此产生的Llama-2-long-chat-70B在ZeroSCROLLS上相对于GPT-3.5-turbo-16k的得分为37.7比36.7。解决长上下文任务的方法包括通过位置插值扩展上下文窗口，其中包括使用更长上下文窗口进行另一轮（短）微调；和检索增强，需要访问检索器以查找相关信息。Xu等人结合了这两种看似相反的技术，将Llama-2-70B推动到GPT-3.5-turbo-16k的平均水平上，在7个长上下文任务中（包括ZeroSCROLLS的4个数据集）上表现优于GPT-3.5-turbo-16k。3.5 应用特定能力这一部分将讨论LLMs在处理特定应用程序时所需的能力。3.5.1 Query焦点摘要「基准测试」Query焦点或基于外表的摘要需要根据一个细粒度的问题或一个方面类别生成摘要。Query焦点数据集包括AQualMuse、QMSum和SQuALITY，而基于方面的数据集包括CovidET、NEWTS、WikiAsp等。「模型」（Yang等人，2023d）发现，与ChatGPT相比，对训练数据的标准微调在性能上仍然更好，对于CovidET、NEWTS、QMSum和SQuALITY的ROUGE-1平均提高了2个点。3.5.2 开放式问答「基准测试」开放式问答有两个子类别：答案要么是短格式，要么是长格式。短格式数据集包括SQuAD 1.1、NewsQA、TriviaQA、SQuAD 2.0、NarrativeQA、Natural Question（NQ）、Quoref和DROP。长格式数据集包括ELI5和doc2dial。对于短格式和长格式数据集，评估指标是答案中的精确匹配（EM）和F1。「模型」InstructRetro在NQ、TriviaQA、SQuAD 2.0和DROP上相对于GPT-3取得了显著改进，同时与类似大小的专有GPT-instruct模型相比，在一系列短格式和长格式的开放式问答数据集上提高了7-10％。InstructRetro从预训练的GPT模型初始化，然后继续通过检索进行预训练，然后经过指导调整。3.5.3 医学「基准测试」LLMs的一个理想能力是在医学相关任务上做出贡献，以使负担得起的、高质量的医疗更容易接触到更广泛的公众。对于心理健康，IMHI基准测试是使用10个现有的心理健康分析数据集构建的，包括心理健康检测：DR、CLP、Dreaddit、孤独、SWMH和T-SID；心理健康原因检测：SAD、CAMS；心理风险因素检测：MultiWD、IRF。对于放射学，OpenI数据集和MIMIC-CXR数据集都包含具有发现和印象文本的放射学报告。「模型」对于心理健康，MentalLlama-chat-13B在IMHI训练集上对Llama-chat-13B模型进行微调。 MentalLlama-chat-13B模型在零样本提示下在IMHI的10项任务中，相对于ChatGPT的few-shot提示或零样本提示，在9项任务上表现优于ChatGPT。Liu等人提出对Llama检查点进行微调，以生成放射学报告发现的印象文本。所得的Radiology-Llama-2模型在MIMIC-CXR和OpenI数据集上相对于ChatGPT和GPT-4都取得了较大的优势。3.5.4 生成结构化响应在按照指令生成格式化响应是支持Agent能力或简化解析或翻译模型响应的手动工作的核心能力。「基准测试」Rotowire包含NBA比赛摘要及相应的比分表。Struc-Bench引入了两个数据集：Struc-Bench-Latex，其输出为Latex格式的表格，以及Struc-Bench-HTML，其输出为HTML格式的表格。「模型」Struc-Bench在结构化生成数据上对Llama-7B模型进行了微调。在上述所有基准测试中，经过微调的7B模型的性能均优于ChatGPT。3.5.5 生成评论「基准测试」LLMs的一个有趣能力是为问题的响应提供反馈或评论。为了评估这种能力，可以使用人工标注员或GPT-4作为评估器来直接评估评论。原始问题可以来自上述其他能力的任何数据集。「模型」Shepherd是一个从Llama-7B初始化的7B模型，经过社区收集的评论数据和1,317个高质量人工标注数据的训练。Shepherd在各种不同的NLP数据集上生成评论：AlpacaFarm、FairEval、CosmosQA、OBQA、PIQA、TruthfulQA和CritiqueEval。通过使用GPT-4作为评估器，Shepherd在60%以上的情况下赢得或与ChatGPT相等。在人类评估员的评估中，Shepherd几乎与ChatGPT持平。3.6 迈向可信赖的人工智能为了确保LLMs在实际应用中能够得到人类的信任，一个重要的考虑因素是它们的可靠性。例如，对于错觉（Ye＆Durrett，2022；Zhao等人，2023）和安全性（Zhiheng等人，2023b）的担忧可能降低用户对LLMs的信任，并导致在高影响应用中存在风险。3.6.1 错觉「基准测试」已经有各种基准测试，以更好地评估LLMs中的错觉。具体而言，它们包括大规模数据集、自动化度量和评估模型。• TruthfulQA是一个问答（QA）基准数据集，包含涵盖38个类别的问题。这些问题被设计成一些人由于误解而错误回答。• FactualityPrompts是一个测量开放式生成中错觉的数据集。它包含事实和非事实提示，以研究提示对LLM继续的影响。• HaluEval是一个包含生成的和人工标注的虚构样本的大型数据集。它涵盖了三个任务：问答、基于知识的对话和文本摘要。• FACTOR提出了一种可伸缩的评估语言模型事实性的方法：它自动将一个事实语料库转化为一个忠实度评估基准。该框架用于创建两个基准：Wiki-FACTOR 和 News-FACTOR。• KoLA构建了一个面向知识的语言模型评估基准（KoLA），其中包含三个关键因素：模仿人类认知以进行能力建模，使用维基百科进行数据收集，并为自动虚构评估设计对比指标。• FActScore提出了一种新的评估方法，首先将语言模型的生成分解为一系列原子事实，然后计算由可靠知识源支持的原子事实的百分比。• Vectara的错觉评估模型是一个小型语言模型，经过二进制分类器的微调，用于将摘要分类为与源文档一致（或不一致）。然后，它用于评估和基准测试各种LLMs生成的摘要的错觉。• FacTool是一个用于检测由LLMs生成的文本的事实错误的任务和领域不可知框架。除了新引入的错觉基准测试之外，以实际知识为基础的先前问答（QA）数据集也被广泛用于衡量忠实度，如HotpotQA、OpenBookQA、MedMC-QA和TriviaQA。除了数据集和自动化指标外，人工评估也被广泛采用作为忠实度的可靠度量。「模型」存在一些关于错觉的现有调查（Zhang等人，2023b；Rawte等人，2023），详细调查了潜在的方法。具体而言，超越当前GPT-3.5-turbo性能的方法可以在微调期间或仅在推理时进行。选择的性能指标显示在下表3中。在微调期间，通过提高正确性和相关性的数据质量可以导致更少的错觉模型。Lee等人（2023a）策划了一个内容过滤、以STEM领域高质量数据为重点的调整数据集。一系列LLMs在这个经过过滤的数据集上进行微调并合并。结果产生的系列，名为Platypus，与GPT-3.5-turbo相比，在TruthfulQA上实现了相当大的改进（约20%）。在推理期间，现有的技术包括特定的解码策略、外部知识增强和多Agent对话。对于解码，Dhuliawala等人（2023）介绍了Chain-of-Verification（CoVe），其中LLM起草验证问题并自我验证响应。CoVe在FactScore上相对于GPT-3.5-turbo实现了相当大的改进。对于外部知识增强，各种框架包含不同的搜索和提示技术，以当前提高GPT-3.5-turbo性能。Li等人（2023c）设计了Chain-of-Knowledge（CoK），在回答之前从异构知识源中检索。Peng等人（2023）提出了LLM-AUGMENTER，该方法使用一组即插即用的模块增强LLMs，并通过由效用函数生成的反馈迭代地修订LLM提示，以改进模型响应。Knowledge Solver（KSL）试图通过利用它们自己的强大泛化能力，教会LLMs从外部知识库中搜索基本知识。CRITIC允许LLM验证和逐渐修正其自己的输出，方式类似于人类与工具的交互。Luo等人（2023b）介绍了Parametric Knowledge Guiding（PKG）框架，该框架配备了一个知识引导模块，以访问相关知识而不更改LLMs的参数。这些推理技术然后相对于使用GPT-3.5-turbo的简单提示策略改善了答案的准确性。目前，GPT-3.5-turbo还已经整合了一个检索插件（OpenAI，2023a），以访问外部知识以减少错觉。对于多Agent对话，Cohen等人（2023）促进了生成声明的Examinee LLM与引入问题的另一个Examiner LLM之间的多轮交互。通过交叉审订过程，改善了各种QA任务的性能。Du等人（2023）要求多个语言模型实例提出和辩论他们各自的响应和推理过程，经过多轮的辩论达成共同的最终答案，从而改善了多个基准。3.6.2 安全性「基准测试」在LLMs中，安全性问题主要可以分为三个方面（Zhiheng等人，2023a）：社会偏见、模型鲁棒性和中毒问题。为了收集更好地评估上述方面的数据集，提出了几个基准测试：• SafetyBench是一个数据集，包括11,435个涵盖7个不同安全问题类别的多元选择问题。• Latent Jailbreak引入了一个基准测试，评估LLMs的安全性和鲁棒性，强调了需要采用平衡的方法。• XSTEST是一个系统地识别夸张安全行为的测试套件，例如拒绝安全提示。• RED-EVAL是一个基准测试，执行红队行动，使用基于Chain of Utterances（CoU）的提示对LLMs进行安全评估。除了自动化基准测试，安全性的一个重要度量是人工评估（Dai等人，2023），其中众包工作者将响应tokens为安全或有害。一些研究还尝试从GPT-4中收集这些标签，因为研究表明它可以取代人类评估者来评估对齐能力（Chiang＆Lee，2023）。「模型」基于当前的评估，GPT-3.5-turbo和GPT-4模型在安全性评估中仍然名列前茅。这主要归因于人工强化学习（RLHF）。RLHF首先在响应上收集人类偏好数据集，然后训练一个奖励模型来模仿人类偏好，最后使用RL来训练LLM以与人类偏好保持一致。在这个过程中，LLMs学会了展示所需的行为，排除了有害的响应，如不礼貌或有偏见的回答。然而，RLHF程序需要收集大量昂贵的人工标注，这阻碍了它在开源LLMs中的使用。为了推动LLMs的安全对齐的努力，Ji等人（2023）收集了一个人类偏好数据集，以将无害性和有用性从人类偏好分数中解开，从而为这两个度量提供独立的排名数据。实验证明，解开人类偏好可以增强安全对齐。Bai等人（2022b）试图通过来自AI反馈的RL（RLAIF）来提高安全性，其中偏好模型使用LLM生成的自我评论和修订进行训练。直接偏好优化（DPO）减少了学习奖励模型和直接使用简单的交叉熵损失从偏好中学习的需要，这在很大程度上可以减少RLHF的成本。结合和改进这些方法可能会在开源LLMs的安全性方面带来潜在的改进。4 讨论4.1 LLMs的发展趋势自从Brown等人（2020）展示了冻结的GPT-3模型在各种任务上可以实现令人印象深刻的零样本和少样本性能以来，人们已经付出了大量努力推动LLMs的发展。一方面的研究集中在扩大模型参数的规模，包括Gopher，GLaM，LaMDA，MT-NLG和PaLM，最终达到了540B参数。尽管展现出卓越的能力，但这些模型的闭源性质限制了它们的广泛应用，因此越来越多的人开始对开源LLMs的发展产生兴趣。与扩大模型规模不同，另一方面的研究探索了更好的策略或目标，以预训练较小模型，如Chinchilla和UL2。在预训练之外，人们还致力于研究LMs的指导调整，例如FLAN，T0和Flan-T5。一年前，OpenAI的ChatGPT的出现极大地改变了NLP社区的研究重点。为了赶上OpenAI，Google和Anthropic分别推出了Bard和Claude。尽管它们在许多任务上显示出与ChatGPT相媲美的性能，但它们与最新的OpenAI模型GPT-4之间仍然存在性能差距。由于这些模型的成功主要归功于人类反馈的强化学习（RLHF），研究人员已经探讨了改进RLHF的各种方式。为了促进开源LLMs的研究，Meta发布了Llama系列模型。自那以后，基于Llama的开源模型开始呈爆炸式增长。一个典型的研究方向是使用指导数据微调Llama，包括Alpaca，Vicuna，Lima和WizardLM。进行中的研究还探讨了提高Llama基于开源LLMs的Agent，逻辑推理和长上下文建模能力。此外，与基于Llama开发LLMs不同，还有许多努力致力于从头开始训练强大的LLMs，例如MPT，Falcon，XGen，Phi，Baichuan，Mistral，Grok和Yi。我们相信，开发更强大、更高效的开源LLMs，以使封闭源LLMs的能力得以民主化，应该是一个非常有前途的未来方向。4.2 结果总结就通用能力而言，Llama-2-chat-70B在一些基准测试中显示出对GPT-3.5-turbo的改进，但在大多数其他测试中仍然落后。Zephir-7B通过蒸馏直接偏好优化接近70B LLMs。WizardLM70B和GodziLLa-70B能够达到与GPT-3.5-turbo可比较的性能，展示了一个有希望的发展方向。在一些领域，开源LLMs能够超越GPT-3.5-turbo。对于基于LLM的Agent，开源LLMs通过更广泛和任务特定的预训练和微调能够超越GPT-3.5-turbo。例如，Lemur-70B-chat在探索环境和在编码任务中遵循反馈方面表现更好。AgentTuning在看不见的Agent任务上有所改进。ToolLLama能更好地掌握工具使用。Gorilla在编写API调用方面优于GPT-4。对于逻辑推理，WizardCoder和WizardMath通过增强的指导调整提高了推理能力。Lemur和Phi通过在质量较高的数据上进行预训练实现了更强大的能力。对于建模长上下文，Llama-2-long通过使用更长的tokens和更大的上下文窗口进行预训练，可以在选定的基准测试中改进。Xu等人（2023b）通过将上下文窗口扩展与位置插值和检索增强相结合，提高了7个长上下文任务的性能。对于特定应用能力，InstructRetro通过检索和指导调整进行预训练，在开放式QA方面有所改进。通过特定任务的微调，MentaLlama-chat13B在心理健康分析数据集中优于GPT-3.5-turbo。RadiologyLlama2可以提高在放射学报告上的性能。Stru-Bench，一个经过微调的7B模型，可以改善结构化响应生成，与GPT-3.5-turbo相比，这是支持Agent任务的核心能力。Shepherd，只有7B参数，可以在生成模型反馈和评论方面达到与GPT-3.5-turbo相媲美或更好的性能。对于值得信赖的AI，可以通过使用质量更高的数据进行微调，上下文感知的解码技术，外部知识增强，例如Li等人（2023c）；Yu等人（2023b）；Peng等人（2023）；Feng等人（2023），或多Agent对话来减少错觉。也有一些领域，GPT-3.5-turbo和GPT-4仍然无法匹敌，如AI安全。由于GPT模型涉及大规模的RLHF，它们以表现更安全和更具道德行为而闻名，这对于商业LLMs而言可能是比开源LLMs更重要的考虑因素。然而，随着最近在民主化RLHF过程方面的努力，可以期待在安全性方面看到更多开源LLMs的性能提升。4.3 最佳开源LLMs的配方训练LLM涉及到复杂且资源密集的实践，包括数据收集和预处理、模型设计以及训练过程。尽管有越来越多的趋势是定期发布开源LLMs，但领先模型的详细实践往往遗憾地被保密。下面列出了社区广泛认可的一些最佳实践。数据预处理预训练涉及使用数万亿数据tokens，通常来自公开可访问的来源。从伦理角度来看，排除包含个人信息的任何数据是至关重要的。与预训练数据不同，微调数据在数量上较小，但在质量上较高。使用高质量数据进行微调的LLMs表现出了改进的性能，特别是在专业领域。模型架构尽管大多数LLMs使用仅解码器的Transformer架构，但模型中采用了不同的技术来优化效率。Llama-2实施了Ghost attention以改进多轮对话控制。Mistral采用滑动窗口注意力来处理扩展上下文长度。训练使用指导调整数据进行监督微调（SFT）的过程至关重要。为了获得高质量的结果，SFT标注数以数万计就足够了，正如Llama-2所使用的27,540个标注一样。这些数据的多样性和质量至关重要。在RLHF阶段，近端策略优化（PPO）通常是更好地使模型的行为与人类偏好和指导保持一致的首选算法，对于增强LLM的安全性起着关键作用。替代PPO的方法是直接偏好优化（DPO）。例如，Zephyr-7B采用蒸馏的DPO，在各种通用基准测试中显示出与70B-LLMs相媲美的结果，甚至在AlpacaEval上超过了GPT-3.5-turbo。4.4 漏洞和潜在问题预训练期间的数据污染随着发布混淆其预训练语料库来源的基础模型，数据污染问题变得越来越明显。这种缺乏透明度可能导致对大型语言模型（LLMs）真正泛化能力的偏见。在忽略了将基准数据手动集成到训练集中的情况，且有人类专家或更大模型的标注，数据污染问题的根本原因在于基准数据的收集源已经包含在预训练语料库中。尽管这些模型并非有意使用监督数据进行预训练，它们仍然可以获得精确的知识。因此，解决检测LLMs预训练语料库的挑战，探索现有基准测试和广泛使用的预训练语料库之间的重叠，并评估对基准测试的过拟合问题变得至关重要。这些努力对于增强LLMs的忠实度和可靠性至关重要。展望未来，未来的方向可能包括建立披露预训练语料库详细信息的标准做法，并开发在整个模型开发生命周期中减轻数据污染的方法。对齐的闭源发展在社区中，使用一般偏好数据进行对齐的强化学习从人类反馈（RLHF）的应用引起了越来越多的关注。然而，由于缺乏高质量、公开可用的偏好数据集和预训练奖励模型，只有有限数量的开源LLMs已经通过RLHF进行了对齐。一些倡议试图为开源社区做出贡献。然而，仍然面临在复杂推理、编程和安全场景中缺乏多样性、高质量和可扩展的偏好数据的挑战。在基本能力上的持续改进的困难回顾本文中概述的基本能力的突破揭示了一些具有挑战性的情景：已经投入了相当大的努力来探索在预训练期间改进数据混合，以提高构建更强大基础模型的平衡性和鲁棒性。然而，相关的探索成本通常使这种方法变得不切实际。超越GPT-3.5-turbo或GPT-4的模型主要是基于从闭源模型进行知识蒸馏和额外专家标注。虽然高效，但在将这些方法扩展到教师模型时，对知识蒸馏效果的潜在问题可能会被掩盖。此外，LLMs预计将充当Agent并提供合理的解释以支持决策，然而为了使LLMs适用于现实世界的场景，标注Agent样式的数据也是昂贵且耗时的。实质上，仅通过知识蒸馏或专家标注的优化无法实现对基本能力的持续改进，并可能接近上限。未来的研究方向可能涉及探索新的方法，如无监督或自监督学习范式，以在缓解相关挑战和成本的同时实现基本LLM能力的持续改进。5 结论在这份调查中，对在ChatGPT发布一周年之际超越或赶上ChatGPT在各种任务领域中表现优异的开源LLMs进行了系统回顾。此外，提供了关于开源LLMs的见解、分析和潜在问题。这份调查为开源LLMs提供了有前途的方向，并将激发该领域更多的研究和开发，有助于缩小它们与付费同行之间的差距。参考文献[1] ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?链接：https://arxiv.org/pdf/2311.1698",
        "voteup_count": 5,
        "updated_time": "2023-12-08 21:36:39",
        "question_id": 623672939,
        "user_id": "40ba3b6fc933ec29876a5a8c8ef8d25c"
    },
    {
        "answer_id": 3483596647,
        "content": "标题：探索开源大模型的世界：AI的无限可能在人工智能的浩瀚星空中，开源大模型如同最璀璨的星辰，不仅引领着技术的革新，更激发了无数创新者的思维火花。让我们启程，深入探索这些令人瞩目的开源大模型。GPT-3：自然语言处理的巨人首先，我们不得不提及GPT-3，这个由OpenAI精心打造的自然语言处理（NLP）巨擘。它的强大能力让人惊叹，从撰写文章到编程，甚至进行哲学探讨，GPT-3似乎都能游刃有余。它的诞生，标志着AI领域的一大飞跃，为未来的语言处理技术奠定了新的里程碑。BERT：理解语言深层含义的先锋紧随其后的是BERT，谷歌开发的这款自然语言处理模型以其独特的双向理解能力而闻名。BERT的全称是Bidirectional Encoder Representations from Transformers，它能够深入理解语言的细微差别，尤其在处理复杂的语言任务时表现出色。其他值得关注的开源大模型除了GPT-3和BERT，还有许多其他模型同样值得关注。例如Facebook的RoBERTa，作为BERT的改进版本，它在性能上实现了显著提升。再如OpenAI的CLIP，这是一种创新的AI模型，能够理解图像与文本之间的关联，在艺术创作和多媒体应用领域展现出巨大的潜力。开源大模型的无限潜力这些模型仅仅是开源大模型世界的冰山一角。随着技术的持续进步，新的模型不断涌现，它们不仅推动了AI技术的发展，也为各种应用场景提供了广阔的想象空间。开源大模型的多样性与创新开源大模型的多样性是其最吸引人的特点之一。从专注于自然语言处理的模型如GPT-3和BERT，到能够处理图像和文本关系的CLIP，再到其他专门针对特定任务或数据类型的模型，开源社区的创新精神不断催生出新的工具和解决方案。这些模型的开放性意味着研究人员和开发者可以自由地访问、修改和改进它们，从而加速了AI技术的发展。技术进步对应用场景的影响随着技术的不断进步，开源大模型正在为各种应用场景提供前所未有的可能性。在医疗领域，AI模型可以帮助分析医学影像，辅助诊断；在金融行业，它们可以用于风险评估和欺诈检测；在教育领域，个性化学习工具可以根据学生的学习习惯和进度提供定制化内容。此外，开源大模型也在推动自动驾驶、智能家居、游戏开发等领域的创新。模型的可访问性与创新的民主化开源大模型的另一个重要影响是它们促进了创新的民主化。通过降低进入门槛，这些模型使得小型企业和个人开发者也能够参与到AI技术的研究和开发中来。这种开放和协作的环境不仅加快了技术的发展，还为解决全球性问题提供了更多的机会。未来展望：开源大模型与AI伦理展望未来，开源大模型将继续推动AI技术的边界。然而，随着技术的发展，我们也需要关注AI伦理和责任问题，确保这些强大的工具被用于促进社会的整体福祉。这包括确保算法的透明度、防止偏见和歧视、保护个人隐私等。开源大模型作为AI技术进步的重要驱动力，不仅展示了AI的无限可能，也为我们提供了探索未知、解决问题和创造新价值的机会。随着技术的不断发展，我们期待这些模型能够继续激发创新，同时我们也必须谨慎地处理伴随而来的伦理和社会责任问题。AI与人类：协作而非取代面对这些开源大模型的兴起，有人担忧AI是否会取代人类。然而，我的看法是，这些模型的出现实际上是在增强而非取代人类的能力。它们更像是一套强大的工具，赋予我们解决更复杂问题、实现更宏伟梦想的能力。展望未来开源大模型的涌现，无疑是AI技术发展史上的一次重大突破。它们不仅展示了AI技术的无限可能，也为人类开辟了新的可能性。展望未来，我坚信这些模型将继续引领AI技术的前进步伐，为我们的生活和工作带来更多的便捷与创新。",
        "voteup_count": 13,
        "updated_time": "2024-04-30 11:54:14",
        "question_id": 623672939,
        "user_id": "eefcdc1d60c3f6c07db77c20b7e11474"
    },
    {
        "answer_id": 3239571339,
        "content": "之前写过一些回答，但是这里：https://github.com/Mooler0410/LLMsPracticalGuide#Usage-and-Restrictions 的信息一直在更新，可以关注另外可以关注hugging face :Hugging Face – The AI community building the future.国内外的开源模型非常丰富，国外的如Meta的LLama，国内最近百川智能开源的大模型：https://github.com/baichuan-inc/Baichuan2，号称碾压LLama以上供参考。",
        "voteup_count": 3,
        "updated_time": "2023-10-07 10:04:31",
        "question_id": 623672939,
        "user_id": "646e30948d93aa29e766eb4a6b11a1ba"
    },
    {
        "answer_id": 3292369917,
        "content": "目前，已知的开源大模型有很多，如：Google的BERT、T5、GPT系列模型、OpenAI的GPT-3、Facebook的BART等等。这些模型在多种任务上表现优秀，但是需要大量的计算资源和专业知识来训练和优化。而对于大部分企业或者个人用户，可能没有足够的资源和技术能力来直接使用这些开源大模型。这时候，你可能需要一些更便捷的工具来帮助你使用这些模型，集简云就是这样一款工具。集简云是一款SaaS无代码软件连接器，可以无需开发轻松连接超过1000款软件的数据与接口，构建自动化与智能化的业务流程。通过集简云，你可以更容易地获取和分析数据，无需关心底层的数据处理和模型训练问题，只需要简单的配置，就可以构建出符合你业务需求的工作流程。这样不仅可以大大节约你的时间和精力，也能让你更专注于业务本身。所以，无论你是数据分析师，产品经理，还是企业决策者，我都强烈推荐你试用集简云，它将帮助你更好地利用现有的开源大模型，实现你的业务目标。",
        "voteup_count": 0,
        "updated_time": "2023-11-17 15:10:06",
        "question_id": 623672939,
        "user_id": "27cc62a5d4544e2de9c2147be88be3a4"
    },
    {
        "answer_id": 3301711443,
        "content": "目前已知大厂发布的大模型有如下几个，仅供参考。国内大厂序号厂商huggingface镜像下载地址1阿里通义千问https://aifasthub.com/models/Qwen2百川智能https://aifasthub.com/models/baichuan-inc3上海人工智能实验室https://aifasthub.com/models/internlm4智谱https://aifasthub.com/models/THUDM5智源人工智能研究院https://aifasthub.com/models/BAAI6FlagAlphahttps://aifasthub.com/models/FlagAlpha7零一万物https://aifasthub.com/models/01-ai海外大厂序号厂商huggingface镜像下载地1openaihttps://aifasthub.com/models/openai2googlehttps://aifasthub.com/models/google3lmsyshttps://aifasthub.com/models/lmsys4NousResearchhttps://aifasthub.com/models/NousResearch5OpenAssistanthttps://aifasthub.com/models/OpenAssistant6diffusershttps://aifasthub.com/models/diffusers7HuggingFaceH4https://aifasthub.com/models/HuggingFaceH48garage-bAIndhttps://aifasthub.com/models/garage-bAInd9microsofthttps://aifasthub.com/models/microsoft10bigcodehttps://aifasthub.com/models/bigcode",
        "voteup_count": 5,
        "updated_time": "2023-11-25 10:11:39",
        "question_id": 623672939,
        "user_id": "370e58856e50f054407ae3c804d13feb"
    },
    {
        "answer_id": 3240327543,
        "content": "文丨郝 鑫，编丨刘雨琦“OpenAI不足为惧，开源会慢慢赶上来。”彼时Hugging Face创始人Clem Delangue的一句预言，正在迅速成为现实。ChatGPT横空出世7个多月后，7月19日，Llama 2宣布开源，并且可直接商用。如今回看，这一天也成为了大模型发展的分水岭。在此之前，全世界开源的大模型不计其数，可只是停留在开发研究层面。“可商业”短短三个字，犹如一颗重磅炸弹引爆了大模型创业圈，引得傅盛连连感叹，“有的人哭晕在厕所，而有的人在梦中也能笑醒”。AI大模型圈一夜之间变了天，同时也宣告着大模型加速商业化时代的到来。自Llama 2后，开源逐渐成为主流趋势。以Llama架构为首，先掀起了一波以其为核心的开源，如Llama 2低成本训练版、Llama 2最强版、微调版等等。截至发稿前，以“LLama 2”为关键词在国外最大的AI开源社区Hugging Face检索模型，有5341条结果；在全世界最大的开源项目托管平台Github上，也有1500个词条。（图源：Hugging Face官网）（图源：Github官网）之后，创业者们的目光从解构、增强Llama 2转向了构建行业专有大模型，于是又掀起了一波Llama 2+司法、Llama 2+医疗等一系列的行业开源大模型。据不完全统计，Llama 2开源后，国内就涌现出了十几个开源行业大模型。国内头部厂商和创业公司纷纷加入开源浪潮中，阿里QWEN-7B开源一个多月下载量破100万，9月25日升级了QWEN-14B；百川智能开源的Baichuan-7B、13B两款开源大模型下载量目前已经突破500万，200多家企业申请部署开源大模型。与此形成强烈对比的是，短时间内，Llama 2对一些闭源的大模型厂商造成了致命性的打击。闭源大模型多采用调取API的方式使用，数据需要先上传至模型厂商，按照调用次数收取费用；而开源则可以在本地部署，且完全免费，可商用后产生的利润也可以收归己有。行业内人士告诉光锥智能：“在这种情况下，基于成本的考虑，已经开始有许多企业选择放弃支付上千万元的费用，转而部署和微调Llama 2”。以上种种，共同揭开了大模型开源闭源之争，发展重心的转移也让人疑惑：开源大模型是否正在“杀死”闭源？01 大模型开源，开的是什么？光锥智能梳理后发现，目前，大模型厂商和创业公司在开源和闭源的选择上，一共有三条路径：一是完全闭源，这类代表公司国外有OpenAI的GPT-3.5、GPT-4，国内有百度的文心大模型；二是先闭源再开源，这类代表公司有阿里云的通义千问，智谱AI开源GLM系列模型；三是先开源再闭源，这类代表公司有百川智能的Baichuan-7B、Baichuan-13B。现在中国市场上能够主动开源大模型，且提供商业许可的企业数量还比较有限，主要公司包括了以开源为切入的百川智能、大模型厂商代表阿里、大模型初创公司代表智谱AI以及走精调Llama 2路线的虎博科技。这从侧面也说明了一个问题，大模型开源并不是没有门槛，相反开源对一家企业的基础技术能力要求十分高，比如智谱AI的GLM-130大模型是去年亚洲唯一入选斯坦福大学评测榜的大模型；阿里通义千问大模型在IDC的“AI大模型技术能力评估测试”中获得了6项满分。如果再进一步将以上的公司分类，可以归为两类，一类是走自研大模型开源路线，一类是走Llama 2路线。这两条路线在国际上也十分典型，譬如走自研模型开源路线的Stability AI，已经陆续开源了Stable DiffusionV1、StableLM、Stable Diffusion XL（SDXL）1.0等模型，凭一己之力撑起了文生图开源领域；另一类如中东土豪研究院就死盯住Llama 2，在其基础上继续做大参数、做强性能， Llama 2开源50天后，地表最强开源模型Falcon 180B横空出世， 霸榜Hugging Face。不过，这两条路线也不是完全泾渭分明，Llama 2的开源也进一步促进了许多自研开源大模型的更新升级。8月Stability AI迅速推出类ChatGPT产品——Stable Chat，背后的大语言模型Stable Beluga就是其在两代Llama的基础上精调出来。更开放，更快迭代发展，这或许也是开源的意义。除了逆天的Falcon，目前开源模型的参数基本都控制在7B-13B左右。大模型厂商告诉光锥智能，“目前7B-13B亿参数量是一个较为合理的开源规模”。这是基于多重因素所得出的参数量规模，如计算资源限制、内存限制、开源成本考量等。阿里云CTO周靖人基于云厂商的角度考虑道：“我们希望企业和开发者，在不同的场景可以根据自己的需求选择不一样规模的模型，来真正地应用在自己的开发环境。我们提供更多可能性。”谈起为何开源大模型，周靖人强调了安全性，“我们不单单只是开源大模型，更重要的是要能够呈现出各项指标的表现效果，基于此，才能够让大家去评估其中的使用风险，更加有效地进行模型应用。”“重要的是，随着参数量的增加，模型效果提升会逐渐收敛。当模型达到一定规模后，继续增加参数对效果提升的边际效益只会下降，70-130亿参数量一般已经接近收敛状态了。”上述大模型厂商道。光锥智能发现，除了阿里云在视觉语言模型的细分领域发布了开源大模型外，其余公司皆提供的是通用能力的大模型。这或许与大模型开源仍处于非常早期阶段有关系，但考虑到开源大模型也要落地到场景中，太过于同质化的通用大模型对企业来说也容易沦为“鸡肋”。如何避免开源大模型重蹈覆辙，体现出开源的价值，回顾Meta接连祭出的“大招”，一条开源的路径似乎逐渐显现——构建开源大模型生态。2月份，Meta凭借开源的Llama大模型回到生成式AI核心阵列；5月9日，开源了新的AI 模型ImageBind，连接文本、图像 / 视频、音频、3D 测量（深度）、温度数据（热）和运动数据六种模态；5个月后，Llama 2开源可商业，含70亿、130亿和700亿三种参数规模，其中700亿参数模型能力已接近GPT-3.5；8月25日，Meta推出一款帮助开发人员自动生成代码的开源模型——Code Llama，该代码生成模型基于其开源大语言模型Llama 2；8月25日，发布全新AI模型SeamlessM4T，与一般AI翻译只能从文本到文本不同，这款翻译器还能够“从语音到文本”或者反过来“从文本到语音”地直接完成翻译；9月1日，允许开源视觉模型DINOv2商业化，同时推出视觉评估模型FACET。可以看到，Meta开源的思路是在各个AI领域遍地开花，通过发布该领域最先进的AI开源模型，吸引更多开发者的关注和使用，壮大整个AI开源生态后来反哺业务、巩固行业地位，这就如同当年的英伟达推动GPU计算的开源策略。当年英伟达推动GPU计算的开源化，不仅吸引了大量研究人员在Caffe、TensorFlow等框架上进行创新，也为自身GPU产品积累了大量优化经验，这些经验后来也帮助英伟达设计出了更适合深度学习的新型GPU架构。另一方面，GPU计算的开源生态越来越繁荣后，也为其带来了巨大的市场空间，Nvidia DGX企业级的深度学习训练平台概念应运而生，为英伟达的显卡和平台销售创造了千亿级市场。国内阿里云也在通过建设完善生态的方式，试图帮助开发者更好的用好大模型，据周靖人介绍，目前阿里云不仅有自研开源大模型，也接入了超过100个开源模型，同时打造了开源社区魔搭，更好地服务开发者和企业用户，用好、调好大模型。02 开源闭源不矛盾，是手段而非目的据外媒爆料，Meta正在加紧研发全新的开源大模型，支持免费商用，能力对标GPT-4，参数量比Llama 2还要大上数倍，计划在2024年初开始训练。国外大模型格局看似是OpenAI“一超多强”，实则是众多公司环伺，可以预见，开源大模型对闭源的围剿，越来越步步紧逼。国外一份研究报告称，大模型前期的发展创新由OpenAI、微软、谷歌等大公司闭源模型主导，但越到后期开源模型和社区的贡献值就越大。光锥智能也了解到，在国内开源大模型也成为了企业的“新卖点”，有企业甚至通过对外宣称已使用了“史上最强大模型Falcon 180B”，来展现其底层模型技术能力的强大，顶着“史上最强”的称号，又收割了“一波韭菜”。现阶段，开源大模型已经证明了几点重要的事实。首先，在非常大的数据集上进行训练，拥有几十亿个参数的大模型在性能上就可以与超大规模大模型相媲美；其次，只需要极少的预算、适量的数据以及低阶适应（Low-rank adaptation，LoRA）等技术就可以把小参数的大模型调到一个满意的效果，且将训练成本降低了上千倍。开源大模型为现在的企业提供了闭源的替代方案，低成本成为最吸引他们的地方；最后，我们也看到开源大模型的发展速度也远快于封闭生态系统。开源固然“迷人”，但更为关键的是，既不能为了开源而开源，也不能为了闭源而闭源。开源与闭源只是形式上的区别，并不矛盾，开源本身不是目的，而是手段。以开源切入大模型赛道的百川智能，在发布完Baichuan-7B、Baichuan-13B开源大模型后，王小川拿出了Baichuan-53B闭源大模型。在问到为什么没有继续开源时，王小川回答称：“模型变大之后没有走开源的这样一种方式，因为大家部署起来成本也会非常的高，就使用闭源模式让大家在网上调用API”。由此可见，是否开源或闭源并非完全没有参考，能够闭源一定是其能够提供价值。在当前，这个价值的集中体现可能是替用户完成高性能的大模型训练、推理和部署，通过调用API的方式来帮助降低门槛，这也是OpenAI闭源的思路，但因为其自身技术的绝对领先优势，使得其价值也非常得大。如果回顾红帽子公司的开源，也能探寻到同样的逻辑。过去十多年间，红帽从销售企业Linux操作系统，扩展到现在的存储、中间件、虚拟化、云计算领域，靠的就是“筛选价值”的逻辑。在最上游的开源社区，参与开源技术贡献，做大做强生态；提取开源社区中的上游技术产品，沉淀到自己小开源社区；再将其认为最有价值的技术检验、测试、打包，形成新的产品组合，完成闭源出售给客户。腾讯云数据库负责人王义成也曾对光锥智能表示：“开源的本质也是商业化，要从宏观层面看是否能满足一家公司的长期商业利益。开源的本质还是扩大生态，扩大你的影响力。开源还是要找清楚自己的定位，目标客户群。开源能否帮助产品突破，帮助公司完成阻击，还需要具体问题具体分析。”03 结尾事实上，开源还是闭源，二者并不是完全对立的关系，只是在技术发展的早期，路径选择的不同。这也并不是科技领域第一次面对这样的分叉路，参考数据库发展的路径，早期需要培育土壤，培植生态，以MySQL为主的开源数据库获得了爆发式的用户增长，但走过第一阶段后，更多企业用户发现开源数据库在面对业务时的短板，毕竟术业有专攻，谁也没办法一招打天下。为此，数据库厂商开始根据不同的企业需求针对性的研发闭源数据库，如在分布式数据库、流数据库等细分类别进行长足的创新。周靖人也认为：“未来，一定不是one size fits all”，不同的场景适配不同的参数，不同的形式，届时大模型将走过野蛮生长阶段，来到精耕细作。这也足以说明，开源还是闭源，或许只是阶段和位置的不同，但可以肯定的是，大模型时代，已经加速进入下一赛段。欢迎关注光锥智能，获取更多科技前沿知识！",
        "voteup_count": 1,
        "updated_time": "2023-10-07 19:30:05",
        "question_id": 623672939,
        "user_id": "71d43ff7c1111af75a397c711eec8104"
    },
    {
        "answer_id": 3482307873,
        "content": "不用怀疑！Llama3 开源即巅峰，霸榜githubMeta Llama 3 官方 GitHub 网站https://github.com/meta-llama/llama3此版本包括预训练和指令调整的 Llama 3 语言模型的模型权重和起始代码 - 包括 8B 到 70B 参数的大小。该存储库旨在作为加载 Llama 3 模型并运行推理的最小示例。有关更详细的示例，请参阅llama-recipes。下载要下载模型权重和分词器，请访问Meta Llama 网站并接受我们的许可证。一旦您的请求获得批准，您将通过电子邮件收到签名的 URL。然后运行 download.sh 脚本，并在提示开始下载时传递提供的 URL。先决条件：确保您已经wget安装md5sum。然后运行脚本：./download.sh。请记住，链接将在 24 小时和一定下载量后过期。如果您开始看到诸如 之类的错误403: Forbidden，您可以随时重新请求链接。获得拥抱脸我们还提供Hugging Face上的下载，包括转换器和本机llama3格式。要从 Hugging Face 下载权重，请按照以下步骤操作：访问其中一个存储库，例如meta-llama/Meta-Llama-3-8B-Instruct。阅读并接受许可证。一旦您的请求获得批准，您将有权访问所有 Llama 3 模型。请注意，处理请求最多需要一小时。要下载原始本机权重以与此存储库一起使用，请单击“文件和版本”选项卡并下载文件夹的内容original。您还可以从命令行下载它们，如果您pip install huggingface-hub：huggingface-cli download meta-llama/Meta-Llama-3-8B-Instruct --include \"original/*\" --local-dir meta-llama/Meta-Llama-3-8B-Instruct要与转换器一起使用，以下管道片段将下载并缓存权重：import transformersimport torchmodel_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"pipeline = transformers.pipeline( \"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", model_kwargs={\"torch_dtype\": torch.bfloat16}, device=\"cuda\",)快速开始您可以按照以下步骤快速启动并运行 Llama 3 模型。这些步骤将让您在本地运行快速推理。有关更多示例，请参阅Llama 食谱存储库。在具有 PyTorch / CUDA 的 conda 环境中，可以克隆并下载此存储库。在顶级目录中运行：pip install -e .访问Meta Llama 网站并注册以下载模型。注册后，您将收到一封电子邮件，其中包含下载模型的 URL。运行 download.sh 脚本时您将需要此 URL。收到电子邮件后，导航到下载的 llama 存储库并运行 download.sh 脚本。确保授予 download.sh 脚本执行权限在此过程中，系统将提示您输入电子邮件中的 URL。不要使用“复制链接”选项，而应确保手动复制电子邮件中的链接。下载所需的模型后，您可以使用以下命令在本地运行该模型：torchrun --nproc_per_node 1 example_chat_completion.py \\\n    --ckpt_dir Meta-Llama-3-8B-Instruct/ \\\n    --tokenizer_path Meta-Llama-3-8B-Instruct/tokenizer.model \\\n    --max_seq_len 512 --max_batch_size 6笔记替换 Meta-Llama-3-8B-Instruct/为检查点目录的路径和Meta-Llama-3-8B-Instruct/tokenizer.model分词器模型的路径。应将其–nproc_per_node设置为您正在使用的型号的MP值。根据需要调整max_seq_len和参数。max_batch_size此示例运行在此存储库中找到的example_chat_completion.py，但您可以将其更改为不同的 .py 文件。推理不同的模型需要不同的模型并行 (MP) 值：模型国会议员8B170B8max_seq_len所有模型都支持高达 8192 个令牌的序列长度，但我们根据和值预先分配缓存max_batch_size。因此，请根据您的硬件进行设置。预训练模型这些模型未针对聊天或问答进行微调。应该提示他们，以便预期的答案成为提示的自然延续。请example_text_completion.py参阅一些示例。为了说明这一点，请参阅下面的命令以使用 llama-3-8b 模型运行它（nproc_per_node需要设置为该MP值）：torchrun --nproc_per_node 1 example_text_completion.py \\\n    --ckpt_dir Meta-Llama-3-8B/ \\\n    --tokenizer_path Meta-Llama-3-8B/tokenizer.model \\\n    --max_seq_len 128 --max_batch_size 4\n指令调整模型经过微调的模型针对对话应用进行了训练。为了获得预期的功能和性能，ChatFormat 需要遵循中定义的特定格式：提示以<|begin_of_text|>特殊标记开始，后面跟随一条或多条消息。每条消息均以<|start_header_id|>标记、角色system或和标记开头。在双换行符之后是消息的内容。每条消息的结尾都由令牌标记。userassistant<|end_header_id|>\\n\\n<|eot_id|>您还可以部署其他分类器来过滤掉被认为不安全的输入和输出。请参阅 llama-recipes 存储库，了解如何向推理代码的输入和输出添加安全检查器的示例。使用 llama-3-8b-chat 的示例：torchrun --nproc_per_node 1 example_chat_completion.py \\\n    --ckpt_dir Meta-Llama-3-8B-Instruct/ \\\n    --tokenizer_path Meta-Llama-3-8B-Instruct/tokenizer.model \\\n    --max_seq_len 512 --max_batch_size 6\nLlama 3 是一项新技术，使用时存在潜在风险。迄今为止进行的测试尚未（也不可能）涵盖所有场景。为了帮助开发人员解决这些风险，我们创建了负责任的使用指南。问题请通过以下方式之一报告任何软件“错误”或模型的其他问题：报告模型问题：https://github.com/meta-llama/llama3/issues报告模型生成的有风险内容：http://developers.facebook.com/llama_output_feedback报告错误和安全问题：http://facebook.com/whitehat/info型号卡请参阅MODEL_CARD.md。搭建基于Llama的应用主要涉及以下步骤：一、准备工作确保你有一台运行操作系统的计算机（Windows、Mac或Linux）。确保你有一定的Python编程经验，因为Llama2通常需要通过Python环境进行搭建和配置。确保你有一个可靠的网络连接，以便下载所需的依赖库和Llama2代码。二、安装Python访问Python官方网站下载并安装适合你操作系统的Python版本。Llama2需要Python 3.5或更高版本。三、安装依赖库打开终端或命令提示符。运行pip install numpy pandas matplotlib来安装这些常用的数据科学库，它们将帮助你处理和分析数据。四、下载Llama2代码访问Llama2的GitHub页面或其他官方代码托管平台。下载最新的Llama2代码压缩包或直接克隆仓库到本地。五、配置环境解压下载的Llama2代码（如果是压缩包）。进入Llama2代码的文件夹。运行pip install -r requirements.txt来安装Llama2所需的所有依赖库。六、运行Llama2在终端或命令提示符中，确保你仍在Llama2代码的文件夹内。运行python llama2.py（或相应的启动脚本）来启动Llama2应用程序。访问指定的URL来访问Llama2的用户界面，并开始使用各种功能来处理和分析数据。此外，如果你想在Docker环境中搭建Llama2，你需要：安装Docker。创建一个Docker镜像，包含Llama2及其依赖。运行Docker容器来执行Llama2。请注意，具体的步骤和命令可能会因Llama2版本和你的操作系统而有所不同。始终建议参考Llama2的官方文档或GitHub页面上的说明以获取最新和最准确的安装指南。最后，虽然上述步骤提供了一个大致的框架来搭建基于Llama的应用，但具体的实现细节和配置可能会因你的具体需求和环境而有所不同。",
        "voteup_count": 1,
        "updated_time": "2024-04-29 09:53:14",
        "question_id": 623672939,
        "user_id": "a17c1d40f792435bb6deecb08e9817b2"
    },
    {
        "answer_id": 3409287305,
        "content": "2月22日，Google发布了轻量化开源模型Gemma：gemma (ollama.com)。Gemma 是一个轻量级、最先进的开放式模型系列，采用与创建 Gemini 模型相同的研究和技术构建。Gemma 由 Google DeepMind 和 Google 的其他团队开发，灵感来自双子座，这个名字反映了拉丁语 gemma，意思是“宝石”。此链接为Gemma的详细介绍：Gemma：Google 推出最先进的新开放模型 (blog.google)在多项基准测试中，Gemma 7B的得分，都超过了拥有130亿参数的LLAMA 13B模型。从Google提供的报告可以看出，Gemma模型在各种 Web 文档数据集上进行训练，以使其具有广泛的语言风格、主题和词汇。这包括用于学习编程语言的语法和模式的代码，以及用于掌握逻辑推理的数学文本。和大语言模型不同的是，轻量化模型可以直接在笔记本和桌面端运行，以适应不同应用场景的需要。此外Nvidia也和Google进行合作，提高Gemma的推理速度和微调能力。Nvidia提供了Gemma在线体验页面，以Gemma 2B模型为例，提出一个问题即可获得详细的答案。使用Gemma 7B模型，提出和代码相关的问题，也可以快速提供答案，或者使用中文提问，也可以输出对应的结果。Perplexity labs也在第一时间添加了Gemma模型到Playground：Perplexity Labs，选择对应的模型即可使用。下面打开网址，我做一下演示：提出一个中文问题“什么是回忆，怎么提高记忆力？”，在以gemma 7b模型输入时，从记忆技巧、注意力专注力、睡眠、药物等方面来回答如何提高记忆力。输出速度可达每秒200 TOKEN，响应速度可以达到毫秒级，回答的内容也十分准确和详细。下面尝试一个英文问题“How to improve interpersonal skills？”上图可以查看输出的回答，切换到其他模型，也可以输出非常详细的答案。如果想要在本地运行Gemma，也可以使用Olegma工具，完成一键本地部署。感兴趣的同学可以在上面网站尝试该模型。",
        "voteup_count": 2,
        "updated_time": "2024-02-26 08:22:07",
        "question_id": 623672939,
        "user_id": "19abb64e8cac1d72144c177a782c5445"
    },
    {
        "answer_id": 3391343394,
        "content": "Open-Source Language Model Pocket 春节都安好，所想皆如愿  All the best for the Spring Festival  May all your wishes come true 完整版（Full Version）: https://github.com/createmomo/Open-Source-Language-Model-Pocket微信公众号版：开源语言模型百宝袋 (Ver. 3.3)相关文章穷穷穷孩子如何体验ColossalAI SFT（Kaggle篇，Colab篇）通俗理解文本生成的常用解码策略通俗理解P-tuning (GPT Understands)通俗理解Gradient Checkpoint（附代码）千“垂”百炼：垂直领域与语言模型系列文章导语获得可用的垂直领域数据 【不限领域】利用未标注文本改进遵循指令的语言模型 (1) Instruction Backtranslation 简介【医疗/健康】ChatDoctor （解读 上 中 下）【医疗/健康】MedicalGPT-zh【医疗/健康】明医(MING)【医疗/健康】灵心(SoulChat)自动评估模型 【不限领域】用语言模型评估语言模型（1）导语【不限领域】用语言模型评估语言模型（2）PandaLM【不限领域】用语言模型评估语言模型（3）Shepherd（1 2 3 4）【医疗/健康】使用BERT-Score比较ChatDoctor与ChatGPT3.5开源模型一览 (Table of Contents)：中文友好或国内主创的开源模型（Chinese Open Source Language Models）多个领域/通用百川中文Alpaca Luotuo中文LLaMA&Alpaca大模型中文LLaMA&Alpaca大模型2流萤Firefly凤凰复旦MOSS复旦MOSS-RLHF悟道·天鹰Aquila&Aquila2雅意大模型通义千问Qwen活字AnimaBayLingBELLEBloomBiLLaBLOOMChat176BChinese-Llama-2-7b (LinkSoul-AI)GPT2 for Multiple LanguageInternLM 书生・浦语Llama2-chat-Chinese-50WLlama2-Chinese (FlagAlpha)Linly伶荔说 中文 LLaMA1-2 & OpenLLaMA & Falcon 大模型ChatRWKVChatYuanChatGLM-6BChatGLM2-6BChinese-Transformer-XLOpenKG-KnowLLMPromptCLUESkyText-Chinese-GPT3CPM-BeeTigerBotXVERSE-13BYuLan-Chat & YuLan-Chat-2Ziya-LLaMATechGPTEVAFLM-101BTinyLlamaColossal-LLaMA-2OpenBA (Encoder-Decoder)Ziya-Reader-13BFirefly-LLaMA2-ChineseMindLLMChatGLM3Skywork大模型Yi-6B/34B（零一万物）Nanbeige-16B（南北阁-16B）OrionStar-Yi-34B-Chat源2.0TechGPT2.0SUS-Chat-34BAlaya 元识OpenBuddy*【MiniGPT4Qwen】*【ChatLM-Chinese-0.2B】*【YAYI 2】*【DeepSeek LLM&MoE】*【MachineMindset(MBTI)】*【星辰语义（电信）】*【Chinese-Mixtral-8x7B】*【Baby-Llama2-Chinese】*【XVERSE-13B-256K】*【Eagle 7B（RWKV-v5）】*【iFlytekSpark-13B】*【MiniCPM】*【通义千问Qwen1.5】医疗健康本草华佗扁鹊灵心启真儿童情感陪伴大模型“巧板”OpenMEDLab 浦医明医 (MING)：中文医疗问诊大模型 (原名：MedicalGPT-zh)情感大模型PICAChinese-Vicuna-medicalMedicalGPTDISC-MedLLM （复旦）DoctorGLMChatMed-TCM&ChatMed-ConsultChatGLM-MedMeChatShenNong-TCM-LLMMindChat(漫谈): 心理大模型WiNGPTCareGPT孙思邈MolGen（药物研发）Taiyi（太一）*【MedAgents】*【Molecule Optimization】经济/金融貔貅FinMA & PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance轩辕BBT-FinCUGE-ApplicationsCornucopia-LLaMA-Fin-ChineseEcomGPTFinGLMDISC-FinLLM法律韩非 HanFei智海 录问ChatLaw 法律大模型LaWGPTLawyer LLaMALexiLawLawGPT_zh夫子•明察司法大模型DISC-LawLLMLawBench交通TransGPT · 致远教育&数学桃李EduChatchatglm-mathsAbel*【InternLM-Math】*【DeepSeekMath】表格/数据分析TableGPTData-Copilot*【Tabular LLM】自媒体&角色扮演MediaGPTCharacterGLM-6B*【Haruhi-Zero】古汉语尔雅 Erya*【荀子】编程/代码/AgentCodeShellCODEFUSION-75MDeepSeek CoderDevOps-Model（运维）MagicoderKwaiAgents*【LLaMA-Pro】*【HuixiangDou】天文/海洋/地球科学/科学星语StarWhisperOceanGPT*【K2&GeoGalactica】*【SciGLM】可参考的其它开源模型CerebrasMPT-7BChatDoctorOpenGPTCode Llama (Meta AI)OrcaDolly 1&2OpenChatKitFinGPTOpen-AssistantFalconPlatypusFacebook/Meta LLaMA/LLaMA2MedLLaMA-13B & PMC-LLaMA: Continue Training LLaMA on Medical PapersGiraffeRedPajamaGALACTICASQLCoder (Defog)Goar-7B for Arithmetic TasksStableLMHuggingChatStableVicunaKoala: A Dialogue Model for Academic ResearchStanford AlpacaLongLLaMAUltraLM-13BLLaMA复刻版OpenLLaMAVicuna: An Open-Source Chatbot Impressing GPT-4 with 90% ChatGPT QualityLlama-X: Open Academic Research on Improving LLaMA to SOTA LLMWombatLit-LLaMA ️WizardMathMammoTHXGen-7BMistral 7BXwin-LMLLaMA 2 LongUltraLM-13B (UltraFeedback)Llemma: An Open Language Model For MathematicsMistral-Trismegistus-7B （神秘学/玄学/灵性）Memory-GPT(MemGPT)MetaMathChipNeMo (芯片设计)Zephyrneural-chat-7b-v3-1（Intel）SteerLMLlama CoderMeditronRankZephyrStableLM Zephyr 3BOrca 2Mixtral 7b 8 ExpertPhiLLM360（Amber,CrystalCoder,Diamond）MambaSOLARNexusRaven（function calling LLM）LLaMA-MoE*【TinyLlama】*【Nous-Hermes-2 Mixtral 8x7B】*【AlphaGeometry】*【MoE-Mamba】*【StarCoder】*【OLMo】*【H2O-Danube-1.8B】训练/推理Alpaca-LoRAllama2.mojoAlpacaFarmLightLLMColossalAIMedusaChatLLaMAMegatron-LLaMAChinese-GuanacoMeZO: Fine-Tuning Language Models with Just Forward PassesDPO (Direct Preference Optimization)MLC LLMDialogADV：Evaluate What You Can't Evaluate: Unassessable Generated Responses QualityPKU-Beaver 河狸 (Safe RLHF)DeepSpeed-ChatPaLM + RLHF (Pytorch)FlexGenRL4LMsFlagAI and FlagDataReinforcement Learning with Language ModelGuanaco & QloRASpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight CompressionGPT4AllScikit-LLM: Sklearn Meets Large Language ModelsHugNLPTransformer Reinforcement LearningINSTRUCTEVALTrain_Transformers_with_INT4LOw-Memory Optimization (LOMO)Transformer Reinforcement Learning Xllama.cppvLLMllama2.cLongLoRARLLTE: Long-Term Evolution Project of Reinforcement LearningFlashAttentionExecuTorchTensorRT-LLMBPO（Black-Box Prompt Optimization）S-LoRASoRAXuanCe(玄策): 开源的深度强化学习(DRL)库EasyLM（JAX/Flax）FATE-LLM - Federated Learning for LLMsDeepSpeed-FastGenNVIDIA NeMo-AlignerRLAIF: Scaling Reinforcement Learning from Human Feedback with AI FeedbackMLXOpenRLHFCoLLiE: Collaborative Training of Large Language Models in an Efficient WaySuperalignmentLLMLingua: Compressing Prompts for Accelerated Inference of Large Language ModelsLarge Language Model UnlearningPowerInferm-LoRA*【LASER】*【StripedHyena-7B】*【SwiftInfer】*【SPIN（Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models）】*【Self-Rewarding Language Models】*【OPO（On-the-fly Preference Optimization）】*【ASPIRE】*【The Impact of Reasoning Step Length on Large Language Models】*【SliceGPT】*【FuseLLM】*【Tree of Thoughts】*【CogGPT】*【KTO（Kahneman-Tversky Optimisation）】*【Aligner】*【RPO（Robust Prompt Optimization）】*【Inference-Time Training Helps Long Text Generation】*【LiPO】*【ChatLLM.cpp】*【Self-Discover】评价天秤（FlagEval）獬豸（Xiezhi）BenchmarkC-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation ModelsHaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language ModelsKoLA: Carefully Benchmarking World Knowledge of Large Language ModelsLucyEval—中文大语言模型成熟度评测CMB: A Comprehensive Medical Benchmark in ChineseMultiscale Positive-Unlabeled Detection of AI-Generated TextsPandaLMAuto-JCLEVA: Chinese Language Models EVAluation PlatformALCUNA: Large Language Models Meet New KnowledgeHalluQA：Evaluating Hallucinations in Chinese Large Language ModelsGLoRE: Evaluating Logical Reasoning of Large Language ModelsHelpSteerAlignBench: 多维度中文对齐评测基准UHGEvalPurple Llama (Meta)OMGEvalSciGuard&SciMT-Safety*【HaluEval 2.0, The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models】*【DebugBench: Evaluating Debugging Capability of Large Language Models】*【GenMedicalEval】*【R-Judge】*【TravelPlanner】*【EasyJailbreak】*【AgentBench】文本向量*【Matryoshka Representation Learning】*【Jina Embeddings】*【BGE-M3】*【Nomic Embed】*【Moka Massive Mixed Embedding（M3E）】AgentAuto-GPTToolBench&ToolLLMHuggingGPTCAMEL:Communicative Agents for “Mind” Exploration of Large Scale Language Model SocietyAgentLM (AgentTuning, AgentInstruct)XAgentOpenAgents*【Personal LLM Agents - Survey】*【AUTOACT】*【MetaGPT】*【Multi-LLM-Agent】其它Alpaca-CoTSelf-InstructChatPiXiuWanda (Pruning by Weights and activations)GorillaStreaming LLMSheared LLAMA (Structured Pruning)gpu_poorLLMPruner：大语言模型裁剪工具QA-LoRALLM-Pruner: On the Structural Pruning of Large Language ModelsLLM for Recommendation SystemsTransformer Index for GEnerative Recommenders (TIGER)KnowPATAuthentiGPT: Detecting Machine-Generated TextCuriosity-driven Red-teaming for Large Language Models*【TinyGSM】*【MathPile】*【Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM】*【Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding】*【QAnything】*【Meta-Prompting】*【Lepton Search】*【RLMRec】“ 持续更新中 (Continuously Updated)... ”",
        "voteup_count": 8,
        "updated_time": "2024-02-08 20:50:54",
        "question_id": 623672939,
        "user_id": "0349dc6ac1dd9a4bcf7d3711da8d9bd3"
    },
    {
        "answer_id": 3483635246,
        "content": "截至2024年4月30日，已知的开源大模型主要有以下几个：LaMDA：由谷歌AI开发，是迄今为止最大的开源语言模型之一，拥有1370亿个参数，在各种自然语言处理任务上表现出卓越的性能。https://blog.google/technology/ai/lamda/Megatron-Turing NLG：由NVIDIA和微软合作开发，拥有5300亿个参数，是世界上最大的神经语言模型之一，在文本生成、机器翻译和问答等任务上表现出强大的能力。https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/WuDao 2.0：由北京智源人工智能研究院开发，拥有1.75万亿个参数，是世界上最大的中文语言模型之一，在中文理解和生成方面表现出最先进的水平。https://gpt3demo.com/apps/wu-dao-20BARD：由百度文心工作室开发，拥有10万亿个参数，是世界上最大的中文-英文双语大模型之一，在跨语言理解和生成方面表现出强大的能力。http://research.baidu.com/Blog/index-view?id=165OPT：由OpenAI开发，拥有1750亿个参数，是一个通用的大型语言模型，在各种自然语言处理任务上表现出良好的性能。https://openai.com/Bloom：由Hugging Face和BigScience合作开发，拥有1760亿个参数，是一个多模态的大型语言模型，可以处理文本、图像和代码等多种信息类型。https://huggingface.co/docs/transformers/en/model_doc/bloomJurassic-1 Jumbo：由AI21 Labs开发，拥有1780亿个参数，是一个大型语言模型，在各种自然语言处理任务上表现出良好的性能。https://www.ai21.com/blog/announcing-ai21-studio-and-jurassic-1Eleuther：由EleutherAI开发，拥有1370亿个参数，是一个大型语言模型，在各种自然语言处理任务上表现出良好的性能。https://www.eleuther.ai/ 除了上述列出的模型之外，还有一些其他的开源大模型正在开发中，例如由Google AI开发的Pathways System 2和由微软开发的DeepSpeed Megatron-Turing NLG。 开源大模型的出现为研究人员和开发人员提供了宝贵的资源，可以促进自然语言处理技术的进步，并推动新的应用和创新。 ",
        "voteup_count": 0,
        "updated_time": "2024-04-30 12:34:54",
        "question_id": 623672939,
        "user_id": "6c6d72e9f948dcf69f0034872c71f291"
    },
    {
        "answer_id": 3235241868,
        "content": "之前写过一篇文章，供参考。目前市面上有大量的开源大模型，而且还在不断涌现。一方面我们要关注最新的发展态势，同时大模型也存在良莠不齐的情况，特别是国内有些大模型只是简单套壳，这存粹是浪费我们的时间。当然我们也不可能什么模型都去碰，以下根据我所了解的信息以及结合试用模型的一些情况，挑选出文本生成大模型以及文生图大模型的一些典型代表，其中也包括国内三家典型的文本生成模型代表，如果我们要学习大模型，可以从如下这些开源大模型入手，比如文本生成模型挑选1到2个，如Llama2或者Mistral，或者挑一个国内的大模型，如ChatGLM，当然如果你要挑选文生图，你可以挑选1个进行学习，如runwayml的stable diffusion 1.5或者StabilityAI的SDXL 1.0，各位可以根据自己的喜好或者目标挑选如下相应的开源大模型：GPT-2：GPT-2 (Generative Pre-trained Transformer 2) 是由 OpenAI 开发的大型自然语言处理模型，在2019年2月发布，是当时最好的开源大模型，可以算是大模型发展的一大里程碑，由于GPT-3和GPT-4不再开源，所以这是唯一能获得OpenAI开源的文本生成大模型。它基于 Transformer 架构，具有 1.5 亿个参数。这个模型是为了处理各种自然语言处理任务而设计的，包括但不限于文本生成、翻译、问答以及摘要生成。Falcon180B：Falcon180B 是由阿布扎比的全球领先技术研究中心 TII 发布的世界顶级开源大模型。此模型在 3.5 万亿 token 的训练下，拥有 1800 亿参数，性能超过了之前的开源模型 Llama2 70B和GPT 3.5 ，甚至接近了OpenAI的 GPT-4​。可惜大部分玩家玩不起。Llama 2：Llama 2 是由 Meta AI 开发的下一代大型开源语言模型，可免费用于研究和商业用途。于 2023 年 7 月 18 日发布。Llama 2 包括预训练和微调的大型语言模型，模型参数范围从 70 亿到 700 亿不等​​。预训练模型是在 2 万亿个令牌上训练的，具有比 Llama 1 两倍的上下文长度。微调模型在超过 100 万个人类注释上进行了训练。目前Llama2已经成为文本生成模型的标杆，大部分模型都会拿它做基准对比，国内很多开源大模型更是在它的基础进行微调和训练。Mistral-7B：Mistral-7B 是 Mistral AI 开发的大型语言模型，定位为 OpenAI 的 GPT-3 和 GPT-4 等高容量模型的替代选择，根据Apache 2.0 许可证发布，允许在各种环境中无限制地使用和部署。它拥有70亿个参数。是目前小模型的佼佼者。Mistral-7B 在多个指标上显著优于 Llama 2 13B，并在某些领域与 Llama 34B 相当。基准测试涵盖了包括常识推理、世界知识、阅读理解、数学和编码等多个主题。值得注意的是，Mistral-7B 的表现相当于一个比其大3倍的假想 Llama 2 模型，显示了其设计的高效率。BLOOM：BLOOM 是一个大型多语言模型，由 BigScience 项目创建，这个项目是一个开放的合作项目，有来自全球的数百名研究人员和机构参与​​。超过 1,000 名 AI 研究人员参与了 BLOOM 的创建，目的是为了提供一个可以大规模公开访问的大型语言模型。BLOOM 模型拥有 1760 亿个参数，能够在 46 种自然语言和 13 种编程语言中生成文本。对于其中的绝大多数语言，例如西班牙语、法语和阿拉伯语，BLOOM 是第一个拥有超过 1000 亿参数的语言模型​。StarCoder：StarCoder 是一个为代码而设计的大型语言模型（LLM），由 Hugging Face 和 ServiceNow 合作开发了 StarCoder。StarCoder 和 StarCoderBase 模型都具有大约 155 亿的参数，能处理长达 8,000 个令牌的文本序列​。这些模型是在来自 GitHub 的许可许可数据上训练的，包括 80 多种编程语言、Git 提交、GitHub 问题和 Jupyter 笔记本​​。StarCoder 现已可用于 Visual Studio Code，作为 GitHub Copilot 的替代品，用于代码生成。Watsonx.ai Geospatial Foundation Model：Watsonx.ai Geospatial Foundation Model 是 IBM 与 NASA 合作开发的一个大型开源地理空间人工智能模型，这是目前Hugging Face最大的开源地理模型。这个模型代表了气候科学和地球研究的重要进展，旨在促进 AI 的民主化和加速这些关键领域的创新​。这模型是基于 NASA 的卫星数据建立，目的是将卫星数据转换为高分辨率的洪水、火灾和其他景观变化的地图，以揭示我们的星球的过去并预示其未来​。ChatGLM-6B：ChatGLM-6B 是一个开源的、支持中英双语问答的对话语言模型，基于 General Language Model (GLM) 架构，具有 62 亿参数。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）。ChatGLM-6B 使用了和 ChatGLM 相同的技术，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62 亿参数的 ChatGLM-6B 已经能生成相当符合人类偏好的回答。 ChatGLM-6B 权重对学术研究完全开放，在填写问卷进行登记后亦允许免费商业使用。Baichuan-7B：Baichuan-7B是由百川智能开发的一个开源的大规模预训练模型。基于Transformer结构，在大约1.2万亿tokens上训练的70亿参数模型，支持中英双语，上下文窗口长度为4096。在标准的中文和英文权威benchmark（C-EVAL/MMLU）上均取得同尺寸最好的效果。Baichuan-7B使用宽松的开源协议，允许用于商业目的。Qwen-7B：通义千问-7B（Qwen-7B）是阿里云研发的通义千问大模型系列的70亿参数规模的模型。Qwen-7B是基于Transformer的大语言模型, 在超大规模的预训练数据上进行训练得到。预训练数据类型多样，覆盖广泛，包括大量网络文本、专业书籍、代码等。同时，在Qwen-7B的基础上，我们使用对齐机制打造了基于大语言模型的AI助手Qwen-7B-Chat。相较于最初开源的Qwen-7B模型，我们现已将预训练模型和Chat模型更新到效果更优的版本。runwayml/stable-diffusion-v1-5：RunwayML 的 Stable-Diffusion-v1-5 是一个基于深度学习的大型模型，专门用于文本到图像的生成。Stable-Diffusion-v1-5 是基于 Stable-Diffusion-v1-2 的权重初始化，随后在 \"laion-aesthetics v2 5+\" 数据集上以 512x512 的分辨率进行了 595k 步的微调，同时减少了 10% 的文本条件化以改善无分类器的引导采样。该模型可以与 Diffusers 库一起使用​。是目前最受欢迎的文生图的开源模型之一。stabilityai/stable-diffusion-2-1：Stable-Diffusion-2-1 是一个继 Stable-Diffusion-2 后微调过的模型，由 Stability AI 开发，并专注于从文本生成图像，能够生成详细和逼真的图像，具有改善的组成和逼真的美学，这些图像是基于较短的文本提示生成的。此模型首先从 Stable-Diffusion-2（768-v-ema.ckpt）微调，通过在相同的数据集上添加额外的 55k 步（带有 punsafe=0.1），然后进行了另外 155k 额外步骤的微调（带有 punsafe=0.98）。它也是目前最受欢迎的文生图的开源模型之一。stabilityai/stable-diffusion-xl-base-1.0：Stability AI的Stable Diffusion XL 1.0 模型，也被称为SDXL 1.0，是一个基于文本到图像的生成模型，包含一个名为\"ensemble of experts\"的流程，用于潜在扩散。首先，基础模型用于生成（带噪声的）潜在变量，然后通过专门用于最终去噪步骤的细化模型进一步处理这些变量。SDXL 1.0是文本到图像生成模型的下一个迭代，Stability AI描述Stable Diffusion XL 1.0为其迄今为止\"最先进\"的文本到图像模型。根据评估结果，SDXL 1.0 基础模型的性能显著优于 Stable Diffusion 1.5 和 2.1 的变体。当与细化模块结合时，SDXL 1.0 实现了最佳的整体性能.",
        "voteup_count": 3,
        "updated_time": "2023-10-03 15:15:48",
        "question_id": 623672939,
        "user_id": "17674eb41cf5ef88cdc1e69d5169a8d7"
    },
    {
        "answer_id": 3484694399,
        "content": "目前已知的开源大模型包括但不限于以下几种：Skywork-13B系列：由昆仑万维开源，包含基础模型和数学模型，专为中文优化，具有130亿参数。GROK：3140亿参数的混合专家模型，是目前为止参数量最大的开源LLM。Gemma：谷歌开源的模型，提供了不同大小的版本，部分版本可免费商用。Mixtral：Mistral AI的模型，宣称超越GPT3.5，重新定义AI性能和多样性。LLama2：OpenAI开源的模型，提供了不同规模的版本，从70亿到650亿参数不等。WizardLM：微软发布的模型，使用ChatGPT对指令进行复杂度进化微调LLama2。Falcon：阿联酋技术研究所推出的模型，训练数据量达3.5万亿token。Vicuna：基于LLama13B使用ShareGPT指令微调的模型。OpenSora：高效复现类Sora视频生成的完全开源方案。Baichuan：百川智能开源的7B大模型，可商用。Qwen：通义千问升级的模型，支持32K上文。这些模型在不同的评测基准上表现优异，覆盖了从基础的语言建模到特定领域的应用，如数学问题解决、编程、企业智能等。开源大模型的可用性为研究人员和开发者提供了丰富的资源，以进行各种NLP任务和探索新的应用场景。",
        "voteup_count": 1,
        "updated_time": "2024-05-01 15:29:12",
        "question_id": 623672939,
        "user_id": "9b5deb65baf04849cac5e85a4e71e715"
    },
    {
        "answer_id": 3307279764,
        "content": "",
        "voteup_count": 0,
        "updated_time": "2023-11-29 19:30:03",
        "question_id": 623672939,
        "user_id": "0e4a13661c2a1112787cd9381a3b90bc"
    },
    {
        "answer_id": 3461546757,
        "content": "一、本地部署1.1 下载客户端官网地址：https://gpt4all.io/index.html，这里根据我们电脑的操作系统选择对应的客户端。1.2 客户端安装和我们平时安装软件一样，这里目录优先选择磁盘空间大的位置，新建一个文件夹用于存放这个软件数据1.3 安装完毕基本设置匿名使用分析，以改进GPT4AI：是 / 否匿名分享聊天到GPT4AI数据湖：是 / 否上面有个内容都勾选否就行，简单来说就是是否需要联网查询1.4 模型选择我们先选择一个评分高的1.5 模型下载这里我们点击download models热点：可以接入gpt4，只是需要电脑配置和API来承接等待下载完毕我们可以多下几个如果客户端下载缓慢，我们直接在网页端下载下载之后导入到对应的文件目录二、模型下载完毕2.1 导入模型特别说明：路径别改，我上面的步骤改了，但是模型导进去不生效，识别不了，大家保持默认就行2.2 效果测试唯一的缺点，没有算力，响应很慢",
        "voteup_count": 2,
        "updated_time": "2024-04-10 23:14:20",
        "question_id": 623672939,
        "user_id": "913f689d53c9bd16c78307c16037942b"
    },
    {
        "answer_id": 3484216477,
        "content": "Meta的LLaMA系列由Meta开源，包含不同规模的模型，从7亿到650亿参数不等，适合在多种设备上运行。官方论文和介绍：LLaMA-2 PaperHugging Face博客介绍：LLaMA-2 Hugging Face BlogGitHub仓库：meta-llama/llama3BLOOMHugging Face提供的1760亿参数模型，支持多语言处理任务。Hugging Face模型页面：BLOOM-176BBLOOM项目介绍：BigScience BLOOM论文：BLOOM Paper斯坦福大学的Stanford Alpaca基于LLaMA 7B进行指令调优的模型，提供了训练数据和代码的开源。官方介绍：Stanford CRFM AlpacaGitHub仓库：tatsu-lab/stanford_alpaca清华大学的ChatGLM支持中英双语的对话语言模型，基于General Language Model（GLM）架构，针对中文进行了优化。清华大学开源：ChatGLM GitHubStableLM由Stability AI开发的语言模型，具有不同参数规模的模型，可以生成文本和代码。Stability AI的模型：StableLM GitHub",
        "voteup_count": 0,
        "updated_time": "2024-04-30 23:31:54",
        "question_id": 623672939,
        "user_id": "2e324bb81939567cdbc283591bd53991"
    },
    {
        "answer_id": 3486909271,
        "content": "目前已知的开源大模型包括多种类型，其中一些在人工智能领域具有广泛影响。以下是一些常见的开源大模型：GPT系列模型：由OpenAI开发的自然语言处理模型，包括GPT-3和GPT-4等。这些模型在文本生成、对话系统、问答系统等多个领域表现出色。TensorFlow 2.x：这是由Google开源的深度学习框架，以其易用性和高性能赢得了广大开发者的青睐。TensorFlow 2.x提供了丰富的API和工具，用于构建和训练深度学习模型。PyTorch Lightning：这是PyTorch框架的一个扩展库，为深度学习研究者和工程师提供了高效、可扩展的训练解决方案。Hugging Face Transformers：这是一个开源的预训练模型库，包含了众多主流的开源大模型，如BERT、GPT等。这些模型可以用于各种自然语言处理任务，如文本分类、命名实体识别等。BERT：由Google开发的大型语言模型，它采用了双向Transformer编码器结构，能够捕捉文本中的上下文信息。BERT在各种自然语言处理任务中都取得了显著的性能提升。T5 (Text-to-Text Transfer Transformer)：这也是由Google开发的大型语言模型，它采用了文本到文本的转换任务设计，可以处理各种自然语言处理任务。Megatron：由NVIDIA开发的大型语言模型，可以扩展到上万亿个参数。这个模型在超大规模语言模型训练方面具有很高的性能。RoBERTa：由Facebook开发的大型语言模型，是BERT的一个变体，具有更大的模型规模和更长的训练时间。RoBERTa在多项自然语言处理任务中都取得了比BERT更好的性能。Turing-NLG：由Microsoft开发的大型语言模型，拥有170亿个参数。这个模型在自然语言生成方面具有很高的性能。DALL-E：由OpenAI开发的大型生成模型，可以生成具有给定描述的图片。这个模型在图像生成领域引起了广泛关注。以上只是目前已知的一部分开源大模型，实际上还有很多其他优秀的开源模型可供选择和使用。这些模型的出现极大地推动了人工智能领域的发展和应用。",
        "voteup_count": 1,
        "updated_time": "2024-05-04 09:33:48",
        "question_id": 623672939,
        "user_id": "5ec106ca5587a76b714f470eb2402652"
    },
    {
        "answer_id": 3484391918,
        "content": "推荐一个大模型的评测榜单，llm arena leaderboard。上面列举了大部分的开源、闭源的大模型。主流的都在那里。这个网站通过人工对比模型生成结果的方式来确定模型之间的好坏，并且最终用游戏elo分数的方式对模型进行排名。所以除了你可以了解到有哪些模型，模型是否开源、是否可以商用，还可以看到模型的性能怎么样。目前开源里比较强的模型是command r+、llama3、mistral、qwen、yi。",
        "voteup_count": 1,
        "updated_time": "2024-05-01 08:36:41",
        "question_id": 623672939,
        "user_id": "d93bc893ade3e64f1f05070490a2effe"
    },
    {
        "answer_id": 3486114603,
        "content": "呕心沥血整理国内外30个热门开源大模型，如下表所示：详情请查看：全球热门大模型架构配置信息汇总国内外 30 个热门大模型的架构的图文解析汇总",
        "voteup_count": 2,
        "updated_time": "2024-05-04 13:30:01",
        "question_id": 623672939,
        "user_id": "18dd8357bb937263823209ff8d4d299a"
    },
    {
        "answer_id": 3485942071,
        "content": "AI21实验室的Jurassic-2亚马逊的AlexaTMAnthropic的Claude百度的Ernie 3.0 Titan彭博的BloombergGPTCerebras的Cerebras-GPTDatabricks&Mosaic ML的DBRX谷歌DeepMind的Gopher、Chinchilla、Gemini、GemmaEleutherAI的GPT-J和GPT-NeoX谷歌的BERT、T5、XLNet、GLaM、LaMDA、PaLM、Minerva华为的PanGu-ΣIndependent的Neuro-samaLAION的OpenAssistantHugging Face的BLOOMMeta的OPT、Galactica、LLaMA微软的Phi-2微软跟英伟达合作的Megatron-Turing NLGMistral AI的Mistral 7B、Mixtral 8x7BOpenAI的GPT1~4RWKV的Eagle 7BTechnology Innovation Institute的Falcon、Falcon 180BxAI的Grok-1Yandex的YaLM 100B",
        "voteup_count": 1,
        "updated_time": "2024-05-03 04:24:01",
        "question_id": 623672939,
        "user_id": "f8751bbee13af508a95bf9228305eb11"
    },
    {
        "answer_id": 3485623411,
        "content": "国外开源大模型：1. LLaMA 2：由Meta（Facebook的母公司）开源，这个模型系列提供了不同大小的变体，从7亿到700亿参数，支持多种自然语言处理任务。2. Grok：由AI21 Labs开源，这是一个3140亿参数的模型，专注于文本生成和理解任务，尽管对计算资源要求较高。3. Falcon：由阿拉伯技术创新研究所开源，提供了不同规模的模型，以处理各种自然语言理解和生成任务。国内开源大模型：1. 书生・浦语（InternLM）：由上海人工智能实验室与商汤科技等联合发布，提供了7B和104B参数规模的模型，支持广泛的自然语言处理任务，并提供商用许可。2. ChatGLM2：由智谱AI发布，是一个6B参数的中英双语对话模型，支持对话生成和文本理解任务。3. 盘古α：“盘古α”是由鹏城实验室联合北京大学等机构发布的业界首个全开源2000亿参数中文预训练语言模型，具备很强的小样本学习能力，适用于多种文本生成领域。开源大模型为研究人员和开发者提供了强大的工具，有助于推动自然语言处理技术的发展和应用。",
        "voteup_count": 1,
        "updated_time": "2024-05-02 18:20:19",
        "question_id": 623672939,
        "user_id": "d7e47105949f70cd7081e4fa608e0198"
    },
    {
        "answer_id": 3483742873,
        "content": "开源大模型近年来在人工智能领域发展迅速，吸引了众多研究者和开发者关注。简单整理了一些知名的开源大模型，涵盖了自然语言处理、计算机视觉、多模态等领域：### 自然语言处理（NLP）1. **BERT（Bidirectional Encoder Representations from Transformers）**   - 来源：Google   - 开源链接：[GitHub](https://github.com/google-research/bert)   - 特点：基于Transformer的双向预训练模型，广泛应用于各种NLP任务。2. **RoBERTa（A Robustly Optimized BERT Pretraining Approach）**   - 来源：Facebook AI Research (FAIR)   - 开源链接：[GitHub](https://github.com/pytorch/fairseq/tree/master/examples/roberta)   - 特点：对BERT进行了优化，通过更大规模的训练数据和更长的训练时间，提升了性能。3. **GPT系列（Generative Pre-trained Transformer）**   - 来源：OpenAI   - 开源版本示例：[EleutherAI的GPT-Neo](https://github.com/EleutherAI/gpt-neo) 和 [GPT-J](https://github.com/eleutherai/jumbo)   - 特点：强大的生成能力，尤其是GPT-3展示了惊人的文本生成和任务适应性，但GPT-3本身并未完全开源，只有部分变体和研究者复现的版本公开。4. **T5（Text-to-Text Transfer Transformer）**   - 来源：Google   - 开源链接：[GitHub](https://github.com/google-research/text-to-text-transfer-transformer)   - 特点：统一的文本到文本框架，适用于多种NLP任务，通过改变输入输出格式即可适应不同任务。5. **ALBERT（A Lite BERT）**   - 来源：Google   - 开源链接：[GitHub](https://github.com/google-research/albert)   - 特点：在BERT基础上进行了参数量减少的优化，使得模型更轻量，同时保持了强大的性能。### 计算机视觉（CV）1. **YOLO（You Only Look Once）系列**   - 来源：Joseph Redmon等人   - 开源链接：[YOLOv5](https://github.com/ultralytics/yolov5) （非官方升级版）   - 特点：实时目标检测模型，速度快且准确，适用于物体识别和定位。2. **DETR（DEtection TRansformer）**   - 来源：Facebook AI Research (FAIR)   - 开源链接：[GitHub](https://github.com/facebookresearch/detr)   - 特点：首次将Transformer架构应用于端到端的目标检测任务，引入了新的训练范式。### 多模态1. **M6（M6: A Large-Scale Multimodal Pretrained Model）**   - 来源： DAMO Academy   - 开源链接：[GitHub](https://github.com/damo-cv/M6)   - 特点：一个大规模的多模态预训练模型，整合了文本、图像、语音等多种模态信息。2. **CLIP（Contrastive Language-Image Pre-training）**   - 来源：OpenAI   - 开源链接：[GitHub](https://github.com/openai/CLIP)   - 特点：通过对比学习，在图像和文本之间建立关联，实现了图像和文本的零样本迁移学习。这些模型代表了当前大模型研究的前沿，不仅推动了自然语言处理、计算机视觉等领域的进步，也促进了跨模态理解和生成技术的发展。值得注意的是，随着技术的快速迭代，新的开源大模型也在不断涌现。",
        "voteup_count": 1,
        "updated_time": "2024-04-30 14:23:55",
        "question_id": 623672939,
        "user_id": "b4dd5831f5fff8cbeac46aacfc66353a"
    },
    {
        "answer_id": 3478224789,
        "content": "大型语言模型（LLMs）无疑是此次AI革命的关键，它们基于Transformer架构，通过预训练大量文本数据，获得惊人的对话和任务处理能力。尽管如此，目前备受欢迎的大模型，诸如ChatGPT和Bard，都建立在专有的闭源基础之上，这无疑限制了它们的使用，并导致技术信息的透明度不足。然而，开源AI大模型（LLMs）正逐渐崭露头角，它们不仅增强了数据的安全性和隐私保护，还为用户节省了成本，减少了对外部依赖，实现了代码的透明性和模型的个性化定制。更重要的是，开源LLMs积极支持社区的发展，推动着整个领域的创新和发展！本文将详细介绍10个顶级开源LLMs大模型！记得提前收藏！1. LLaMA 3来源网络近期，Meta 重磅发布发布两款开源Llama 3 8B与Llama 3 70B模型，供外部开发者免费使用。Meta表示，Llama 3 8B和Llama 3 70B是目前同体量下，性能最好的开源模型。LLaMA 无疑是开源模型的顶流，国内好多大模型都是基于它实现的！它通过人类反馈的强化学习 （RLHF） 进行了微调。它是一种生成文本模型，可以用作聊天机器人，并且可以适应各种自然语言生成任务，包括编程任务。从其分享的基准测试可以看出，Llama 3 400B+ 的实力几乎媲美 Claude 超大杯以及 新版 GPT-4 Turbo，虽然仍有一定的差距，但足以证明其在顶尖大模型中占有一席之地。模型下载链接：https://llama.meta.com/llama-downloads/GitHub项目地址：https://github.com/meta-llama/llama32. Phi-3Phi 是由微软 AI 研究院最新开发的一个开源「小型语言模型」，可商用，卖点是小，需要的资源少。模型包括 Phi-3-Mini、Phi-3-Small 和 Phi-3-Medium。其中，Phi-3-Mini 最小，只有 3.8B 的参数，但在重要的基准测试中的表现可与大型模型如 Mixtral 8x7B 和 GPT-3.5 媲美而更大的 Small 和 Medium ，在扩展的数据集的加持下就更牛逼了。来源网络《Phi-3 技术报告：一个能跑在手机上的大模型》：https://arxiv.org/abs/2404.14219链接：https://huggingface.co/microsoft（待上线）3. BERT来源网络BERT是早期大型语言模型的代表作，底层技术基于Transformer架构。谷歌于2017年开发并在《注意力是你所需要的一切》中介绍了它。作为测试Transformer潜力的首批实验之一，BERT在2018年开源后迅速在自然语言处理任务中取得先进性能。由于其创新和开源性质，BERT成为最受欢迎的LLMs之一，有数千种开源、免费和预训练的模型用于各种用例。不可否认的是，近年来谷歌对开源大模型的态度变得较为冷漠。链接：https://github.com/google-research/bert4. Falcon 180B来源网络Falcon 40B已在开源LLM社区获得高度评价，位列Hugging Face排行榜首位。新推出的Falcon 180B展示了专有与开源LLM间差距正快速缩小。2023年9月，阿联酋技术创新研究所宣布，Falcon 180B正在接受1800亿参数的训练，其计算能力令人瞩目，已在多种NLP任务中超越LLaMA 2和GPT-3.5。尽管免费供商业和研究使用，但运行Falcon 180B需大量计算资源。5. BLOOM来源网络BLOOM 于 2022 年推出，经过与来自 70+ 个国家的志愿者和 Hugging Face 的研究人员为期一年的合作项目，BLOOM 是一种自回归LLM训练，可使用工业规模的计算资源在大量文本数据上继续从提示开始文本。BLOOM 的发布标志着生成式 AI 开源化的重要里程碑。BLOOM 拥有 1760 亿个参数，是最强大的开源之一LLMs，能够以 46 种语言和 13 种编程语言提供连贯准确的文本。其透明度是其核心特点，源代码和训练数据均可访问，方便运行、研究和改进。此外，BLOOM可通过Hugging Face生态系统免费使用。链接：http://bigscience.huggingface.co6. XGen-7B来源网络多家公司纷纷参与LLM竞赛，Salesforce于2023年7月推出XGen-7BLLM。多数开源LLMs提供有限信息的大答案，而XGen-7B支持更长的上下文窗口。其高级版本XGen-7B-8K-base拥有8K上下文窗口。虽然XGen使用7B参数进行训练，远少于其他强大LLMs，但效率颇高。尽管尺寸较小，XGen表现卓越，适用于商业和研究，但XGen-7B-{4K，8K}-inst版本除外，该版本经教学数据和RLHF训练，以非商业许可发布。7. GPT-NeoX 和 GPT-J来源网络GPT-NeoX和GPT-J是由EleutherAI实验室开发的GPT的开源替代品，分别拥有200亿和60亿参数。尽管与其他超过1000亿参数的LLMs相比，它们依然能提供高精度结果。它们经过22个高质量数据集的训练，适用于多个领域和用例。与GPT-3不同，GPT-NeoX和GPT-J未接受RLHF训练。可用于各种自然语言处理任务，包括文本生成、情感分析以及研究和营销活动开发，并可通过NLP Cloud API免费获取。8. Vicuna13-B图源网络Vicuna-13B是一个开源对话模型，基于LLaMa 13B进行微调，训练数据来自ShareGPT收集的用户共享对话。作为一款智能聊天机器人，Vicuna-13B在多个行业有广泛应用，如客户服务、医疗保健、教育、金融和旅游/酒店。初步评估表明，Vicuna-13B在90%以上的案例中优于LLaMa2和Alpaca等其他模型。9. Mistral 7B来源官网Mistral 7B v0.2 是 Mistral-7B-Instruct-v0.2 的基础预训练模型，属于「Mistral Tiny」系列。此次更新主要提升上下文长度至32K，Rope Theta设为1e6，并取消滑动窗口。链接：https://mistral.ai/10. 零一万物Yi系列模型是01.AI推出的下一代开源大型语言模型，旨在成为双语语言模型领域的佼佼者。该模型利用3T多语言语料库进行训练，具备出色的语言理解、常识推理和阅读理解等能力。据2024年1月数据显示，Yi-34B-Chat模型在AlpacaEval排行榜上位列第二，仅次于GPT-4 Turbo，且优于其他LLM如GPT-4、Mixtral、Claude。此外，Yi-34B模型在Hugging Face Open LLM Leaderboard和C-Eval等各种基准测试中，均排名第一，超越所有现有开源模型，如Falcon-180B、Llama-70B、Claude。这些成绩使Yi系列模型成为全球最强大的LLM模型之一，展现出广阔的应用前景。论文：https://arxiv.org/abs/2403.04652链接：https://github.com/01-ai/Yi如何选择适合您需求的开源LLM开源LLM领域正迅速扩大，全球开发人员合作升级和优化LLMs版本，性能差距有望缩小。选择开源LLM时，需考虑以下因素，找到最适合您需求的LLM：您的目标是什么？注意许可限制，选择适合商业用途的LLM。为什么需要LLM？考虑是否有必要使用LLM实现您的想法，避免不必要的花费。您需要的精度如何？大型LLMs通常更准确。如需要高精度，可选择如LLaMA或Falcon的大型模型。您愿意投入多少资金？大型模型需要更多资源。考虑基础设施和云提供商成本。能否使用预训练模型？如适合预训练模型的使用场景，可节省时间和金钱。结语IT行业的历史告诉我们，开源是软件领域里的一大潮流，它推动了应用生态的繁荣。但自从GPT3出现后，Open AI却选择了闭源，这使得开源大模型的发展似乎停滞在了GPT3.5的阶段。不过，业界还是有一些口碑不错的前沿开源大模型，比如Meta的LLaMA3、Mistral的Mistral 8x7B和零一万物的Yi-34B等。虽然开源模式在构建生态方面很给力，但因为算力和算法等方面的限制，它在大模型领域的发展还充满了不确定性。甚至有人担心，开源模型会逐渐落后。好在Llama 3的出现，给开源模型带来了一线希望。这场关于开源与闭源的辩论还在继续，咱们就拭目以待，看看开源和闭源将如何共同塑造AI的未来吧！",
        "voteup_count": 1,
        "updated_time": "2024-04-25 14:47:21",
        "question_id": 623672939,
        "user_id": "2e20e9051b943f64b52e979069aff153"
    },
    {
        "answer_id": 3486236868,
        "content": "安卓。",
        "voteup_count": 1,
        "updated_time": "2024-05-03 13:11:00",
        "question_id": 623672939,
        "user_id": "aaefec26e16725e596c84284c33fd64a"
    },
    {
        "answer_id": 3483037709,
        "content": "我说一个：羊驼3",
        "voteup_count": 0,
        "updated_time": "2024-04-29 21:28:58",
        "question_id": 623672939,
        "user_id": "7e2468fe0b1f140b0115478bb11598b0"
    },
    {
        "answer_id": 3480841829,
        "content": "上次写过一篇《本地部署搭建自己的ChatGPT》，主要是使用套壳软件+APIKey搭建本地大模型。这次主要是利用开源大模型在本地搭建聊天平台（或者本地知识库）。优点：私密；免费；灵活（各个模型随便用）。缺点：速度慢；非最新；中文效果不一定好。背景需求1.本人没有GPU，只能在CPU电脑上搭建，所以速度比较慢，仅作为练手或体验。2.使用的linux（麒麟）操作系统，基本上是源码部署。3.局域网使用，没法连接互联网。（声明：网上有比较多的教程、这里更多的是记录本人搭建过程）本人使用Ollama、LMStudio都体验过，这里以Ollama+AnythingLLM实现。一、安装Ollama（其实官网中有详细介绍）项目地址：https://ollama.com/download/1.在联网电脑中：sudo curl -L https://ollama.com/download/ollama-linux-amd64 -o /usr/bin/ollama相当于把ollama下载到/usr/bin/ollama中2.将ollama拷贝到离线电脑相同位置，并sudo chmod +x /usr/bin/ollama赋予相应权限3.添加Ollama作为一个系统服务：创建用户：sudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama创建服务（ollama.service），放在/etc/systemd/system/下：[Unit]Description=Ollama ServiceAfter=network-online.target[Service]ExecStart=/usr/bin/ollama serveUser=ollamaGroup=ollamaRestart=alwaysRestartSec=3[Install]WantedBy=default.target启动服务：sudo systemctl daemon-reloadsudo systemctl enable ollamasudo systemctl start ollama4.在命令行窗口中输入：ollama，如果出现下图表示安装成功(显示了一些几本命令，如create、run、pull等)二、使用Ollama1.使用官网的模型（library）ModelParametersSizeDownloadLlama 38B4.7GBollama run llama3Llama 370B40GBollama run llama3:70bPhi-33,8B2.3GBollama run phi3Mistral7B4.1GBollama run mistralNeural Chat7B4.1GBollama run neural-chatStarling7B4.1GBollama run starling-lmCode Llama7B3.8GBollama run codellamaLlama 2 Uncensored7B3.8GBollama run llama2-uncensoredLLaVA7B4.5GBollama run llavaGemma2B1.4GBollama run gemma:2bGemma7B4.8GBollama run gemma:7bSolar10.7B6.1GBollama run solar直接在命令行窗口输入 ollama run 模型名（如llama3），下图是使用qwen0.5b的界面在这个界面就可以进行问答了。若要退出，直接输入/bye即可2.使用其他模型（这里主要使用GGUF格式的模型）下载GGUF模型，下载地址：国内下载大模型的极速通道：替代 Huggingface 的优选方案下载GGUF格式文件后，如下载Llama3-8B-Chinese-Chat.q4_k_m.GGUF，在这个文件的同目录下新建Modelfile文件，内容如下：FROM ./Llama3-8B-Chinese-Chat.q4_k_m.GGUFTEMPLATE \"\"\"{{ if .System }}<|start_header_id|>system<|end_header_id|>{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>{{ .Response }}<|eot_id|>\"\"\"PARAMETER stop \"<|start_header_id|>\"PARAMETER stop \"<|end_header_id|>\"PARAMETER stop \"<|eot_id|>\"PARAMETER stop \"<|reserved_special_token\"在命令行窗口，使用ollama  create  example  -f  Modelfile（example替换为你想使用的名字）使用ollama list可以查看刚才创建的模型了使用ollama run example可以对话了三、使用AnythingLLM界面看到下图的界面总是不舒服，能不能也有ChatGPT一样的界面呢，答案是肯定的，且不止一种。AnythingLLM的部署运行可以参考上一篇文章（江南水乡：本地部署搭建自己的ChatGPT）要在AnythingLLM中使用ollama，只需要作如下设置：注意：Ollama Base URL:http://127.0.0.1:11434，选择刚才自己创建的模型（如qwen:0.5b）就可以使用了。",
        "voteup_count": 0,
        "updated_time": "2024-04-27 22:47:32",
        "question_id": 623672939,
        "user_id": "f5d276802f5b52872235b2171b77e0b7"
    },
    {
        "answer_id": 3262478457,
        "content": "# Top 1: langchain-ai/langchain作者：langchain-ai简介：⚡ Building applications with <em>LLMs</em> through composability ⚡关注人数：65618标签：Python链接：https://www.github.com/langchain-ai/langchain# Top 2: nomic-ai/gpt4all作者：nomic-ai简介：gpt4all: open-source <em>LLM</em> chatbots that you can run anywhere关注人数：53467标签：llm-inference、C++链接：https://www.github.com/nomic-ai/gpt4all# Top 3: binary-husky/gpt_academic作者：binary-husky简介：为ChatGPT/GLM提供实用化交互界面，特别优化论文阅读/润色/写作体验，模块化设计，支持自定义快捷按钮&amp;函数插件，支持Python和C++等项目剖析&amp;自译解功能，PDF/LaTex论文翻译&amp;总结功能，支持并行问询多种<em>LLM</em>模型，支持chatglm2等本地模型。兼容文心…关注人数：44263标签：chatglm-6b、chatgpt、Python、academic、gpt-4、large-language-models链接：https://www.github.com/binary-husky/gpt_academic",
        "voteup_count": 0,
        "updated_time": "2023-10-24 14:48:47",
        "question_id": 623672939,
        "user_id": "70a71f14ca46c4ec7bb94385833ea1f5"
    }
]