[
    {
        "answer_id": 3485806241,
        "content": "这个问题很难有个标准答案，会取决于模型的量化和推理框架。先说我自己的选择吧：Qwen1.5-32B/Yi-34B int4，或者用 LM Studio 运行 Qwen-72B 的 gguf q8_0 量化。楼上有不少小伙伴分享了自己实测的情况，在这里推荐给大家一个工具，是 HuggingFace 官方推出的用于计算模型所需显存的：https://huggingface.co/spaces/hf-accelerate/model-memory-usage根据工具的介绍：此工具计算在HuggingFace上托管的模型进行训练和推理时所需的显存（vRAM）。模型所需的最低推荐 vRAM 是以其“最大层”的大小来标注的，而进行模型训练时的显存需求大约是该模型大小的四倍（使用 Adam 算法）。这些计算结果的准确率通常在几个百分点之内，例如 bert-base-cased 的实际大小为 413.68 MB，而计算器的估算为 413.18 MB。根据 EleutherAI 的研究，进行模型推理时，可能需要额外增加高达 20% 的显存。目前，该工具支持所有使用 transformers 和 timm 框架的托管模型。用法也很简单，只要输入模型名称/地址，选择所需计算的模型量化精度，点击计算即可：比如，计算 Qwen1.5-32B-Chat[1]所需的 VRAM 可得，int4 需要 15 GB 显存，int8 就需要 30G 显存了，所以 4090 是可以直接运行 Qwen1.5-32B-Chat 的 int4 量化模型的。而 Qwen1.5-72B-Chat 的 int4 则需要 33.39 GB 显存，也就超出 4090 24GB 了：当然，即便超出了 24GB 显存，也不是没有办法突破，比较傻瓜的方式就是使用 LM Studio ，将一部分层加载到内存中来运行 GGUF 格式的量化模型。在 LM Studio 中，可以手动调整内存/显存的负载比例，以 Qwen1.5-72B-Chat-GGUF[2]为例，在 LM Studio 的硬件设置中，最大可以把 GPU 负载调到 42 层，再大就会爆显存了：这样就可以混合利用 CPU/GPU 能力来进行模型推理：以上，供参考，如有错漏，也请评论指出。评论区有人问生成 tokens 的速度，补两张图：",
        "voteup_count": 23,
        "updated_time": "2024-05-03 00:13:13",
        "question_id": 649233834,
        "user_id": "fc91ae5ff90c3248f0fd00102d6c0f08"
    },
    {
        "answer_id": 3452284325,
        "content": "4090显存共有24GB，一般认为模型半精度（fp16）加载占用的显存1B～2G。通过llama.cpp或者上交的PowerInfer等类似的推理框架进行量化，最高可以压缩成4bit，所以1B～0.5G。因此理论上可以加载约48B的模型，但量化不是简单的比例关系，而且模型加载后推理也会占用一部分显存，因此实际情况下会小于这个值。目前这个尺寸的开源大模型有：Falcon-40B、 Vicuna-33B，国内有yi-34B等",
        "voteup_count": 58,
        "updated_time": "2024-04-02 18:55:28",
        "question_id": 649233834,
        "user_id": "c1e444d799157725fca331ab1ed4581e"
    },
    {
        "answer_id": 3484858849,
        "content": "Llama2 13B （Chinese指令精调）半精度运行，显存占用基本已满，推理速度和4位量化比显著下降。Llama3 8B（原生、Chinese指令精调）半精度运行，显存占用一半，推理速度飞起来。chatglm3 6B 半精度运行，显存占用一半，推理速度流畅。微调，lora或ptuning v2，显存基本占满，demo提供样本，时间1小时。Qwen1.5 14B，半精度运行，显存占用快满，推理速度流畅。Qwen1.5 32B 8位量化运行，这是目前推理跑过的最大参数，推理速度显著降低。Qwen1.5 72B，原始模型转换gguf，128G内存直接爆了。直接下载4位量化运行推理，显存提示需要32GB，运行失败。最终得到：Qwen1.5 32B 8位量化，是我用4090跑过的最大模型。chatglm3 6B，是我跑过7B以内中文输出结果最好模型。Llama3 8B的中文理解输出，垃圾！",
        "voteup_count": 44,
        "updated_time": "2024-05-01 19:44:26",
        "question_id": 649233834,
        "user_id": "9e0525b86b2aa3abda4c79b8e2e3e2c6"
    },
    {
        "answer_id": 3447793000,
        "content": "2024年4月19日，本地模型即将迎来一波更新，首先登场的是Llama3 8B版本，坐等13B/33B GPTQ版本发布。Meta 公司今天发布新闻稿，宣布推出下一代大语言模型 Llama 3，共有 80 亿和 700 亿参数两种版本，号称是最强大的开源大语言模型。Meta 发布 Llama 3，号称是最强大的开源大语言模型2024年3月29日最适合4090跑的大语言模型是30/33/34B参数4bit量化的GPTQ模型，其次是13B参数的8bit量化的GPTQ模型。挑选模型最重要的就是参数量，比如llama2的13B参数模型跑分不如llama1的33B参数模型，另外，同一个模型的33B 4bit版本也大概率强于它的13B 8bit版本。开源大模型基本都是基于llama的，一般每个模型都会有跑分，根据跑分来挑选，国外的模型普遍中文能力差，不如国内几个大厂的。因此英文交流首选国外的，中文交流首选国产的。本人在用的是下面这几个，从跑分，评测和评论里面挑出来的比较好的，未来还会持续更新，这几个很快也会落伍。对最新的算法来说，大模型已经不是必需跑在显存里了，内存甚至虚拟内存都可以用来运行大模型，只是速度差了几个数量级，4090也可以运行70B原版llama跑着玩，日常使用就算了。",
        "voteup_count": 29,
        "updated_time": "2024-04-19 16:18:41",
        "question_id": 649233834,
        "user_id": "2766e95ce721900cbe6ef1ec333d2d28"
    },
    {
        "answer_id": 3445497657,
        "content": "OPT-175B。Offline inference：参见盛颖他们的ICML 2023 FlexGen，用长达数小时的延迟换吞吐。Interactive inference：参见上交的PowerInfer，利用了ReLU引起的稀疏性减少了显存压力。",
        "voteup_count": 43,
        "updated_time": "2024-03-27 17:47:27",
        "question_id": 649233834,
        "user_id": "ae7a37b4fb1d158f11b3b50b9c260e79"
    },
    {
        "answer_id": 3473593975,
        "content": "基本上就根据显存计算了，另外也要考虑性能，毕竟就算能运行某参数量大模型，如果每分钟跑1个字，这样虽然能运行，但是相当于没运行，所以我理解你说的运行是流畅运行，1:因此如果不考虑跑量化模型的话，24g显存可以流畅跑9B参数的模型，所以中文方面能运行的非量化模型为qwen1.5 7B,英文的话LLAMA3 7B2:考虑量化模型，毕竟参数量大还是王道，精度下降还是能部分容忍的，那么24g显存量化到int2中文方面能够流畅跑YI34B ，int4的话，差不多，在个人电脑方面30B左右参数已经相当不错了。",
        "voteup_count": 4,
        "updated_time": "2024-04-21 16:24:54",
        "question_id": 649233834,
        "user_id": "3c9684fbe7c031456d623cc8862b36cd"
    },
    {
        "answer_id": 3436912907,
        "content": "4090是24G显存，使用4int量化的模型每10亿参数需要0.5G显存，换算一下能运行480亿参数的模型",
        "voteup_count": 8,
        "updated_time": "2024-03-20 10:21:56",
        "question_id": 649233834,
        "user_id": "570aa59ba3fa655515997f4d1c915276"
    },
    {
        "answer_id": 3484836284,
        "content": "关于单张NVIDIA GeForce RTX 4090显卡能运行的最强开源大模型，这个问题并没有一个固定的答案，因为它取决于多个因素，如模型的复杂度、计算需求、内存需求等。单张NVIDIA RTX 4090显卡能够运行的较强开源大模型包括Meta发布的Llama 3模型。Llama 3提供了80亿和700亿参数两种版本，号称是最强大的开源大语言模型。该模型在多个关键基准测试中性能优于业界先进同类模型，尤其在代码生成等任务上实现了全面领先。Llama 3模型的训练数据超过15T token，是Llama 2数据集的7倍以上的规模，支持8K长文本输入，训练效率是Llama 2的3倍。此外，该模型在安全性方面也有显著提升，搭载了Llama Guard 2、Code Shield和CyberSec Eval 2等新一代的信任与安全工具。除了Llama 3，还有其他一些开源大模型也值得关注，如千问系列、glm系列、百川系列等。但具体哪个模型是最强，还需要根据实际场景和需求进行评估，因为不同的模型可能在不同任务上有不同的优势。值得注意的是，运行大型模型不仅需要考虑显卡的显存容量，还需要考虑模型的参数量、优化程度以及运行时的内存和CPU需求。因此，即使是4090显卡，运行70B参数级别的大模型也可能面临挑战，实际使用时可能需要选择适合自己硬件配置和应用需求的模型。RTX 4090是一款性能强大的显卡，具有大量的CUDA核心和高带宽显存，可以处理复杂的计算任务。然而，不同的开源大模型对硬件的要求各不相同，有的模型可能需要更高的计算能力和内存才能运行。目前，有一些知名的开源大模型，如GPT-4、TensorFlow 2.x和PyTorch Lightning等，它们都在不同领域表现出色。但是，要确定哪个模型是RTX 4090能运行的最强模型，需要考虑具体的模型和任务需求。如果你有一个特定的开源大模型需要运行，并且想知道RTX 4090是否足够强大，我建议你查阅该模型的官方文档或相关资源，以了解其硬件要求和性能需求。同时，你也可以参考其他用户的经验和反馈，以了解RTX 4090在该模型上的实际表现。需要注意的是，即使RTX 4090是一款强大的显卡，但在运行某些复杂的开源大模型时，可能仍然需要额外的硬件支持，如多个GPU、高性能CPU、大容量内存等。因此，在选择硬件时，你需要根据具体的任务需求进行评估和选择。Nvidia B100/B200/GB200 关键技术解读 - 知乎 (zhihu.com)大模型训练推理如何选择GPU？一篇文章带你走出困惑（附模型大小GPU推荐图） - 知乎 (zhihu.com)一文看懂英伟达A100、A800、H100、H800各个版本有什么区别？ - 知乎 (zhihu.com)AI核弹B200发布：超级GPU新架构30倍H100单机可训15个GPT-4模型，AI进入新摩尔时代 - 知乎 (zhihu.com)紧跟“智算中心”这波大行情！人工智能引领算力基建革命！ - 知乎 (zhihu.com)先进计算技术路线图（2023） - 知乎 (zhihu.com)建议收藏！大模型100篇必读论文 - 知乎 (zhihu.com)马斯克起诉 OpenAI：精彩程度堪比电视剧，马斯克与奥特曼、OpenAI的「爱恨纠缠史」 - 知乎 (zhihu.com)生物信息学必备网站大全 - 知乎 (zhihu.com)生物信息学简史 - 知乎 (zhihu.com2023第一性原理科研服务器、量化计算平台推荐 - 知乎 (zhihu.com)Llama-2 LLM各个版本GPU服务器的配置要求是什么？ - 知乎 (zhihu.com)人工智能训练与推理工作站、服务器、集群硬件配置推荐整理了一些深度学习，人工智能方面的资料，可以看看机器学习、深度学习和强化学习的关系和区别是什么？ - 知乎 (zhihu.com)人工智能 (Artificial Intelligence, AI)主要应用领域和三种形态：弱人工智能、强人工智能和超级人工智能。买硬件服务器划算还是租云服务器划算？ - 知乎 (zhihu.com)深度学习机器学习知识点全面总结 - 知乎 (zhihu.com)自学机器学习、深度学习、人工智能的网站看这里 - 知乎 (zhihu.com)2023年深度学习GPU服务器配置推荐参考（3） - 知乎 (zhihu.com)多年来一直专注于科学计算服务器，入围政采平台，H100、A100、H800、A800、L40、L40S、RTX6000 Ada，RTX A6000，单台双路256核心服务器等。",
        "voteup_count": 2,
        "updated_time": "2024-05-01 19:07:10",
        "question_id": 649233834,
        "user_id": "fdd23d5babd02447757c668338a4cd59"
    },
    {
        "answer_id": 3439923156,
        "content": "实测，13b模型！20b要40g！如果算上量化后的模型，20b，但量化后的20b还是20b么？单张4090的显存非常尴尬，不如3090组link来的好。",
        "voteup_count": 7,
        "updated_time": "2024-03-22 18:58:35",
        "question_id": 649233834,
        "user_id": "64c8b9567b2e27040b6000747f97942d"
    },
    {
        "answer_id": 3473294455,
        "content": "像ChatGPT、Claude.ai和phind这样的聊天机器人非常有用，但可能并不总是希望的问题或敏感数据由外部应用程序处理。在平台上尤其如此，在这些平台上，你的互动可能会被人类审查，并以其他方式用于帮助训练未来的模型。一种解决方案是下载一个大型语言模型（LLM）并在自己的机器上运行它。这样一来，外部公司就永远无法访问的数据。这也是尝试一些新专业模型的一个快速选择，如Meta最近宣布的Code Llama系列模型，这些模型经过了编码调整，以及Seamless M4T，旨在实现文本到语音和语言翻译。运行自己的LLM听起来可能很复杂，但有了正确的工具，它会出奇地容易。而且许多型号的硬件要求并没有想象中那么疯狂。我已经在两个系统上测试了本文中提供的选项：一个是带有英特尔i9处理器、64GB RAM和英伟达GeForce 12GB GPU（可能没有参与运行该软件的大部分工作）的戴尔PC，另一个是只有16GB RAM的M1芯片的Mac。请注意，可能需要一点研究才能找到一个在的任务中运行良好并在桌面硬件上运行的模型。而且，很少有人能像你习惯的ChatGPT（尤其是GPT-4）或Claude.ai这样的工具那样好。命令行工具LLM的创建者Simon Willison在上个月的一次演示中认为，即使本地模型的响应是错误的，运行它也是值得的：在你的笔记本电脑上运行的[一些]会产生疯狂的幻觉——我认为这实际上是运行它们的一个很好的理由，因为在你的电脑上运行弱型号是了解这些东西如何工作及其局限性的一种更快的方式。同样值得注意的是，开源模型可能会不断改进，一些行业观察人士预计，它们与商业领袖之间的差距会缩小。1.使用GPT4All运行本地聊天机器人如果你想要一个在本地运行、不会在其他地方发送数据的聊天机器人，ﾠGPT4Allﾠ提供了一个易于设置的桌面客户端供下载。它包括在自己的系统上运行的型号的选项，还有Windows、macOS和Ubuntu的版本。当第一次打开GPT4All桌面应用程序时，将看到下载大约10个（截至本文撰写之时）可以在本地运行的模型的选项。其中包括Meta AI的模型Llama-2-7B聊天。如果你有API密钥，你也可以设置OpenAI的GPT-3.5和GPT-4（如果你有访问权限）供非本地使用。GPT4All接口的模型下载部分一开始有点令人困惑。在我下载了几个模型后，我仍然可以选择全部下载。这表明下载不起作用。然而，当我检查下载路径时，模型就在那里。GPT4All中模型下载接口的一部分。一旦我打开应用程序的使用部分，我下载的模型就会自动出现。一旦建立了模型，聊天机器人界面本身就很干净，易于使用。方便的选项包括将聊天复制到剪贴板并生成响应。GPT4All聊天界面简洁易用还有一个新的测试版LocalDocs插件，可以让你在本地与自己的文档“聊天”。可以在中启用Settings > Plugins 选项卡，将看到“LocalDocs插件（BETA）设置”标题和在特定文件夹路径创建集合的选项。该插件正在进行中，并且ﾠ文档ﾠ 警告说，即使LLM可以访问添加的专家信息，它仍可能“产生幻觉”（编造）。尽管如此，这是一个有趣的功能，随着开源模型变得更加强大，它可能会得到改进。除了聊天机器人应用程序，GPT4All还绑定了Python、Node和命令行界面（CLI）。还有一种服务器模式，可以让你通过一个结构非常像OpenAI的HTTP API与本地LLM交互。目标是通过更改几行代码，让你用本地LLM交换OpenAI的LLM。2.命令行启动LLMSimon Willison的LLM是我见过的在自己的机器上本地下载和使用开源LLM的更简单的方法之一。虽然确实需要安装Python来运行它，但不应该接触任何Python代码。如果你在Mac电脑上使用Homebrew，只需使用：brew install llm如果在Windows计算机上，请使用最喜欢的安装Python库的方式，例如：pip install llmLLM默认使用OpenAI模型，但可以使用插件在本地运行其他模型。例如，如果安装gpt4all插件，将可以从gpt4all访问其他本地模型。还有用于llama、MLC项目和MPT-30B的插件，以及其他远程模型。在命令行上安装一个插件，名称为llm Install model：llm install llm-gpt4all可以使用命令llm-models-list查看所有可用的型号——远程和已安装的型号，包括每种型号的简要信息。图片当要求LLM列出可用型号时显示。要将查询发送到本地LLM，语法如下：llm -m the-model-name \"Your query\"如何选择正确的模型？可以前往ﾠGPT4All主页ﾠ并向下滚动到与GPT4All兼容的模型的模型资源管理器。falcon-q4_0选项是一款评级较高的相对较小的型号，具有允许商业使用的许可证，所以我从那里开始。然后，我问了它一个类似于ChatGPT的问题，但没有发出单独的命令下载模型：llm -m ggml-model-gpt4all-falcon-q4_0 \"Tell me a joke about computer programming\"这是LLM用户体验如此优雅的一点：如果本地系统上不存在GPT4All模型，LLM工具会在运行查询之前自动为下载它。下载模型时，将在终端中看到进度条。LLM自动下载了我在查询中使用的模型这里有一个笑话：“程序员为什么关掉电脑？因为他想看看它是否还在工作！”——但事实上，这个查询确实起了作用。如果结果令人失望，那是因为模型性能或用户提示不足，而不是LLM工具。还可以为LLM中的模型设置别名，以便可以使用较短的名称来引用它们：llm aliases set falcon ggml-model-gpt4all-falcon-q4_0要查看所有可用的别名，请输入：llm aliases。这个ﾠMeta的Llama模型的LLM插件ﾠ需要比GPT4All多一点的设置。阅读上的详细信息ﾠLLM插件的GitHub回购。请注意，通用llama-2-7b-chat确实设法在我的工作Mac上运行了M1 Pro芯片，只有16GB的RAM。与为没有GPU的小型机器优化的GPT4All型号相比，它运行得相当慢，在我更健壮的家用电脑上表现更好。LLM还有其他功能，例如argument标志，使可以从以前的聊天中继续，并能够在Python脚本中使用它。9月初，该应用程序获得了ﾠ用于生成文本嵌入的工具，可用于搜索相关文档的文本含义的数字表示。可以在LLM网站上看到更多信息。Willison是流行的Python Django框架的联合创建者，他希望社区中的其他人能为LLM生态系统贡献更多插件。3.桌面上调用模型：OllamaOllama是一种比LLM更容易下载和运行模型的方法。然而，该项目仅限于macOS和Linux，直到2月中旬，Windows的预览版终于问世。我测试了Mac版本。图片通过点击安装是一种优雅的体验。尽管Ollama是一个命令行工具，但只有一个命令具有以下语法ollama run model-name名称。与LLM一样，如果模型还没有在的系统上，它将自动下载。可以在上查看可用型号的列表ﾠhttps://ollama.ai/library，截至本文撰写之时，它包括基于Llama的模型的几个版本，如通用Llama 2、Code Llama、针对某些编程任务进行微调的DeepSE的CodeUp，以及针对回答医学问题进行微调的meddlama2。这个ﾠOllama GitHub repo's README 文件包括一些型号规格的有用列表和建议，“你应该至少有8GB的RAM来运行3B型号，16GB来运行7B型号，32GB来运行13B型号。”在我的16GB RAM Mac上，7B Code Llama的性能出奇地快。它将回答有关的问题ﾠbash/zshﾠshell命令以及Python和JavaScript等编程语言。在Ollama终端窗口中运行Code Llama的外观尽管它是家族中最小的模型，但如果不完美地回答了一个R编码问题，这会让一些更大的模型感到困惑，那它还是很好的：“为ggplot2图写R代码，其中的条形图是钢蓝色的。”代码是正确的，只是其中两行代码中有两个额外的右括号，这很容易在我的IDE中找到。我怀疑更大的Code Llama本可以做得更好。Ollama还有一些附加功能，例如ﾠLangChainﾠ集成和使用PrivateGPT运行的能力，除非检查ﾠGitHub repo的教程页面。如果你在Mac电脑上，想使用Code Llama，你可以在终端窗口中运行它，每次有问题时都会把它拉出来。我期待着在我的家用电脑上使用Ollama Windows版本。4.与自己的文档聊天：H2OGPTH2O.aiﾠ一段时间以来，该公司一直致力于自动化机器学习，因此很自然地进入了聊天LLM领域。它的一些工具最好由熟悉该领域的人使用，但安装其测试版本的说明ﾠh20GPTﾠ聊天桌面应用程序快速而直接，即使对于机器学习新手来说也是如此。你可以访问web上的演示版本（显然不是使用系统本地的LLM），网址：ﾠgpt.h2o.ai。本地LLaMa模型基于VS代码文档回答问题无需添加自己的文件，就可以将该应用程序用作通用聊天机器人。或者，可以上传一些文档并询问有关这些文件的问题。兼容的文件格式包括PDF、Excel、CSV、Word、文本、标记等。测试应用程序在我的16GB Mac上运行良好，尽管较小型号的结果无法与带GPT-4的付费ChatGPT相比（一如既往，这是型号的函数，而不是应用程序的函数）。H2OGPT UI为知道自己在做什么的用户提供了一个专家选项卡，其中包含许多配置选项。这为更有经验的用户提供了尝试改进其结果的选项。如果你想对流程进行更多控制，并为更多型号提供选项，请下载完整的应用程序。对于带有GPU或仅带有CPU的系统，有适用于Windows和macOS的一键安装程序。请注意，我的Windows防病毒软件对Windows版本不满意，因为它没有签名。我熟悉H2O.ai的其他软件，代码在GitHub上也有，所以我愿意下载并安装它。Rob Mulla，现在在H2O.ai，发布了一个ﾠYouTube视频ﾠ关于在Linux上安装该应用程序。尽管该视频已经发布了几个月，应用程序用户界面似乎也发生了变化，但该视频仍然有有用的信息，包括关于H2O.ai LLM的有用解释。5.轻松但缓慢的数据聊天：PrivateGPTPrivateGPTﾠ还设计用于让使用自然语言查询自己的文档，并获得生成的人工智能响应。此应用程序中的文档可以包括几十种不同的格式。README确保数据“100%私有，任何时候都不会有数据离开的执行环境。可以在没有互联网连接的情况下获取文档并提出问题！”PrivateGPT的特点是脚本可以摄取数据文件，将其分割成块，创建“embeddings”（文本含义的数字表示），并将这些嵌入存储在本地Chroma矢量存储中。当你提出问题时，该应用程序会搜索相关文档，并将其发送给LLM以生成答案",
        "voteup_count": 2,
        "updated_time": "2024-04-21 10:28:51",
        "question_id": 649233834,
        "user_id": "a17c1d40f792435bb6deecb08e9817b2"
    },
    {
        "answer_id": 3485787674,
        "content": "全球移动互联网公司APUS与大模型创企新旦智能宣布，联手开源国内首个千亿参数的MoE（混合专家模型）APUS-xDAN大模型4.0，这也是国内首个可以在消费级显卡上运行的千亿MoE中英文大模型。APUS-xDAN-4.0（MoE）参数规模为1360亿，可在消费级显卡4090上运行，据APUS实测，其综合性能超过GPT-3.5，达到GPT-4的90%。数学能力上，测评基准GSM8K的测评得分为79，理解能力MMLU达到73分。项目地址：https://github.com/shootime2021/APUS-xDAN-4.0-moe?tab=readme-ov-file01.数学、推理能力碾压推理成本下降400%APUS-xDAN-4.0（MoE）在GitHub的页面显示了基准测评结果，其与Mixtral-8x7B（MoE）、Llama2-70B、Grok-1（MoE）进行了对比。其中衡量模型语言理解、知识和推理能力的基准测试MMLU中，APUS-xDAN-4.0（MoE）排名第一，超过了Grok-1（MoE）。在测试多步骤数学推理能力的单词问题集合测试GSM-9K以及MATH中，该模型得分均远高于其他三大模型。四项测试中，APUS-xDAN-4.0（MoE）在多学科任务的BIG-Bench-Hard测试中，得分为66.4，接近Mixtral-8x7B（MoE），低于Grok-1（MoE）的71.7分。其中，Mixtral-8x7B（MoE）由大模型创企Mistral AI于去年年底发布，并在多项基准测试中性能都基本达到GPT-3.5；Llama2-70B是去年7月Meta开源的Llama 2大模型系列中，参数规模最大的版本；Grok-1（MoE）为马斯克旗下AI创企xAI本月初开源的大模型，参数规模为3140亿参数，是目前开源大模型中参数规模之最。此外，在GitHub页面显示，APUS-xDAN-4.0（MOE）开源模型在“IQ-Quantized Tech”上量化为1.5位、2位和4位，可以在消费级显卡4090上运行。具体来说，APUS-xDAN 大模型4.0（MoE）采用GPT-4类似的MoE架构，特点是多专家模型组合，同时激活使用只有2个子模块，实际运行效率对比传统Dense同尺寸模型效率提升200%，推理成本下降400%。在实际部署中，研究人员通过进一步高精度微调量化技术，使得模型尺寸缩小500%。02.32个MoE Transformer块组成可处理多线程复杂需求在实际的效果中，APUS-xDAN-4.0（MoE）可以理解复杂需求，如撰写广告文案时，要求涵盖“火焰人”、固定口号、摇滚乐歌词等。该模型还可以找出段落中的实时性错误，并给出修改版本，包括品牌所属地区、语病等。APUS-xDAN-4.0（MOE）模型的架构特点为，主要由32个相同的MoE Transformer块组成，与普通Transformer块相比，MoE Transformer块的FFN层被MoE FFN层替换。张量经过门层计算每个专家模型的分数，根据专家分数从8个专家模型中选择Top-K专家。张量通过Top-K专家的输出进行聚合，从而得到MoE FFN层的最终输出。每个专家由3个线性层（Linear Layers）组成。APUS-xDAN-4.0的所有Norm Layer都是用RMSNorm，与开源大模型Llama的方式一致。在注意力层中，APUS-xDAN-4.0（MoE）中的QKV矩阵的Q矩阵形状为（4096,4096），K和V矩阵形状为（4096,1024）。▲APUS-xDAN-4.0（MoE）模型架构图03.APUS是投资方7B模型表现超GPT-4APUS-xDAN-4.0（MoE）背后的两家公司均来自国内，且APUS于今年3月投资了新旦智能。新旦智能成立于2023年5月，此前该公司发布的xDAN-7B-Global在用于衡量大模型在机器翻译任务性能的综合性评估指标MT-Bench中，性能表现仅次于GPT-4。▲MT-Bench排名今年3月，新旦智能完成千万级别天使轮融资，投资方包括APUS和AI投资人周弘扬。据了解，新旦智能的创始团队汇集了清华、伯克利等顶尖学府以及腾讯、Meta等头部科技玩家的员工，包括全球开源AI社区知名开发者、腾讯云架构师等。与此同时，这也是APUS在开源大模型领域的最新进展。今年2月初，APUS与深圳大学国家工程实验室联合开源了APUS大模型3.0伶荔。04.结语：更强开源MoE模型或加速AI开发及应用创新开源大模型对于全球大模型产业的技术进步与应用落地的重要性与日俱增，并逐渐显现出对标当下大模型顶流GPT-4的潜力。在这个趋势下，APUS与新旦智能联手，既开源了目前国内参数规模最大的MoE模型，同时降低了其部署成本，为更多开发者提供了应用大模型能力的可能性，这或许将进一步加速AI开发及应用创新。",
        "voteup_count": 1,
        "updated_time": "2024-05-02 22:16:49",
        "question_id": 649233834,
        "user_id": "dd0cf3f9b8809033760b59336b599f6d"
    },
    {
        "answer_id": 3479455451,
        "content": "运行普通模型70B应该很轻松，但是并发能力就没有了",
        "voteup_count": 1,
        "updated_time": "2024-04-26 15:24:52",
        "question_id": 649233834,
        "user_id": "ba77fc840468ddc03aa81159052e7734"
    },
    {
        "answer_id": 3481821078,
        "content": "目前4090可以运行的最好的中文模型应该是Qwen-1.5-Chat-32B-int4",
        "voteup_count": 8,
        "updated_time": "2024-04-28 19:39:08",
        "question_id": 649233834,
        "user_id": "57afb76045bf2489f53f8ac516898ac1"
    },
    {
        "answer_id": 3473695136,
        "content": "不能简单的算模型量与显存容量的关系，因为现在不少框架还可以利用内存。单4090，实测可以跑70B 的Llama3, 用的Ollama，一秒钟平均蹦出1.5 -2 个token，慢，但是确实能用。",
        "voteup_count": 5,
        "updated_time": "2024-04-21 18:11:18",
        "question_id": 649233834,
        "user_id": "5a51b769c3c5b3501288141cd2f01d01"
    },
    {
        "answer_id": 3437044514,
        "content": "单纯论参数量的话，可以跑的最大模型是01的34b模型的量化版本。但这个参数量大，实际效果不好，按我个人不全面的测试，千问1.5的14B的效果还要好一点。",
        "voteup_count": 4,
        "updated_time": "2024-03-20 11:52:30",
        "question_id": 649233834,
        "user_id": "2a4f63ec0aa052bd873bee4040953c85"
    },
    {
        "answer_id": 3486208846,
        "content": "我已经找到了单张4050能运行的最强开源大模型：llama 3 8b",
        "voteup_count": 0,
        "updated_time": "2024-05-03 12:31:50",
        "question_id": 649233834,
        "user_id": "bc90e9b328077fb51404c8e84e7b0a88"
    },
    {
        "answer_id": 3486202897,
        "content": "围棋软件katago，不要说单张4090，100张都可以把资源榨干。",
        "voteup_count": 0,
        "updated_time": "2024-05-03 12:23:53",
        "question_id": 649233834,
        "user_id": "84c481ead7c9a9e95ab365a2f2ed71c8"
    },
    {
        "answer_id": 3486118831,
        "content": "现在模型底座还是缺陷比较大，算力能耗要求太高，再等一两年，看看有没新算法。",
        "voteup_count": 0,
        "updated_time": "2024-05-03 10:42:56",
        "question_id": 649233834,
        "user_id": "9377f91fc232d54034a9287377143d7d"
    },
    {
        "answer_id": 3485800446,
        "content": "单张NVIDIA GeForce RTX 4090显卡能够运行的最强开源大模型取决于多个因素，包括模型的参数量、模型的优化程度、显卡的内存大小（4090通常拥有24GB的显存），以及模型运行时所需的具体计算资源。从搜索结果中可以看到，有多个开源大模型（LLM）被提及，例如Meta的Llama 3模型，它在性能上取得了显著的进展，并且有不同参数规模的版本，如8B（80亿参数）和70B（700亿参数）。Llama 3模型在多个基准测试中展现了优异的性能，并且支持长达8K的文本输入，具有改进的tokenizer和更大的词汇量。此外，还有如谷歌的Gemma模型、阿里巴巴的通义千问模型等，这些模型也提供了不同尺寸的版本，以适应不同硬件配置的需求。对于RTX 4090而言，由于其拥有较大的显存，理论上可以运行一些参数量较大的模型。然而，实际上能够运行的“最强”模型还需要考虑模型的优化程度和运行效率。例如，一些模型可能经过了特定的优化，使得它们能够在消费级硬件上更高效地运行。在实际应用中，用户可能需要根据自己的具体需求和模型的性能特点来选择最适合的模型。同时，由于开源模型的领域不断发展，可能会有新的模型发布，提供更高的性能或更适合特定任务的优化。综上所述，没有一个明确的答案指出哪个是单张RTX 4090能运行的最强开源大模型，因为这取决于上述多种因素。用户可以根据自己的硬件配置和需求，选择经过优化且适合自己使用场景的模型。",
        "voteup_count": 0,
        "updated_time": "2024-05-02 22:33:45",
        "question_id": 649233834,
        "user_id": "1fe4eac24b482c9d42c72dd662c2df2f"
    },
    {
        "answer_id": 3485806241,
        "content": "这个问题很难有个标准答案，会取决于模型的量化和推理框架。先说我自己的选择吧：Qwen1.5-32B/Yi-34B int4，或者用 LM Studio 运行 Qwen-72B 的 gguf q8_0 量化。楼上有不少小伙伴分享了自己实测的情况，在这里推荐给大家一个工具，是 HuggingFace 官方推出的用于计算模型所需显存的：https://huggingface.co/spaces/hf-accelerate/model-memory-usage根据工具的介绍：此工具计算在HuggingFace上托管的模型进行训练和推理时所需的显存（vRAM）。模型所需的最低推荐 vRAM 是以其“最大层”的大小来标注的，而进行模型训练时的显存需求大约是该模型大小的四倍（使用 Adam 算法）。这些计算结果的准确率通常在几个百分点之内，例如 bert-base-cased 的实际大小为 413.68 MB，而计算器的估算为 413.18 MB。根据 EleutherAI 的研究，进行模型推理时，可能需要额外增加高达 20% 的显存。目前，该工具支持所有使用 transformers 和 timm 框架的托管模型。用法也很简单，只要输入模型名称/地址，选择所需计算的模型量化精度，点击计算即可：比如，计算 Qwen1.5-32B-Chat[1]所需的 VRAM 可得，int4 需要 15 GB 显存，int8 就需要 30G 显存了，所以 4090 是可以直接运行 Qwen1.5-32B-Chat 的 int4 量化模型的。而 Qwen1.5-72B-Chat 的 int4 则需要 33.39 GB 显存，也就超出 4090 24GB 了：当然，即便超出了 24GB 显存，也不是没有办法突破，比较傻瓜的方式就是使用 LM Studio ，将一部分层加载到内存中来运行 GGUF 格式的量化模型。在 LM Studio 中，可以手动调整内存/显存的负载比例，以 Qwen1.5-72B-Chat-GGUF[2]为例，在 LM Studio 的硬件设置中，最大可以把 GPU 负载调到 42 层，再大就会爆显存了：这样就可以混合利用 CPU/GPU 能力来进行模型推理：以上，供参考，如有错漏，也请评论指出。评论区有人问生成 tokens 的速度，补两张图：",
        "voteup_count": 48,
        "updated_time": "2024-05-03 00:13:13",
        "question_id": 649233834,
        "user_id": "fc91ae5ff90c3248f0fd00102d6c0f08"
    },
    {
        "answer_id": 3452284325,
        "content": "4090显存共有24GB，一般认为模型半精度（fp16）加载占用的显存1B～2G。通过llama.cpp或者上交的PowerInfer等类似的推理框架进行量化，最高可以压缩成4bit，所以1B～0.5G。因此理论上可以加载约48B的模型，但量化不是简单的比例关系，而且模型加载后推理也会占用一部分显存，因此实际情况下会小于这个值。目前这个尺寸的开源大模型有：Falcon-40B、 Vicuna-33B，国内有yi-34B等",
        "voteup_count": 60,
        "updated_time": "2024-04-02 18:55:28",
        "question_id": 649233834,
        "user_id": "c1e444d799157725fca331ab1ed4581e"
    },
    {
        "answer_id": 3484858849,
        "content": "Llama2 13B （Chinese指令精调）半精度运行，显存占用基本已满，推理速度和4位量化比显著下降。Llama3 8B（原生、Chinese指令精调）半精度运行，显存占用一半，推理速度飞起来。chatglm3 6B 半精度运行，显存占用一半，推理速度流畅。微调，lora或ptuning v2，显存基本占满，demo提供样本，时间1小时。Qwen1.5 14B，半精度运行，显存占用快满，推理速度流畅。Qwen1.5 32B 8位量化运行，这是目前推理跑过的最大参数，推理速度显著降低。Qwen1.5 72B，原始模型转换gguf，128G内存直接爆了。直接下载4位量化运行推理，显存提示需要32GB，运行失败。最终得到：Qwen1.5 32B 8位量化，是我用4090跑过的最大模型。chatglm3 6B，是我跑过7B以内中文输出结果最好模型。Llama3 8B的中文理解输出，垃圾！",
        "voteup_count": 57,
        "updated_time": "2024-05-01 19:44:26",
        "question_id": 649233834,
        "user_id": "9e0525b86b2aa3abda4c79b8e2e3e2c6"
    },
    {
        "answer_id": 3447793000,
        "content": "2024年4月19日，本地模型即将迎来一波更新，首先登场的是Llama3 8B版本，坐等13B/33B GPTQ版本发布。Meta 公司今天发布新闻稿，宣布推出下一代大语言模型 Llama 3，共有 80 亿和 700 亿参数两种版本，号称是最强大的开源大语言模型。Meta 发布 Llama 3，号称是最强大的开源大语言模型2024年3月29日最适合4090跑的大语言模型是30/33/34B参数4bit量化的GPTQ模型，其次是13B参数的8bit量化的GPTQ模型。挑选模型最重要的就是参数量，比如llama2的13B参数模型跑分不如llama1的33B参数模型，另外，同一个模型的33B 4bit版本也大概率强于它的13B 8bit版本。开源大模型基本都是基于llama的，一般每个模型都会有跑分，根据跑分来挑选，国外的模型普遍中文能力差，不如国内几个大厂的。因此英文交流首选国外的，中文交流首选国产的。本人在用的是下面这几个，从跑分，评测和评论里面挑出来的比较好的，未来还会持续更新，这几个很快也会落伍。对最新的算法来说，大模型已经不是必需跑在显存里了，内存甚至虚拟内存都可以用来运行大模型，只是速度差了几个数量级，4090也可以运行70B原版llama跑着玩，日常使用就算了。",
        "voteup_count": 29,
        "updated_time": "2024-04-19 16:18:41",
        "question_id": 649233834,
        "user_id": "2766e95ce721900cbe6ef1ec333d2d28"
    },
    {
        "answer_id": 3445497657,
        "content": "OPT-175B。Offline inference：参见盛颖他们的ICML 2023 FlexGen，用长达数小时的延迟换吞吐。Interactive inference：参见上交的PowerInfer，利用了ReLU引起的稀疏性减少了显存压力。",
        "voteup_count": 43,
        "updated_time": "2024-03-27 17:47:27",
        "question_id": 649233834,
        "user_id": "ae7a37b4fb1d158f11b3b50b9c260e79"
    },
    {
        "answer_id": 3473593975,
        "content": "基本上就根据显存计算了，另外也要考虑性能，毕竟就算能运行某参数量大模型，如果每分钟跑1个字，这样虽然能运行，但是相当于没运行，所以我理解你说的运行是流畅运行，1:因此如果不考虑跑量化模型的话，24g显存可以流畅跑9B参数的模型，所以中文方面能运行的非量化模型为qwen1.5 7B,英文的话LLAMA3 7B2:考虑量化模型，毕竟参数量大还是王道，精度下降还是能部分容忍的，那么24g显存量化到int2中文方面能够流畅跑YI34B ，int4的话，差不多，在个人电脑方面30B左右参数已经相当不错了。",
        "voteup_count": 4,
        "updated_time": "2024-04-21 16:24:54",
        "question_id": 649233834,
        "user_id": "3c9684fbe7c031456d623cc8862b36cd"
    },
    {
        "answer_id": 3436912907,
        "content": "4090是24G显存，使用4int量化的模型每10亿参数需要0.5G显存，换算一下能运行480亿参数的模型",
        "voteup_count": 8,
        "updated_time": "2024-03-20 10:21:56",
        "question_id": 649233834,
        "user_id": "570aa59ba3fa655515997f4d1c915276"
    },
    {
        "answer_id": 3484836284,
        "content": "关于单张NVIDIA GeForce RTX 4090显卡能运行的最强开源大模型，这个问题并没有一个固定的答案，因为它取决于多个因素，如模型的复杂度、计算需求、内存需求等。单张NVIDIA RTX 4090显卡能够运行的较强开源大模型包括Meta发布的Llama 3模型。Llama 3提供了80亿和700亿参数两种版本，号称是最强大的开源大语言模型。该模型在多个关键基准测试中性能优于业界先进同类模型，尤其在代码生成等任务上实现了全面领先。Llama 3模型的训练数据超过15T token，是Llama 2数据集的7倍以上的规模，支持8K长文本输入，训练效率是Llama 2的3倍。此外，该模型在安全性方面也有显著提升，搭载了Llama Guard 2、Code Shield和CyberSec Eval 2等新一代的信任与安全工具。除了Llama 3，还有其他一些开源大模型也值得关注，如千问系列、glm系列、百川系列等。但具体哪个模型是最强，还需要根据实际场景和需求进行评估，因为不同的模型可能在不同任务上有不同的优势。值得注意的是，运行大型模型不仅需要考虑显卡的显存容量，还需要考虑模型的参数量、优化程度以及运行时的内存和CPU需求。因此，即使是4090显卡，运行70B参数级别的大模型也可能面临挑战，实际使用时可能需要选择适合自己硬件配置和应用需求的模型。RTX 4090是一款性能强大的显卡，具有大量的CUDA核心和高带宽显存，可以处理复杂的计算任务。然而，不同的开源大模型对硬件的要求各不相同，有的模型可能需要更高的计算能力和内存才能运行。目前，有一些知名的开源大模型，如GPT-4、TensorFlow 2.x和PyTorch Lightning等，它们都在不同领域表现出色。但是，要确定哪个模型是RTX 4090能运行的最强模型，需要考虑具体的模型和任务需求。如果你有一个特定的开源大模型需要运行，并且想知道RTX 4090是否足够强大，我建议你查阅该模型的官方文档或相关资源，以了解其硬件要求和性能需求。同时，你也可以参考其他用户的经验和反馈，以了解RTX 4090在该模型上的实际表现。需要注意的是，即使RTX 4090是一款强大的显卡，但在运行某些复杂的开源大模型时，可能仍然需要额外的硬件支持，如多个GPU、高性能CPU、大容量内存等。因此，在选择硬件时，你需要根据具体的任务需求进行评估和选择。Nvidia B100/B200/GB200 关键技术解读 - 知乎 (zhihu.com)大模型训练推理如何选择GPU？一篇文章带你走出困惑（附模型大小GPU推荐图） - 知乎 (zhihu.com)一文看懂英伟达A100、A800、H100、H800各个版本有什么区别？ - 知乎 (zhihu.com)AI核弹B200发布：超级GPU新架构30倍H100单机可训15个GPT-4模型，AI进入新摩尔时代 - 知乎 (zhihu.com)紧跟“智算中心”这波大行情！人工智能引领算力基建革命！ - 知乎 (zhihu.com)先进计算技术路线图（2023） - 知乎 (zhihu.com)建议收藏！大模型100篇必读论文 - 知乎 (zhihu.com)马斯克起诉 OpenAI：精彩程度堪比电视剧，马斯克与奥特曼、OpenAI的「爱恨纠缠史」 - 知乎 (zhihu.com)生物信息学必备网站大全 - 知乎 (zhihu.com)生物信息学简史 - 知乎 (zhihu.com2023第一性原理科研服务器、量化计算平台推荐 - 知乎 (zhihu.com)Llama-2 LLM各个版本GPU服务器的配置要求是什么？ - 知乎 (zhihu.com)人工智能训练与推理工作站、服务器、集群硬件配置推荐整理了一些深度学习，人工智能方面的资料，可以看看机器学习、深度学习和强化学习的关系和区别是什么？ - 知乎 (zhihu.com)人工智能 (Artificial Intelligence, AI)主要应用领域和三种形态：弱人工智能、强人工智能和超级人工智能。买硬件服务器划算还是租云服务器划算？ - 知乎 (zhihu.com)深度学习机器学习知识点全面总结 - 知乎 (zhihu.com)自学机器学习、深度学习、人工智能的网站看这里 - 知乎 (zhihu.com)2023年深度学习GPU服务器配置推荐参考（3） - 知乎 (zhihu.com)多年来一直专注于科学计算服务器，入围政采平台，H100、A100、H800、A800、L40、L40S、RTX6000 Ada，RTX A6000，单台双路256核心服务器等。",
        "voteup_count": 4,
        "updated_time": "2024-05-01 19:07:10",
        "question_id": 649233834,
        "user_id": "fdd23d5babd02447757c668338a4cd59"
    },
    {
        "answer_id": 3439923156,
        "content": "实测，13b模型！20b要40g！如果算上量化后的模型，20b，但量化后的20b还是20b么？单张4090的显存非常尴尬，不如3090组link来的好。",
        "voteup_count": 7,
        "updated_time": "2024-03-22 18:58:35",
        "question_id": 649233834,
        "user_id": "64c8b9567b2e27040b6000747f97942d"
    },
    {
        "answer_id": 3473294455,
        "content": "像ChatGPT、Claude.ai和phind这样的聊天机器人非常有用，但可能并不总是希望的问题或敏感数据由外部应用程序处理。在平台上尤其如此，在这些平台上，你的互动可能会被人类审查，并以其他方式用于帮助训练未来的模型。一种解决方案是下载一个大型语言模型（LLM）并在自己的机器上运行它。这样一来，外部公司就永远无法访问的数据。这也是尝试一些新专业模型的一个快速选择，如Meta最近宣布的Code Llama系列模型，这些模型经过了编码调整，以及Seamless M4T，旨在实现文本到语音和语言翻译。运行自己的LLM听起来可能很复杂，但有了正确的工具，它会出奇地容易。而且许多型号的硬件要求并没有想象中那么疯狂。我已经在两个系统上测试了本文中提供的选项：一个是带有英特尔i9处理器、64GB RAM和英伟达GeForce 12GB GPU（可能没有参与运行该软件的大部分工作）的戴尔PC，另一个是只有16GB RAM的M1芯片的Mac。请注意，可能需要一点研究才能找到一个在的任务中运行良好并在桌面硬件上运行的模型。而且，很少有人能像你习惯的ChatGPT（尤其是GPT-4）或Claude.ai这样的工具那样好。命令行工具LLM的创建者Simon Willison在上个月的一次演示中认为，即使本地模型的响应是错误的，运行它也是值得的：在你的笔记本电脑上运行的[一些]会产生疯狂的幻觉——我认为这实际上是运行它们的一个很好的理由，因为在你的电脑上运行弱型号是了解这些东西如何工作及其局限性的一种更快的方式。同样值得注意的是，开源模型可能会不断改进，一些行业观察人士预计，它们与商业领袖之间的差距会缩小。1.使用GPT4All运行本地聊天机器人如果你想要一个在本地运行、不会在其他地方发送数据的聊天机器人，ﾠGPT4Allﾠ提供了一个易于设置的桌面客户端供下载。它包括在自己的系统上运行的型号的选项，还有Windows、macOS和Ubuntu的版本。当第一次打开GPT4All桌面应用程序时，将看到下载大约10个（截至本文撰写之时）可以在本地运行的模型的选项。其中包括Meta AI的模型Llama-2-7B聊天。如果你有API密钥，你也可以设置OpenAI的GPT-3.5和GPT-4（如果你有访问权限）供非本地使用。GPT4All接口的模型下载部分一开始有点令人困惑。在我下载了几个模型后，我仍然可以选择全部下载。这表明下载不起作用。然而，当我检查下载路径时，模型就在那里。GPT4All中模型下载接口的一部分。一旦我打开应用程序的使用部分，我下载的模型就会自动出现。一旦建立了模型，聊天机器人界面本身就很干净，易于使用。方便的选项包括将聊天复制到剪贴板并生成响应。GPT4All聊天界面简洁易用还有一个新的测试版LocalDocs插件，可以让你在本地与自己的文档“聊天”。可以在中启用Settings > Plugins 选项卡，将看到“LocalDocs插件（BETA）设置”标题和在特定文件夹路径创建集合的选项。该插件正在进行中，并且ﾠ文档ﾠ 警告说，即使LLM可以访问添加的专家信息，它仍可能“产生幻觉”（编造）。尽管如此，这是一个有趣的功能，随着开源模型变得更加强大，它可能会得到改进。除了聊天机器人应用程序，GPT4All还绑定了Python、Node和命令行界面（CLI）。还有一种服务器模式，可以让你通过一个结构非常像OpenAI的HTTP API与本地LLM交互。目标是通过更改几行代码，让你用本地LLM交换OpenAI的LLM。2.命令行启动LLMSimon Willison的LLM是我见过的在自己的机器上本地下载和使用开源LLM的更简单的方法之一。虽然确实需要安装Python来运行它，但不应该接触任何Python代码。如果你在Mac电脑上使用Homebrew，只需使用：brew install llm如果在Windows计算机上，请使用最喜欢的安装Python库的方式，例如：pip install llmLLM默认使用OpenAI模型，但可以使用插件在本地运行其他模型。例如，如果安装gpt4all插件，将可以从gpt4all访问其他本地模型。还有用于llama、MLC项目和MPT-30B的插件，以及其他远程模型。在命令行上安装一个插件，名称为llm Install model：llm install llm-gpt4all可以使用命令llm-models-list查看所有可用的型号——远程和已安装的型号，包括每种型号的简要信息。图片当要求LLM列出可用型号时显示。要将查询发送到本地LLM，语法如下：llm -m the-model-name \"Your query\"如何选择正确的模型？可以前往ﾠGPT4All主页ﾠ并向下滚动到与GPT4All兼容的模型的模型资源管理器。falcon-q4_0选项是一款评级较高的相对较小的型号，具有允许商业使用的许可证，所以我从那里开始。然后，我问了它一个类似于ChatGPT的问题，但没有发出单独的命令下载模型：llm -m ggml-model-gpt4all-falcon-q4_0 \"Tell me a joke about computer programming\"这是LLM用户体验如此优雅的一点：如果本地系统上不存在GPT4All模型，LLM工具会在运行查询之前自动为下载它。下载模型时，将在终端中看到进度条。LLM自动下载了我在查询中使用的模型这里有一个笑话：“程序员为什么关掉电脑？因为他想看看它是否还在工作！”——但事实上，这个查询确实起了作用。如果结果令人失望，那是因为模型性能或用户提示不足，而不是LLM工具。还可以为LLM中的模型设置别名，以便可以使用较短的名称来引用它们：llm aliases set falcon ggml-model-gpt4all-falcon-q4_0要查看所有可用的别名，请输入：llm aliases。这个ﾠMeta的Llama模型的LLM插件ﾠ需要比GPT4All多一点的设置。阅读上的详细信息ﾠLLM插件的GitHub回购。请注意，通用llama-2-7b-chat确实设法在我的工作Mac上运行了M1 Pro芯片，只有16GB的RAM。与为没有GPU的小型机器优化的GPT4All型号相比，它运行得相当慢，在我更健壮的家用电脑上表现更好。LLM还有其他功能，例如argument标志，使可以从以前的聊天中继续，并能够在Python脚本中使用它。9月初，该应用程序获得了ﾠ用于生成文本嵌入的工具，可用于搜索相关文档的文本含义的数字表示。可以在LLM网站上看到更多信息。Willison是流行的Python Django框架的联合创建者，他希望社区中的其他人能为LLM生态系统贡献更多插件。3.桌面上调用模型：OllamaOllama是一种比LLM更容易下载和运行模型的方法。然而，该项目仅限于macOS和Linux，直到2月中旬，Windows的预览版终于问世。我测试了Mac版本。图片通过点击安装是一种优雅的体验。尽管Ollama是一个命令行工具，但只有一个命令具有以下语法ollama run model-name名称。与LLM一样，如果模型还没有在的系统上，它将自动下载。可以在上查看可用型号的列表ﾠhttps://ollama.ai/library，截至本文撰写之时，它包括基于Llama的模型的几个版本，如通用Llama 2、Code Llama、针对某些编程任务进行微调的DeepSE的CodeUp，以及针对回答医学问题进行微调的meddlama2。这个ﾠOllama GitHub repo's README 文件包括一些型号规格的有用列表和建议，“你应该至少有8GB的RAM来运行3B型号，16GB来运行7B型号，32GB来运行13B型号。”在我的16GB RAM Mac上，7B Code Llama的性能出奇地快。它将回答有关的问题ﾠbash/zshﾠshell命令以及Python和JavaScript等编程语言。在Ollama终端窗口中运行Code Llama的外观尽管它是家族中最小的模型，但如果不完美地回答了一个R编码问题，这会让一些更大的模型感到困惑，那它还是很好的：“为ggplot2图写R代码，其中的条形图是钢蓝色的。”代码是正确的，只是其中两行代码中有两个额外的右括号，这很容易在我的IDE中找到。我怀疑更大的Code Llama本可以做得更好。Ollama还有一些附加功能，例如ﾠLangChainﾠ集成和使用PrivateGPT运行的能力，除非检查ﾠGitHub repo的教程页面。如果你在Mac电脑上，想使用Code Llama，你可以在终端窗口中运行它，每次有问题时都会把它拉出来。我期待着在我的家用电脑上使用Ollama Windows版本。4.与自己的文档聊天：H2OGPTH2O.aiﾠ一段时间以来，该公司一直致力于自动化机器学习，因此很自然地进入了聊天LLM领域。它的一些工具最好由熟悉该领域的人使用，但安装其测试版本的说明ﾠh20GPTﾠ聊天桌面应用程序快速而直接，即使对于机器学习新手来说也是如此。你可以访问web上的演示版本（显然不是使用系统本地的LLM），网址：ﾠgpt.h2o.ai。本地LLaMa模型基于VS代码文档回答问题无需添加自己的文件，就可以将该应用程序用作通用聊天机器人。或者，可以上传一些文档并询问有关这些文件的问题。兼容的文件格式包括PDF、Excel、CSV、Word、文本、标记等。测试应用程序在我的16GB Mac上运行良好，尽管较小型号的结果无法与带GPT-4的付费ChatGPT相比（一如既往，这是型号的函数，而不是应用程序的函数）。H2OGPT UI为知道自己在做什么的用户提供了一个专家选项卡，其中包含许多配置选项。这为更有经验的用户提供了尝试改进其结果的选项。如果你想对流程进行更多控制，并为更多型号提供选项，请下载完整的应用程序。对于带有GPU或仅带有CPU的系统，有适用于Windows和macOS的一键安装程序。请注意，我的Windows防病毒软件对Windows版本不满意，因为它没有签名。我熟悉H2O.ai的其他软件，代码在GitHub上也有，所以我愿意下载并安装它。Rob Mulla，现在在H2O.ai，发布了一个ﾠYouTube视频ﾠ关于在Linux上安装该应用程序。尽管该视频已经发布了几个月，应用程序用户界面似乎也发生了变化，但该视频仍然有有用的信息，包括关于H2O.ai LLM的有用解释。5.轻松但缓慢的数据聊天：PrivateGPTPrivateGPTﾠ还设计用于让使用自然语言查询自己的文档，并获得生成的人工智能响应。此应用程序中的文档可以包括几十种不同的格式。README确保数据“100%私有，任何时候都不会有数据离开的执行环境。可以在没有互联网连接的情况下获取文档并提出问题！”PrivateGPT的特点是脚本可以摄取数据文件，将其分割成块，创建“embeddings”（文本含义的数字表示），并将这些嵌入存储在本地Chroma矢量存储中。当你提出问题时，该应用程序会搜索相关文档，并将其发送给LLM以生成答案",
        "voteup_count": 2,
        "updated_time": "2024-04-21 10:28:51",
        "question_id": 649233834,
        "user_id": "a17c1d40f792435bb6deecb08e9817b2"
    },
    {
        "answer_id": 3485787674,
        "content": "全球移动互联网公司APUS与大模型创企新旦智能宣布，联手开源国内首个千亿参数的MoE（混合专家模型）APUS-xDAN大模型4.0，这也是国内首个可以在消费级显卡上运行的千亿MoE中英文大模型。APUS-xDAN-4.0（MoE）参数规模为1360亿，可在消费级显卡4090上运行，据APUS实测，其综合性能超过GPT-3.5，达到GPT-4的90%。数学能力上，测评基准GSM8K的测评得分为79，理解能力MMLU达到73分。项目地址：https://github.com/shootime2021/APUS-xDAN-4.0-moe?tab=readme-ov-file01.数学、推理能力碾压推理成本下降400%APUS-xDAN-4.0（MoE）在GitHub的页面显示了基准测评结果，其与Mixtral-8x7B（MoE）、Llama2-70B、Grok-1（MoE）进行了对比。其中衡量模型语言理解、知识和推理能力的基准测试MMLU中，APUS-xDAN-4.0（MoE）排名第一，超过了Grok-1（MoE）。在测试多步骤数学推理能力的单词问题集合测试GSM-9K以及MATH中，该模型得分均远高于其他三大模型。四项测试中，APUS-xDAN-4.0（MoE）在多学科任务的BIG-Bench-Hard测试中，得分为66.4，接近Mixtral-8x7B（MoE），低于Grok-1（MoE）的71.7分。其中，Mixtral-8x7B（MoE）由大模型创企Mistral AI于去年年底发布，并在多项基准测试中性能都基本达到GPT-3.5；Llama2-70B是去年7月Meta开源的Llama 2大模型系列中，参数规模最大的版本；Grok-1（MoE）为马斯克旗下AI创企xAI本月初开源的大模型，参数规模为3140亿参数，是目前开源大模型中参数规模之最。此外，在GitHub页面显示，APUS-xDAN-4.0（MOE）开源模型在“IQ-Quantized Tech”上量化为1.5位、2位和4位，可以在消费级显卡4090上运行。具体来说，APUS-xDAN 大模型4.0（MoE）采用GPT-4类似的MoE架构，特点是多专家模型组合，同时激活使用只有2个子模块，实际运行效率对比传统Dense同尺寸模型效率提升200%，推理成本下降400%。在实际部署中，研究人员通过进一步高精度微调量化技术，使得模型尺寸缩小500%。02.32个MoE Transformer块组成可处理多线程复杂需求在实际的效果中，APUS-xDAN-4.0（MoE）可以理解复杂需求，如撰写广告文案时，要求涵盖“火焰人”、固定口号、摇滚乐歌词等。该模型还可以找出段落中的实时性错误，并给出修改版本，包括品牌所属地区、语病等。APUS-xDAN-4.0（MOE）模型的架构特点为，主要由32个相同的MoE Transformer块组成，与普通Transformer块相比，MoE Transformer块的FFN层被MoE FFN层替换。张量经过门层计算每个专家模型的分数，根据专家分数从8个专家模型中选择Top-K专家。张量通过Top-K专家的输出进行聚合，从而得到MoE FFN层的最终输出。每个专家由3个线性层（Linear Layers）组成。APUS-xDAN-4.0的所有Norm Layer都是用RMSNorm，与开源大模型Llama的方式一致。在注意力层中，APUS-xDAN-4.0（MoE）中的QKV矩阵的Q矩阵形状为（4096,4096），K和V矩阵形状为（4096,1024）。▲APUS-xDAN-4.0（MoE）模型架构图03.APUS是投资方7B模型表现超GPT-4APUS-xDAN-4.0（MoE）背后的两家公司均来自国内，且APUS于今年3月投资了新旦智能。新旦智能成立于2023年5月，此前该公司发布的xDAN-7B-Global在用于衡量大模型在机器翻译任务性能的综合性评估指标MT-Bench中，性能表现仅次于GPT-4。▲MT-Bench排名今年3月，新旦智能完成千万级别天使轮融资，投资方包括APUS和AI投资人周弘扬。据了解，新旦智能的创始团队汇集了清华、伯克利等顶尖学府以及腾讯、Meta等头部科技玩家的员工，包括全球开源AI社区知名开发者、腾讯云架构师等。与此同时，这也是APUS在开源大模型领域的最新进展。今年2月初，APUS与深圳大学国家工程实验室联合开源了APUS大模型3.0伶荔。04.结语：更强开源MoE模型或加速AI开发及应用创新开源大模型对于全球大模型产业的技术进步与应用落地的重要性与日俱增，并逐渐显现出对标当下大模型顶流GPT-4的潜力。在这个趋势下，APUS与新旦智能联手，既开源了目前国内参数规模最大的MoE模型，同时降低了其部署成本，为更多开发者提供了应用大模型能力的可能性，这或许将进一步加速AI开发及应用创新。",
        "voteup_count": 1,
        "updated_time": "2024-05-02 22:16:49",
        "question_id": 649233834,
        "user_id": "dd0cf3f9b8809033760b59336b599f6d"
    },
    {
        "answer_id": 3479455451,
        "content": "运行普通模型70B应该很轻松，但是并发能力就没有了",
        "voteup_count": 1,
        "updated_time": "2024-04-26 15:24:52",
        "question_id": 649233834,
        "user_id": "ba77fc840468ddc03aa81159052e7734"
    },
    {
        "answer_id": 3486789360,
        "content": "能够运行的最强开源大模型可能是像GPT-NeoX-20B这样的模型。这类模型通常设计为多个亿级的参数量，而4090因为其高显存（24GB）和强大的计算能力，使得它适合运行这种规模的模型。在实际操作中，可能还需要进行一些内存优化技巧，如模型裁剪或使用一些内存节省的技术，以适应单卡的内存限制。",
        "voteup_count": 0,
        "updated_time": "2024-05-04 04:21:15",
        "question_id": 649233834,
        "user_id": "daf297137e09c79a2cce633230892cb9"
    },
    {
        "answer_id": 3481821078,
        "content": "目前4090可以运行的最好的中文模型应该是Qwen-1.5-Chat-32B-int4",
        "voteup_count": 8,
        "updated_time": "2024-04-28 19:39:08",
        "question_id": 649233834,
        "user_id": "57afb76045bf2489f53f8ac516898ac1"
    },
    {
        "answer_id": 3473695136,
        "content": "不能简单的算模型量与显存容量的关系，因为现在不少框架还可以利用内存。单4090，实测可以跑70B 的Llama3, 用的Ollama，一秒钟平均蹦出1.5 -2 个token，慢，但是确实能用。",
        "voteup_count": 5,
        "updated_time": "2024-04-21 18:11:18",
        "question_id": 649233834,
        "user_id": "5a51b769c3c5b3501288141cd2f01d01"
    },
    {
        "answer_id": 3437044514,
        "content": "单纯论参数量的话，可以跑的最大模型是01的34b模型的量化版本。但这个参数量大，实际效果不好，按我个人不全面的测试，千问1.5的14B的效果还要好一点。",
        "voteup_count": 4,
        "updated_time": "2024-03-20 11:52:30",
        "question_id": 649233834,
        "user_id": "2a4f63ec0aa052bd873bee4040953c85"
    },
    {
        "answer_id": 3486118831,
        "content": "现在模型底座还是缺陷比较大，算力能耗要求太高，再等一两年，看看有没新算法。",
        "voteup_count": 2,
        "updated_time": "2024-05-03 10:42:56",
        "question_id": 649233834,
        "user_id": "9377f91fc232d54034a9287377143d7d"
    },
    {
        "answer_id": 3486208846,
        "content": "我已经找到了单张4050能运行的最强开源大模型：llama 3 8b",
        "voteup_count": 1,
        "updated_time": "2024-05-03 12:31:50",
        "question_id": 649233834,
        "user_id": "bc90e9b328077fb51404c8e84e7b0a88"
    },
    {
        "answer_id": 3486202897,
        "content": "围棋软件katago，不要说单张4090，100张都可以把资源榨干。",
        "voteup_count": 1,
        "updated_time": "2024-05-03 12:23:53",
        "question_id": 649233834,
        "user_id": "84c481ead7c9a9e95ab365a2f2ed71c8"
    },
    {
        "answer_id": 3486343127,
        "content": "只是运行，4090都不行了还要什么卡",
        "voteup_count": 0,
        "updated_time": "2024-05-03 15:43:36",
        "question_id": 649233834,
        "user_id": "fa49ba0701eb46c36a3ede9fa1a1f3b6"
    },
    {
        "answer_id": 3485800446,
        "content": "单张NVIDIA GeForce RTX 4090显卡能够运行的最强开源大模型取决于多个因素，包括模型的参数量、模型的优化程度、显卡的内存大小（4090通常拥有24GB的显存），以及模型运行时所需的具体计算资源。从搜索结果中可以看到，有多个开源大模型（LLM）被提及，例如Meta的Llama 3模型，它在性能上取得了显著的进展，并且有不同参数规模的版本，如8B（80亿参数）和70B（700亿参数）。Llama 3模型在多个基准测试中展现了优异的性能，并且支持长达8K的文本输入，具有改进的tokenizer和更大的词汇量。此外，还有如谷歌的Gemma模型、阿里巴巴的通义千问模型等，这些模型也提供了不同尺寸的版本，以适应不同硬件配置的需求。对于RTX 4090而言，由于其拥有较大的显存，理论上可以运行一些参数量较大的模型。然而，实际上能够运行的“最强”模型还需要考虑模型的优化程度和运行效率。例如，一些模型可能经过了特定的优化，使得它们能够在消费级硬件上更高效地运行。在实际应用中，用户可能需要根据自己的具体需求和模型的性能特点来选择最适合的模型。同时，由于开源模型的领域不断发展，可能会有新的模型发布，提供更高的性能或更适合特定任务的优化。综上所述，没有一个明确的答案指出哪个是单张RTX 4090能运行的最强开源大模型，因为这取决于上述多种因素。用户可以根据自己的硬件配置和需求，选择经过优化且适合自己使用场景的模型。",
        "voteup_count": 0,
        "updated_time": "2024-05-02 22:33:45",
        "question_id": 649233834,
        "user_id": "1fe4eac24b482c9d42c72dd662c2df2f"
    }
]