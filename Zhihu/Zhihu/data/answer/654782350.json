[
    {
        "answer_id": 3485724004,
        "content": "mlp之所以经久不衰，就是因为他简单，快速，能scale-up。KAN让人想起来之前的Neural ODE，催生出来比如LTC（liquid time constant）网络这种宣称19个神经元做自动驾驶。（当然只是名字噱头，其实只是自动驾驶最简单的车道保持任务）再广一点，不管是我们熟知的ANN（人工神经网络），还是BNN（贝叶斯神经网络），SNN（脉冲神经网络），GNN（图神经网络）。我觉得对我们这种科研狗来说多一个新的网络绝对不是坏事，因为就能在上面做（shui）好多论文（bushi。但是工业界来说，这么多年还是只有ANN发扬光大，就是因为简单，快速，能scale up。KAN和这些老前辈一样面临的问题就是怎么堆大，怎么更方便硬件调度，很显然这方面KAN是大短板。如何解决这个问题是替代mlp的核心，毕竟如果只是单论可学习激活，这个topic已经是老话题了。只不过这次KAN。yysy我觉得MIT的人真的很聪明，像之前LTC一样，KAN的写作包装是值得所有科研人学习的。nature那个例子真的是太会找了",
        "voteup_count": 81,
        "updated_time": "2024-05-02 20:55:38",
        "question_id": 654782350,
        "user_id": "d66e9eacd12ccb0beb70e68dc7c6fa6d"
    },
    {
        "answer_id": 3485866401,
        "content": "主要是炒作。KAN本质上就是向量值激活函数，把1个实数映射成1+m个实数，m是B-spline基函数B_i的个数。从理论上讲KAN是把权重矩阵中的元素从实数（考虑乘法，也可以看作实数到实数的映射，把x映到wx）推广为实数到实数的映射。但实际上这个映射的参数化方式是w(σ(x)+c_i*B_i(x))，其中σ是SiLU，所以也可以看作先把x提升为σ(x)和B_i(x)拼成的1+m维向量（i取1到m），再与w和wc_i拼成的向量做内积。也就是说KAN的一层相当于output=einsum('ijk,jk->i', weight, h(input))，其中input是形状为(d,)的输入特征，h(input)是形状为(d, 1+m)的激活值，weight是形状为(out_d, d, 1+m)的权重。常规MLP的一层相当于output=einsum('ij,j->i', weight, σ(input))。不难看出，KAN甚至可以被视为每一支采用不同激活函数的Inception结构。另见 @2prime 的推文",
        "voteup_count": 59,
        "updated_time": "2024-05-03 04:05:46",
        "question_id": 654782350,
        "user_id": "de55655c466549f343ad2f175e593b9d"
    },
    {
        "answer_id": 3485932943,
        "content": "笑点解析：toy dataset。好歹跑个MNIST数据集给大伙看看比MLP高多少啊。",
        "voteup_count": 46,
        "updated_time": "2024-05-03 03:09:40",
        "question_id": 654782350,
        "user_id": "279d5560809d4fbc6ba06c1ba942f0b8"
    },
    {
        "answer_id": 3485818375,
        "content": "实际上主体部分可以用矩阵乘做，不会很慢。flops打满还是问题不大的。简单写了个高效实现，能拟合数学函数，还没有做正经测试。坑点在README里有简单提及。https://github.com/Blealtan/efficient-kan如果有必要的话再来开坑多写点东西吧，我感觉这玩意对大模型用处不大，但是对ai4science的小模型和可解释性应该会非常有用。b-spline拟合连续函数太有优势了，relu系在它面前不堪一击（）",
        "voteup_count": 39,
        "updated_time": "2024-05-02 22:55:07",
        "question_id": 654782350,
        "user_id": "070de1975285b0ba734532c84d002c8a"
    },
    {
        "answer_id": 3485703777,
        "content": "科尔莫戈洛夫-阿诺德表示定理本质上是一个特殊情况的隐函数定理在一个点，以及一个不等式估计。根本没有用到全空间的刚性性质，没办法对全空间进行很好的编码勿cue Arnold，这套东西对机器学习没有帮助，比trransformer架构弱得多。",
        "voteup_count": 23,
        "updated_time": "2024-05-02 20:23:04",
        "question_id": 654782350,
        "user_id": "15a9f8b1a3ddcb9514a56ddd669846fd"
    },
    {
        "answer_id": 3485782154,
        "content": "论文链接：https://arxiv.org/pdf/2404.19756关于论文的介绍，推荐 @量子位 的文章介绍，包括了一些研究人员的看法、文章概要以及实验，比较全面。量子位：全新神经网络架构KAN一夜爆火！200参数顶30万，MIT华人一作，轻松复现Nature封面AI数学研究KAN与MLP在架构上的不同，在于采用了可学习的非线性激活函数而不是可学习的线性变换。从文章的初步实验来看，有以下优势：精度更高：在一些toy datasets、求解PDE、某些小规模科学问题（文章中复现了DeepMind发表在Nature的结果）具有更高的精度。对于规模不大、精度要求高的任务，不妨尝试使用KAN代替MLP，冲击一波SOTA。参数更少：文章中复现了DeepMind发表在Nature的结果，MLP要用大约300000 个参数，而KAN大约要200 个参数。有望解决灾难性遗忘问题、缩放速度更快：显然，这都是指向大模型的。文章中使用了简单的case来说明KAN的这两点优势。可惜没能看到直接应用在大语言模型上的效果。如果能够结合最近发布的Llama3进行验证，感觉会是一个很有意义的工作。缺点：训练时间长：作者在文章中表明，KAN通常比MLP慢10倍。这里比较好奇的是居然是一个常数倍数关系。而且作者表示自己没有像一个engineer一样尽力去优化。不过，如果只是慢10倍的话，考虑到更快的缩放速度以及更少的模型权重参数，应用于LLM中也未尝不可。就当前KAN发展阶段来看，在精度要求高或者模型规模小的时候，可以尝试使用KAN代替MLP，是否有潜力取代MLP，得看最终实践效果。--------------------------摘选一些网友的意见评论：看完了这些例子，优化器是 LBFGS 的原因是什么？看到 Adam 在 pykan 中也有用，但在例子中没有用到，它也能用吗？作者回复：在高精度的情况下，LBFGS 优于 Adam（因为它是准二阶和线搜索）。但如果用户不执着于高精度，Adam 也可以很好地发挥作用。评论：我认为这是一个不公平的比较。你可以将更复杂的模型在拓扑上压缩为相同大小的架构。但是你隐藏了任意多个权重，而 MLP 包含单独的权重。（赞同这个观点，KAN中单个神经元的权重更多了，训练也更加困难。同时，堆叠KAN层的时候，这些参数将会指数级）评论：你在论文中提到，与 MLP 相比，训练速度慢约 10 倍 - 这种额外的架构复杂性是否会影响训练稳定性？如果我们将许多 KAN 层堆叠在一起，网络在训练过程中是否总是会收敛？（训练稳定问题需要搭建深层KAN进行实验探究，如果不稳定，增加残差连接或者dense Net的方式会不会解决问题？）评论：我有一个很深层次的问题，我们可以在 transformer 架构中使用这个 KANs 算法，并在 Q、K、V 或输出线性层中应用这个可学习函数。这会影响(+) ive 方向或(-) ive 方向的准确性吗？（没太看懂这个(+) ive direction是什么意思。不过尝试使用可学习的非线性激活函数代替  确实是个有意思的想法，KAFormer什么的，感觉很快会有了吧）评论：KAN在训练过程中的反向梯度传播是怎么实现的呢？网友回复：单变量函数（可学习函数，可有效充当网络中的权重和激活函数）。与 MLP 类似，误差梯度在输出处计算并通过网络向后传播，但不是更新权重，而是使用它来调整每层单变量函数的参数。网友回复：有啥区别？除非它具有另一个参数，否则如何更新单变量函数——这不是与权重完全相同的东西吗？",
        "voteup_count": 17,
        "updated_time": "2024-05-02 22:57:49",
        "question_id": 654782350,
        "user_id": "d2d2b75a434bab27f88a991084485e42"
    },
    {
        "answer_id": 3485883305,
        "content": "媒体宣传的力量太可怕了，KAN一天拿下了三大号其中两个的头版头条和一个的次条（上一次颠覆的架构这么牛还是mamba，听说mamba有几篇ICML收了，看来mamba已经势不可挡了）。这波造势，Github星标已经突破了2.2k。Github：https://github.com/KindXiaoming/pykan甚至已经写好了API文档：https://kindxiaoming.github.io/pykan/intro.html推上影响力也是非凡的，有接近60W的浏览，近900的转帖。咱们再来看看远处的mamba，从去年12月到现在也才79W浏览，400多转帖。华为的刘群老师认为这个想法很insightful。不知道是不是很快会出现各种KAN的变体，建议结合Transformer的标题可以来一篇论文的title就叫”KAN KAN Need“（doge。另外，发现好多学物理的大佬搞AI这真的是降维打击（数不过来了）。",
        "voteup_count": 16,
        "updated_time": "2024-05-03 02:15:02",
        "question_id": 654782350,
        "user_id": "3822721cffa3715db62037c499353ccd"
    },
    {
        "answer_id": 3486166838,
        "content": "我看到新智元的标题里“轻松复现Nature封面AI数学研究“这一部分时，心里想的是， 我们学数学的上辈子是欠了dl圈的人一个亿吗， 隔三差五地就来一波。。。自从我老婆生完娃月经暂停了一段时间后，我能接触的最接近月经的事情就是新智元有事没事说某种dl方法能在数学上怎么怎么大展拳脚了。老实说，我到现在还没从之前deepmind硬蹭矩阵乘法那件事里缓过来。当时看到标题，然后再看实际的paper，差点没给我气心梗。本来工作就不好找，题目也不好做，然后这边还隔三差五地来碰个瓷。心好累。",
        "voteup_count": 11,
        "updated_time": "2024-05-03 11:44:27",
        "question_id": 654782350,
        "user_id": "6f2a1765b7b406d262de8d3002af0d3e"
    },
    {
        "answer_id": 3485608211,
        "content": "想象你有一个复杂的拼图，需要将其拼完整。传统的拼图是由许多小块组成的，而每块都有一个特定的形状，需要与周围的块恰好契合。而KANs就像是一个特殊的拼图，可以通过学习的方式自我调整每个块的形状和位置，使其与整体更加吻合。那么，如何理解MLP与KANs之间的区别呢？MLP的拼图：MLP的拼图有一个固定的规则，每一层都像是给拼图块添加了一层统一的涂层，使其在某种程度上都能拼接起来。虽然这种方式能够完成拼图，但在面对一些复杂的形状时，它可能需要更多的拼图块才能完成。KAN的拼图：KAN的拼图则是灵活且动态的。每一块拼图都可以根据其他块的情况进行自我调整。每一层就像是对每一块拼图进行单独的雕刻，使其与其他块完美契合。KANs的这种灵活性使得它们在完成拼图的过程中具有一些天然的好处：（1）更准确：由于KANs可以对每一块拼图单独雕刻，它们在面临复杂的数据集或问题时能够提供更准确的解决方案。（2）更快速：因为KANs通过对拼图块进行调整和雕刻，使得整体结构更加协调，因此可以用较少的块数完成拼图，提高了模型的效率。（3）可解释性：KANs就像是一个透明的拼图，让我们能够看到每一块的形状是如何调整的，从而更好地理解模型的工作原理。不过，KANs也有一些缺点有待进一步的改进。首先是雕刻的时间较长：KANs需要花费更多的时间来雕刻每一块拼图，尤其是当拼图块的数量很多时，这个过程会非常耗时。这反映了KANs在训练效率上的不足，需要一些算法上的改进，比如批量化的计算，来加速训练。其次，对不同风格的拼图的适应性有待验证，KANs擅长处理一些特定风格的拼图（如科学计算问题），但对于风格迥异的其他类型的拼图（如语言理解任务）表现如何，还需要更多的实践来检验。",
        "voteup_count": 23,
        "updated_time": "2024-05-03 09:32:55",
        "question_id": 654782350,
        "user_id": "b9ac75fda4638d8013476b9855039c65"
    },
    {
        "answer_id": 3486218400,
        "content": "挺好的，会让越来越多的人意识到面向公众号做科研的危害性",
        "voteup_count": 0,
        "updated_time": "2024-05-03 12:45:26",
        "question_id": 654782350,
        "user_id": "c62e94ffec30821174e7c07a8ad6ff52"
    },
    {
        "answer_id": 3486209232,
        "content": "五一期间，新神经网络架构KAN发布，在性能和可解释性上超越MLP架构，但目前训练时间较长。深度学习模型(推动人工智能革命的神秘引擎)不再笼罩在神秘之中。如果我们能够窥视它们的内部运作，理解它们的推理，甚至与它们合作揭开宇宙的秘密，那会怎样？这就是柯尔莫哥洛夫-阿诺德网络(KAN)的承诺，这是一种革命性的新架构，有望改变人工智能的格局。多层感知器(MLP），目前深度学习的主力架构的黑箱性质，阻碍了可解释性，低效率限制了你们的潜力，在高维数据方面的挣扎让广阔的知识领域尚未被探索。现在是时候出现一种新型的神经网络了，它将深度学习的力量与数学的优雅和人类理解的透明性结合起来MLP的核心问题在于其结构。虽然它们的通用近似能力已经得到充分证实，但它们在节点上的固定激活函数和对线性变换的依赖限制了它们有效表示复杂函数的能力，尤其是那些具有组合结构的函数。这种低效率导致模型更大，计算成本增加，并且阻碍了可解释性，因为理解其预测背后的原因变得具有挑战性。此外，MLP 经常受到数据维数灾难的影响，随着输入数据维数的增加，它们的性能会下降。KAN借鉴了柯尔莫哥洛夫-阿诺德表示定理，解决了这些痛点。柯尔莫哥洛夫-阿诺德表示定理指出，任何连续多变量函数都可以分解为单变量函数和加法的组合。KAN不使用节点上的固定激活函数，而是在边缘上使用可学习的激活函数,用样条函数表示。这一关键区别使KAN能够有效地学习函数的组合结构以及该组合中的各个函数。与MLP相比，KAN实现了更高的准确率尤其是在处理高维数据和复杂函数时。此外，KAN在可解释性方面具有显著优势。其结构允许直观地可视化所学习到的函数，从而深入了解模型的决策过程。此外,本文介绍了在不牺牲准确性的情况下简化KAN的技术，进一步提高了其透明度。",
        "voteup_count": 1,
        "updated_time": "2024-05-03 12:32:19",
        "question_id": 654782350,
        "user_id": "cb7237a0b23ea5e3dc80ea2ccb8463b9"
    },
    {
        "answer_id": 3485979440,
        "content": "目前所谓的爆火仅仅只是新闻传播中的刷屏而已，而在不同场景中去使用KAN产生效果，并广为被人接受。估计这个还需要有一段时间。时间+实践是检验一切的方法，毕竟MLP经过了这么长时间，并在业界被广泛使用。所以KAN到底火不火，还得再等等看。",
        "voteup_count": 1,
        "updated_time": "2024-05-03 07:23:07",
        "question_id": 654782350,
        "user_id": "fbd5c2e8aa9a9f651b2ffbe3aafb3f16"
    },
    {
        "answer_id": 3486189092,
        "content": "01 KAN为何一夜爆火？！近日，一种突破性的神经网络架构——KAN(Kolmogorov–Arnold Networks)诞生啦！！ 机器学习范式就要变天啦？！它的设计哲学与传统的MLP(多层感知机)有着明显差异，且在使用更少的参数解决数学和物理问题上显示出了更高的精确度。举个例子，仅用200个参数的KAN就能重现DeepMind利用30万参数的MLP在发现数学定理方面的研究成果。下周奥特曼庆生，GPT-5或将惊艳亮相？Lla…GPT-5就要发布啦，还没体验过原生OpenAI Chat GPT4.0？快戳最新版升级教程，教你如何几分钟搞定：手把手升级ChatGPT4.0 Turbo详细步骤教程（2024年4月）这种新架构不仅加强了精确度，同时还揭示了未知的数学公式。而这些研究，DeepMind的成果曾被登载在Nature杂志的封面上呢~在函数逼近、求解偏微分方程，乃至处理凝聚态物理问题的能力上，KAN都展现出比MLP更出色的表现。 论文地址：https://arxiv.org/abs/2404.19756 项目地址：https://kindxiaoming.github.io/pykan/在解决大规模模型问题时，KAN能自然避免灾难性遗忘并且轻松融合人类的直觉偏好或特定领域知识。MIT、加州理工学院、东北大学等组成的研究团队的这项新研究一经发布，立即在整个科技界引发轰动：Yes We KAN！KAN与MLP在激活函数的配置上存在显著的差异，这也是二者最为直观的区别之一。传统MLP的激活函数通常位于神经元上，而KAN则创新地将可学习的激活函数直接置于权重之上。在作者眼中，这一改动看似“简单”，实则蕴含了深刻的变革。研究团队在MLP的基础上做出了一个微妙而关键的调整：他们把可以学习的激活函数从神经元（即节点）搬到了连接它们的连接点（即边）上！这一变化初听起来似无道理，然而它与数学领域的\"近似理论\"紧密相连，含义深远。实际上，根据Kolmogorov-Arnold展示理论，在两层网络中，拥有可学习激活函数的边而不是节点，确实显示了更大的潜力。研究人员受到展示定理的鼓舞，将Kolmogorov-Arnold表示在神经网络中明确参数化，具现化了其理念。而KAN这个名称，也是为了向两位杰出的数学家Andrey Kolmogorov以及Vladimir Arnold致敬，他们的贡献为这一理论提供了基础。02 KAN如何实现？2.1 理论根基柯尔莫哥洛夫-阿诺德表示定理（Kolmogorov–Arnold representation theorem）告诉我们，在有限界限内定义的任何连续的多变量函数，都能够被看做是单变量连续函数的有限叠加。在机器学习的视角下，这意味着学习高维函数可以转变为学习数量有限的一维函数。然而，这些一维函数可能在实际应用中是不光滑甚至是具有分形属性的，从而导致在实际中难以进行学习，因此这一定理在机器学习领域曾几乎被认为是不可行的——理论上恰当但实用性弱。即便如此，研究者们仍然看好该理论在机器学习中的潜力，并且提出如下两项改良措施：1、而不是局限于原始的方程提到的单一隐藏层（2n+1）以及两层非线性性质，可以将网络扩展为任意的宽度与深度；2、在科学和日常生活中，多数遇到的函数都较为光滑并含有简单的组合结构，能有利于构造更平滑的柯尔莫哥洛夫-阿诺德表示。这就如同区分物理学家与数学家所关注的焦点：物理学家倾向于探究通常状况下的典型例子，而数学家则更多地考虑极限情况。2.2 实现细节KAN网络的设计概念源于通过一系列单变量函数学习多变量函数的简化。在此框架中，各单变量函数可采用可学习参数的B样条曲线来表示。研究团队从增加网络层数以加深MLPs的建构概念中获得启示，提出KAN层概念，构成一维函数的矩阵，每个函数均带可学习参数。依据柯尔莫哥洛夫-阿诺德定理，KAN基层由两类函数组合——内部和外部，对应输入输出维度。堆叠KAN层增强了深度与表达力，保持解释性，其中每层单变量函数独立学习，易于解读。此处的f可以看作KAN的具体实现：03 KAN比MLP强在何处？在解释性方面，KAN能更好地揭示数据集背后的结构和变量依赖关系，通过符号公式提供洞见。神经网络缩放效率：与MLP相比，KAN的扩展效率更高。它不仅基于数学的柯尔莫哥洛夫-阿诺德定理，其缩放性能也可通过实验验证得出。为了证明其有效性，研究团队使用了5个已知可以平滑表达KA（柯尔莫哥洛夫-阿诺德）的案例作为测试数据集，并以每200步增加一个网格点的方式对KANs进行训练，覆盖的G值集合为{3,5,10,20,50,100,200,500,1000}。将不同尺寸的MLPs作为标准对照，并且在相同条件下，即使用LBFGS优化算法训练1800步，KANs与MLPs的性能通过RMSE（均方根误差）进行比较。在函数逼近任务上，KAN展现出了比MLP更高的精度在求解偏微分方程，如泊松方程时，KAN的表现也超越MLP一个意外的发现是，KAN天生能避免MLP常见的灾难性遗忘问题，为大型模型提供了从根本上避免遗忘的解决方案。04 小结关于KAN是否能替代Transformer中的MLP层，社区内部观点分歧。首先，关键在于学习算法——如SGD、AdamW、Sophia等——是否能为KANs参数找到有效的局部最小值其次，考虑到在GPU上实施KANs层的效率问题，理想情况下应优于MLPs的执行速度论文作者还贴心地提供了一个实用的决策树，帮助判断何时采用KAN！那么，您是否考虑尝试KAN，还是暂时观望？把论文丢给GPT-4进行撤稿预测，和人类审…GPT-4都可以预测论文撤稿啦，快戳最新版升级教程：手把手升级ChatGPT4.0 Turbo详细步骤教程（2024年4月）推荐文章：革命性神经网络架构KAN诞生！一夜爆火！…手把手升级ChatGPT4.0 Turbo详细步骤教程（2024年4月）下周奥特曼庆生，GPT-5或将惊艳亮相？Lla…",
        "voteup_count": 2,
        "updated_time": "2024-05-03 12:05:51",
        "question_id": 654782350,
        "user_id": "03d359f96a3d75c1b65f315e4e5ba0d6"
    },
    {
        "answer_id": 3485628041,
        "content": "训练速度慢10倍",
        "voteup_count": 9,
        "updated_time": "2024-05-02 18:27:17",
        "question_id": 654782350,
        "user_id": "2dc3093d3cd6663673fa57a63056f92d"
    },
    {
        "answer_id": 3486022922,
        "content": "COLT: Conference of Langeng in Tieba:KAN KAN NEED: two KANs are all you NEED to replace one transformer layer名字出处：如何评价全新的神经网络架构KAN的爆火，是否有潜力取代MLP？",
        "voteup_count": 4,
        "updated_time": "2024-05-03 08:42:55",
        "question_id": 654782350,
        "user_id": "10c387ed0815758d03d320568ee51efd"
    },
    {
        "answer_id": 3486042975,
        "content": "目前的状态，KAN和MLP只能说各有优势，而且MLP的应用已经很成熟，单从缓慢的处理速度来说KAN要快速应用落地就有难度，要说取代还需要更长的时间和优化升级，不过KAN在某些特定领域上会成为有力的工具比如工业系统相关的设计和控制。KAN与传统的神经网络架构截然不同，基于Kolmogorov-Arnold表示定理，用更少的参数在数学和物理问题上取得更高的精度，能天然地规避灾难性遗忘问题，并且容易注入人类的习惯偏差或领域知识，优点显而易见。但是复杂的多层嵌套单变量连续函数，算法网络结构和参数化方法的特殊性决定了训练过程中需要更多的迭代和更复杂的优化步骤，这会导致处理速度慢，要解决这个问题还需要从算法本身的结构和逻辑去优化。",
        "voteup_count": 2,
        "updated_time": "2024-05-03 09:12:39",
        "question_id": 654782350,
        "user_id": "f279fdadf9b4d24dac5203639a8e849f"
    },
    {
        "answer_id": 3485919027,
        "content": "这么说吧，只有SNN潜力是最大的，因为脉冲神经网络是最接近人脑的网络，然而现在主流的数据输入方式和训练方式都把SNN当做静态神经网络来对待，但其实即使是动态神经网络也不能发挥出SSN的潜力。SNN应该像生物一样保持持续的信号输入和处理，才能发挥出其优势。可惜并没有很好的硬件支持，而其他神经网络和深度学习方式的高回报和成熟路线，也导致没有多少人愿意投入到跨专业知识要求高、回报较低、路线不成熟的SNN里。",
        "voteup_count": 3,
        "updated_time": "2024-05-03 02:03:33",
        "question_id": 654782350,
        "user_id": "d8471aff85aef10552774b981d5887bd"
    },
    {
        "answer_id": 3485770535,
        "content": "是一个值得关注的方向，但训练慢 10倍，很难和产业落地。KAN 并不是对传统神经网络完全的替代，而是与之融合。这是作者的的图：KAN =  Kolmogorov-Arnold + 传统网络一维函数可能是非光滑甚至是分形的，因此在实践中可能无法学习。由于这个问题吧，科尔莫戈洛夫-阿诺德表示定理基本上被判处在机器学习中死刑，被认为在理论上是正确的，但在实践中是无用的。然而作者的主要贡献是： 将 Kolmogorov-Arnold 和 MLP 结合起来，并用现代方法进行优化 。我们不必坚持原始的方程式 ，该方程式仅具有两层非线性和隐藏层中的少量项 (2n + 1)：我们将将网络推广到任意宽度和深度。其次，科学和日常生活中的大多数函数通常是平滑的，并具有稀疏的组合结构，可能有助于实现平滑的 Kolmogorov-Arnold 表示。这里的哲学与物理学家的思维方式接近，他们通常更关心典型情况而不是最坏情况。毕竟，我们的物理世界和机器学习任务必须具有结构，以使物理学和机器学习在所有情况下都具有用处或可推广性。传统的 MLP 是多层（线性函数 +固定的激活函数），训练过程学习的参数是权重 a+  偏置 b，在 KAN 变成了多层一维函数（B样条函数）组合，而这些一维函数本身的表示都是可以学习的。而线性函数 + 固定激活函数本身就是一维函数的一种，所以 KAN 不需要传统的激活函数。理论上 MLP 是  KAN的一种特殊形态。如果KANs的函数库被限制为只包含特定的线性变换和后续的非线性激活（即MLP中使用的形式），那么KANs就退化为传统的MLPs。然而，在KANs中通常允许更广泛的函数形式和更复杂的参数调整，提供了比MLPs更丰富的模型表达能力。这样做有什么优势呢 ? 除了参数效率、可解释性等。 我觉得最要的是这一点：灾难性遗忘是当前机器学习中的严重问题。当人类掌握一项任务并转而执行另一项任务时，他们不会忘记如何执行第一个任务。神经网络并非如此。当神经网络在任务 1 上训练后转移到任务 2 上训练时，网络很快会忘记如何执行任务 1。一个关键网络很快会忘记如何执行任务 1，人工神经网络和人类大脑之间的区别在于人类大脑在空间中具有功能上不同的模块。当学习新任务时，只有负责相关技能的局部区域发生结构重组[25, 26]，其他区域保持不变。大多数人工神经网络，包括 MLP，没有这种局部性的概念，这可能是灾难性遗忘的原因。作者展示了 KAN 具有局部可塑性，并且可以通过利用样条的局部性来避免灾难性遗忘。这个想法很简单：由于样条基是局部的，一个样本只会影响到几个附近的样条系数，远处的系数保持不变（这是期望的，因为远处的区域可能已经存储了我们想要保留的信息）。相比之下，由于 MLP 通常使用全局激活函数，例如 ReLU/Tanh/SiLU 等，任何局部变化可能无法控制地传播到远处的区域，破坏那里存储的信息。但目前案例都是小规模样本和测试。没有更大规模的验证。最主要的问题是训练效率低下，无法有效利用 GPU 资源。 在当下计算性能缺乏的背景下，如果不做优化，工业化落地是不太可能，但优化方向值得继续研究。",
        "voteup_count": 10,
        "updated_time": "2024-05-02 21:54:57",
        "question_id": 654782350,
        "user_id": "3db8625345af401aeae7628e25c8ddc3"
    },
    {
        "answer_id": 3486182025,
        "content": "KAN is all you need？未必吧，单一场景好用而已，不一定会像MLP那样可以作为通用的架构。简单、好用、通用才是真理。",
        "voteup_count": 0,
        "updated_time": "2024-05-03 11:57:10",
        "question_id": 654782350,
        "user_id": "3eca4d3e7fe6c4c329e5973fb2b792e5"
    },
    {
        "answer_id": 3486131335,
        "content": "我现在唯一的社交 是到处评论， 有人回复 就高兴半天",
        "voteup_count": 0,
        "updated_time": "2024-05-03 10:57:36",
        "question_id": 654782350,
        "user_id": "9377f91fc232d54034a9287377143d7d"
    },
    {
        "answer_id": 3485655941,
        "content": "全新的神经网络架构Kolmogorov–Arnold Networks（KAN）由来自MIT、加州理工、东北大学等机构的研究人员开发，它提出了一种与标准神经网络不同的架构，特别是在处理物理和数学问题时展现出了显著的优势。以下是KAN网络架构的一些关键特点和优势：1. 理论基础：KAN的设计灵感来源于Kolmogorov-Arnold表示定理，该定理表明多变量连续函数可以用单变量连续函数的有限组合表示。2. 结构创新：在KAN中，激活函数是可学习的，并且放置在网络的边缘（权重）上，而不是传统MLP中在节点（神经元）上使用固定的激活函数。3. 参数效率：KAN在实现相同性能的情况下，所需的参数数量远少于MLP，这使得KAN在参数效率上有显著优势。4. 性能：实验结果显示，KAN在数据拟合和偏微分方程（PDE）求解方面，比MLP具有更高的准确性。5. 可解释性：KAN提供了更好的可视化和交互性，有助于科学家发现新的数学和物理规律，增强了模型的可解释性。6. 持续学习：KAN展示了在持续学习中的优势，能够避免灾难性遗忘问题，这是传统MLP面临的一个重要挑战。7. 科学应用：KAN在数学的纽结理论和物理学中的Anderson局域化现象等领域展示了其潜在的应用价值。尽管KAN在多个方面展现出了潜力，但它是否能够取代MLP还需要更多的研究和实践来验证。MLP作为深度学习领域长期的基础架构，具有成熟的理论和广泛的应用基础。KAN作为一种新兴的架构，尽管在特定领域表现出色，但在通用性、训练效率、以及大规模应用方面是否能够超越MLP，还需要进一步的探索和优化。目前，KAN的爆火显示了学术界对于创新神经网络架构的渴望，以及对于提高模型性能、效率和可解释性的关注。随着进一步的研究和发展，KAN或其衍生架构有潜力在特定的机器学习和人工智能应用中发挥重要作用。",
        "voteup_count": 5,
        "updated_time": "2024-05-02 19:11:46",
        "question_id": 654782350,
        "user_id": "bfd9d54b7de2d7f65623ea9713d894a4"
    },
    {
        "answer_id": 3485917784,
        "content": "KAN (Kolmogorov-Arnold Network)作为一种全新的神经网络架构,的确在学术界引起了广泛关注。它有几个有趣的特点和潜在优势:1. KAN受Kolmogorov-Arnold表示定理的启发,该定理指出任何连续函数都可以表示为一系列一维函数的叠加。KAN试图利用这一思想,用一系列可学习的一维函数来拟合高维非线性映射。2. 与传统MLP将激活函数放在神经元上不同,KAN将可学习的激活函数放在权重矩阵上。这种架构上的改变可能带来更强的表达能力和灵活性。3. KAN中权重矩阵的参数化形式允许网络自适应地学习特定任务所需的激活函数,而无需像MLP那样预先确定激活函数的具体形式(如ReLU、sigmoid等)。这可能有利于自动搜索出更适合的激活函数。4. 一些实验结果表明,在图像分类、语言建模等任务上,KAN能够取得与SOTA模型相媲美的性能,参数量和计算量还更小。这展现了KAN的潜力。不过,我认为要判断KAN能否取代MLP还为时尚早:1. KAN提出的时间还很短,在更广泛的任务和数据集上的表现还有待更全面的评估验证。目前的实验还不足以下定论。2. MLP是一个成熟的通用架构,在工业界已得到大规模应用,积累了丰富的改进经验。KAN要想完全取代MLP,除了理论优势,还需在工程实践中证明其鲁棒性、可扩展性、易用性等。3. KAN本身还有一些局限,比如对高维输入的计算效率问题。后续还需要在架构和训练上做更多优化。4. 即使KAN最终不能完全取代MLP,其引入的一些新颖思路(如将激活函数与权重绑定)也可能被吸收到未来的其他架构中,推动神经网络架构的持续演进。总的来说,KAN是一个很有潜力的全新架构,但能否取代MLP还需要更长时间的发展和验证。无论如何,KAN的出现都为深度学习注入了新的活力,其思路很可能会启发更多的后续工作。让我们拭目以待。",
        "voteup_count": 0,
        "updated_time": "2024-05-03 01:59:42",
        "question_id": 654782350,
        "user_id": "7682bb4cf3288c9f14ab87f8e6e90870"
    },
    {
        "answer_id": 3485724004,
        "content": "mlp之所以经久不衰，就是因为他简单，快速，能scale-up。KAN让人想起来之前的Neural ODE，催生出来比如LTC（liquid time constant）网络这种宣称19个神经元做自动驾驶。（当然只是名字噱头，其实只是自动驾驶最简单的车道保持任务）再广一点，不管是我们熟知的ANN（人工神经网络），还是BNN（贝叶斯神经网络），SNN（脉冲神经网络），GNN（图神经网络）。我觉得对我们这种科研狗来说多一个新的网络绝对不是坏事，因为就能在上面做（shui）好多论文（bushi。但是工业界来说，这么多年还是只有ANN发扬光大，就是因为简单，快速，能scale up。KAN和这些老前辈一样面临的问题就是怎么堆大，怎么更方便硬件调度，很显然这方面KAN是大短板。如何解决这个问题是替代mlp的核心，毕竟如果只是单论可学习激活，这个topic已经是老话题了。只不过这次KAN。yysy我觉得MIT的人真的很聪明，像之前LTC一样，KAN的写作包装是值得所有科研人学习的。nature那个例子真的是太会找了",
        "voteup_count": 165,
        "updated_time": "2024-05-02 20:55:38",
        "question_id": 654782350,
        "user_id": "d66e9eacd12ccb0beb70e68dc7c6fa6d"
    },
    {
        "answer_id": 3485818375,
        "content": "实际上主体部分可以用矩阵乘做，不会很慢。flops打满还是问题不大的。简单写了个高效实现，能拟合数学函数，还没有做正经测试。坑点在README里有简单提及。https://github.com/Blealtan/efficient-kan如果有必要的话再来开坑多写点东西吧，我感觉这玩意对大模型用处不大，但是对ai4science的小模型和可解释性应该会非常有用。b-spline拟合连续函数太有优势了，relu系在它面前不堪一击（）",
        "voteup_count": 102,
        "updated_time": "2024-05-02 22:55:07",
        "question_id": 654782350,
        "user_id": "070de1975285b0ba734532c84d002c8a"
    },
    {
        "answer_id": 3486166838,
        "content": "我看到新智元的标题里“轻松复现Nature封面AI数学研究“这一部分时，心里想的是， 我们学数学的上辈子是欠了dl圈的人一个亿吗， 隔三差五地就来一波。。。自从我老婆怀孕生娃月经暂停了一段时间后，我能接触的最接近月经的事情就是新智元有事没事说某种dl方法能在数学上怎么怎么大展拳脚了。老实说，我到现在还没从之前deepmind硬蹭矩阵乘法那件事里缓过来。当时看到标题，然后再看实际的paper，差点没给我气心梗。本来工作就不好找，题目也不好做，然后这边还隔三差五地来碰个瓷。心好累。",
        "voteup_count": 65,
        "updated_time": "2024-05-04 13:00:37",
        "question_id": 654782350,
        "user_id": "6f2a1765b7b406d262de8d3002af0d3e"
    },
    {
        "answer_id": 3485866401,
        "content": "更新：展示的经验结果表明KAN在函数近似领域应该还是有用的。单个MLP确实比较容易卡在1e-4 RMSE（当然multi-stage会好很多）。这并不是fp32的问题（fp32只要实现不是太奇怪支持1e-5肯定是没问题的，理想情况应该能1e-7）。KAN能实现1e-7 RMSE，这是一个显著的优势。虽然作者的实现中每step时间是MLP的10倍，但考虑到KAN等价于向量值激活函数，应该不难优化到和同参数量的MLP每step时间接近。主要是炒作。KAN本质上就是向量值激活函数，把1个实数映射成1+m个实数，m是B-spline基函数B_i的个数。从理论上讲KAN是把权重矩阵中的元素从实数（考虑乘法，也可以看作实数到实数的映射，把x映到wx）推广为实数到实数的映射。但实际上这个映射的参数化方式是w(σ(x)+c_i*B_i(x))，其中σ是SiLU，所以也可以看作先把x提升为σ(x)和B_i(x)拼成的1+m维向量（i取1到m），再与w和wc_i拼成的向量做内积。也就是说KAN的一层相当于output=einsum('ijk,jk->i', weight, h(input))，其中input是形状为(d,)的输入特征，h(input)是形状为(d, 1+m)的激活值，weight是形状为(out_d, d, 1+m)的权重。常规MLP的一层相当于output=einsum('ij,j->i', weight, σ(input))。不难看出，KAN甚至可以被视为每一支采用不同激活函数的Inception结构。另见 @Yiping Lu 的一些考古（截图已删除，参见其回答https://www.zhihu.com/question/654774505/answer/3486199822）仔细看了一眼，1e-16指的是machine precision，需要4个stage才能nearly reach machine precision达到1e-13（Fig.5），1个stage是小于1e-4，2个stage是1e-8。虽然2个MLP就超过KAN了，但KAN给MLP标1e-4也是有一定合理性的。",
        "voteup_count": 92,
        "updated_time": "2024-05-03 17:03:49",
        "question_id": 654782350,
        "user_id": "de55655c466549f343ad2f175e593b9d"
    },
    {
        "answer_id": 3486235672,
        "content": "看到了好多大家的夸奖和批评（知乎上批评居多哈哈），受宠若惊。我设计网络和编程的时候，脑子里面想的都是数学物理的应用，所以模块化/效率等等就没有太怎么考虑，请大家多多包涵。然后也没有想到AI/ML大家这么关注。我的目标受众本来是做科学发现的群体，比较小众的。大家还是理性看待吧，什么是公众号的噱头什么真的只有自己试了才知道。欢迎大家多多尝试，探索KAN的边界在哪里，它和MLP的关系是什么，存不存在更大的框架可以包含两者。KAN/MLP肯定是各有优缺点的，看应用场景了。另外，我的默认参数都是我在文章的数学物理场景的例子中调的，不一定可以直接迁移到其它场景，可能需要仔细调调，尤其是优化部分。当然也有可能其它场景（比如大规模计算），KAN现阶段就是不如MLP合适。KAN更适合高精度和可解释的计算和科学发现。了解到大家的负面结果我也会很开心，因为能让我更好理解KAN的局限。理解大家喷喷，但也更希望大家去GitHub提提有建设性的建议。",
        "voteup_count": 126,
        "updated_time": "2024-05-03 13:09:38",
        "question_id": 654782350,
        "user_id": "34a8434ebc778731ade3734d8f291ccf"
    },
    {
        "answer_id": 3486218400,
        "content": "挺好的，会让越来越多的人意识到面向公众号做科研的危害性",
        "voteup_count": 69,
        "updated_time": "2024-05-03 12:45:26",
        "question_id": 654782350,
        "user_id": "c62e94ffec30821174e7c07a8ad6ff52"
    },
    {
        "answer_id": 3486239665,
        "content": "最近 KAN模型突然炒的火起来，但实际上我们团队在5年前就比号称nature封面的这篇更早的发表了相近工作，并奠定了核心思路。其优势是使用非线性算子（典型的是多项式或样条）可以更快的逼近任意函数，难度在于训练的算力要求过高。这个工作的本质是借鉴了MLP及SVM Kernel Function的思路。关键点有2个：1）构造算子空间，提升表征能力。把原来的MLP实数空间扩展到“实数+算子”的泛函空间，将算子/核函数视为空间中的离散元素，将MLP中的乘法扩展为算子空间的元素。2）参考泰勒展开，减小算子空间元素数量。参考泰勒展开的思路，可以使用多项式（非线性函数）累加做任意实数函数的逼近。这也就解释了为什么这类方法可以很好的应用于数学计算。关于KAN的详细解读，可以看我们团队的长文：陈巍 博士：KAN架构最全解析—最热KAN能否干掉MLP？（收录于GPT-4/ChatGPT技术与产业分析）KAN退化后就是MLP结构，是建立在MLP思路基础上的。谈不上替代，其实是新一代的MLP。但最大的训练算力问题，仍然需要攻克。相比之下，这类网络的训练算力要求增长是比MLP高几个数量级的。具体的倍数与算子空间的大小有关。核心要点：1）精度比传统MLP更高；2）训练算力要求更高，训练难度更高；3）现在看起来toy的主要原因是因为Training的算力完全跟不上；4）对AI芯片/算力的挑战巨大，目前的AI芯片Architecture普遍对这类模型支持不好；5）能否成为Main Stream Foundation架构要看有没有足够好的算力支持；6）是否做稀疏化不是难点，非线性算子的稠密inference代价可以通过结构优化方法降低，难的还是训练。我们团队在2019年发表的EDA algorithm dissertation中就定义了类似KAN层的算符隐层（具备非线性表征能力），直接将非线性函数/算子引入MLP结构，使之成为更加通用，适配范围更大的基础架构。延伸阅读陈巍：LLaMA3大模型技术精要——模型架构与训练方法详解（收录于GPT-4/ChatGPT技术与产业分析）kimi chat大模型的200万长度无损上下文可能是如何做到的？陈巍：Sora大模型技术精要万字详解——原理、关键技术、模型架构详解与应用（收录于GPT-4/ChatGPT技术与产业分析）陈巍：LLaMA-2的多模态版本架构与训练详解（收录于GPT-4/ChatGPT技术与产业分析）陈巍：AI大模型 & GPT-4技术学习与产业资源地图（上次更新于23/07/25）陈巍：GPT-4核心技术分析报告（2）——GPT-4的技术分析（收录于GPT-4/ChatGPT技术与产业分析）陈巍：GPT-4核心技术分析报告（5）——GPT-4的算力要点与芯片（收录于GPT-4/ChatGPT技术与产业分析）",
        "voteup_count": 23,
        "updated_time": "2024-05-04 01:39:51",
        "question_id": 654782350,
        "user_id": "455fe9c3762126aad1a7bf808902266a"
    },
    {
        "answer_id": 3486716784,
        "content": "期待以下网络、算法：Weighted Heuristic Attention Time-series networkScalable Activation Y-networkOntology-based User-centric Training这样就可以组成 what kan I say, manba out",
        "voteup_count": 20,
        "updated_time": "2024-05-04 00:12:19",
        "question_id": 654782350,
        "user_id": "43ef6eaa8fffe307ef2a6d44994f7bd2"
    },
    {
        "answer_id": 3486478763,
        "content": "看到这个不禁想说一句: 有木有量化的道友，挖过因子的，看看这个，眼熟不？[doge]",
        "voteup_count": 6,
        "updated_time": "2024-05-03 18:56:31",
        "question_id": 654782350,
        "user_id": "9517fc65fb05acb1933bce2ac5ecc4d0"
    },
    {
        "answer_id": 3486812967,
        "content": "讲一个改进思路吧。一个样条激活函数本质上就是使用n个2维点来插值近似一个函数。换句话说，激活函数可以等价的用二维点云刻画。不难类比最近爆火的gaussian splatting，我们可以反传梯度来自适应更新point 的split 和merge，实现自适应的函数复杂度。传统的比如基于relu的方法，亦或是KAN应该可以看作feature 都是zero-centered 的，因为他们需要设计各种normalization 方法来确保特征分布在零的周围。如果使用我这种自适应函数复杂度的样条，想必可以在远离zero center的地方照样提供非线性能力。因而在这套体系下，可以预想到，batch normalization 和 weight initialization等策略是不必要的，因为他们最初是为了解决feature shifted 的问题而设计的；而自适应函数天生地会自动在函数复杂的地方插入更多的样本点增加复杂度，相当于相对的自动引入一个bias来纠正均值偏差。当然，实际做出做的出来是另一回事。考虑到浮点数密度是不均匀的，在-1~1这个区间内密度更大精度更大，如果feature的scale太大便会损失精度。这间接说明了为啥之前的方法倾向于zero centered，变是由于机器的浮点数表示法制约的。纯数学来说，不考虑精度问题，我的思路也许是可行的？但是不好说，这么简单的idea说不定几百年前就有人做过了。也许不会完全一样，但可能会很相似，只是应用在别的领域，甚至不是ml领域罢了。反正我也不做这个方向，不妨说出来让大家思考一下。要是发现idea确实work能挂我个作者我就心满意足了，到时候就写inspired by a zhihu comment, we propose...",
        "voteup_count": 5,
        "updated_time": "2024-05-04 06:55:46",
        "question_id": 654782350,
        "user_id": "773b9041dbc3a49ddf8ef0d0fbc3a113"
    },
    {
        "answer_id": 3485883305,
        "content": "媒体宣传的力量太可怕了，KAN一天拿下了三大号其中两个的头版头条和一个的次条（上一次颠覆的架构这么牛还是mamba，听说mamba有几篇ICML收了，看来mamba已经势不可挡了）。这波造势，Github星标已经突破了2.2k。Github：https://github.com/KindXiaoming/pykan甚至已经写好了API文档：https://kindxiaoming.github.io/pykan/intro.html推上影响力也是非凡的，有接近60W的浏览，近900的转帖。咱们再来看看远处的mamba，从去年12月到现在也才79W浏览，400多转帖。华为的刘群老师认为这个想法很insightful。不知道是不是很快会出现各种KAN的变体，建议结合Transformer的标题可以来一篇论文的title就叫”KAN KAN Need“（doge。另外，发现好多学物理的大佬搞AI这真的是降维打击（数不过来了）。",
        "voteup_count": 34,
        "updated_time": "2024-05-03 02:15:02",
        "question_id": 654782350,
        "user_id": "3822721cffa3715db62037c499353ccd"
    },
    {
        "answer_id": 3485782154,
        "content": "论文链接：https://arxiv.org/pdf/2404.19756关于论文的介绍，推荐 @量子位 的文章介绍，包括了一些研究人员的看法、文章概要以及实验，比较全面。量子位：全新神经网络架构KAN一夜爆火！200参数顶30万，MIT华人一作，轻松复现Nature封面AI数学研究KAN与MLP在架构上的不同，在于采用了可学习的非线性激活函数而不是可学习的线性变换。从文章的初步实验来看，有以下优势：精度更高：在一些toy datasets、求解PDE、某些小规模科学问题（文章中复现了DeepMind发表在Nature的结果）具有更高的精度。对于规模不大、精度要求高的任务，不妨尝试使用KAN代替MLP，冲击一波SOTA。参数更少：文章中复现了DeepMind发表在Nature的结果，MLP要用大约300000 个参数，而KAN大约要200 个参数。有望解决灾难性遗忘问题、缩放速度更快：显然，这都是指向大模型的。文章中使用了简单的case来说明KAN的这两点优势。可惜没能看到直接应用在大语言模型上的效果。如果能够结合最近发布的Llama3进行验证，感觉会是一个很有意义的工作。缺点：训练时间长：作者在文章中表明，KAN通常比MLP慢10倍。这里比较好奇的是居然是一个常数倍数关系。而且作者表示自己没有像一个engineer一样尽力去优化。不过，如果只是慢10倍的话，考虑到更快的缩放速度以及更少的模型权重参数，应用于LLM中也未尝不可。就当前KAN发展阶段来看，在精度要求高或者模型规模小的时候，可以尝试使用KAN代替MLP，是否有潜力取代MLP，得看最终实践效果。--------------------------摘选一些网友的意见评论：看完了这些例子，优化器是 LBFGS 的原因是什么？看到 Adam 在 pykan 中也有用，但在例子中没有用到，它也能用吗？作者回复：在高精度的情况下，LBFGS 优于 Adam（因为它是准二阶和线搜索）。但如果用户不执着于高精度，Adam 也可以很好地发挥作用。评论：我认为这是一个不公平的比较。你可以将更复杂的模型在拓扑上压缩为相同大小的架构。但是你隐藏了任意多个权重，而 MLP 包含单独的权重。（赞同这个观点，KAN中单个神经元的权重更多了，训练也更加困难。同时，堆叠KAN层的时候，这些参数将会指数级）评论：你在论文中提到，与 MLP 相比，训练速度慢约 10 倍 - 这种额外的架构复杂性是否会影响训练稳定性？如果我们将许多 KAN 层堆叠在一起，网络在训练过程中是否总是会收敛？（训练稳定问题需要搭建深层KAN进行实验探究，如果不稳定，增加残差连接或者dense Net的方式会不会解决问题？）评论：我有一个很深层次的问题，我们可以在 transformer 架构中使用这个 KANs 算法，并在 Q、K、V 或输出线性层中应用这个可学习函数。这会影响(+) ive 方向或(-) ive 方向的准确性吗？（没太看懂这个(+) ive direction是什么意思。不过尝试使用可学习的非线性激活函数代替  确实是个有意思的想法，KAFormer什么的，感觉很快会有了吧）评论：KAN在训练过程中的反向梯度传播是怎么实现的呢？网友回复：单变量函数（可学习函数，可有效充当网络中的权重和激活函数）。与 MLP 类似，误差梯度在输出处计算并通过网络向后传播，但不是更新权重，而是使用它来调整每层单变量函数的参数。网友回复：有啥区别？除非它具有另一个参数，否则如何更新单变量函数——这不是与权重完全相同的东西吗？评论：没太明白为什么KAN能克服灾难性遗忘。能解释下嘛？灾难性遗忘问题指的是在task 1中训练好的神经网络模型，然后送到task 2中训练，则该网络会迅速遗忘task 1中的知识。从类比人脑的角度去想，文章说到：人工神经网络和人类大脑之间的一个关键区别在于，人类在大脑空间局部放置了功能不同的模块。 当学习新任务时，结构重组仅发生在负责相关技能的局部区域，而其他区域保持不变。 大多数人工神经网络，包括 MLP，没有这种局部性概念，这可能是灾难性遗忘的原因。而KAN可以解决拥有这种局部放置不同模块的能力，这是因为：样条基数是局部的，因此样本只会影响一些附近的样条系数，而使远处的系数保持不变（这是可取的，因为远处的区域可能已经存储了我们想要保留的信息）。文章里做了这样的一个实验：一维回归任务由 5 个高斯峰组成。 每个峰周围的数据按顺序（而不是一次全部）呈现给 KAN 和 MLP，如图 3.4 顶行所示。 每个训练阶段后的 KAN 和 MLP 预测显示在中间和底部行中。 正如预期的那样，KAN 仅重构当前阶段存在数据的区域，而使之前的区域保持不变。 相比之下，MLP 在看到新的数据样本后会重塑整个区域，从而导致灾难性的遗忘。",
        "voteup_count": 28,
        "updated_time": "2024-05-03 15:54:35",
        "question_id": 654782350,
        "user_id": "d2d2b75a434bab27f88a991084485e42"
    },
    {
        "answer_id": 3485932943,
        "content": "笑点解析：toy dataset。好歹跑个MNIST数据集给大伙看看比MLP高多少啊。",
        "voteup_count": 98,
        "updated_time": "2024-05-03 03:09:40",
        "question_id": 654782350,
        "user_id": "279d5560809d4fbc6ba06c1ba942f0b8"
    },
    {
        "answer_id": 3486022922,
        "content": "COLT: Conference of Langeng in Tieba:KAN KAN NEED: two KANs are all you NEED to replace one transformer layer名字出处：如何评价全新的神经网络架构KAN的爆火，是否有潜力取代MLP？",
        "voteup_count": 23,
        "updated_time": "2024-05-03 08:42:55",
        "question_id": 654782350,
        "user_id": "10c387ed0815758d03d320568ee51efd"
    },
    {
        "answer_id": 3486294965,
        "content": "来源 | 量子位  ID | QbitAI一种全新的神经网络架构KAN，诞生了！与传统的MLP架构截然不同，且能用更少的参数在数学、物理问题上取得更高精度。比如，200个参数的KANs，就能复现DeepMind用30万参数的MLPs发现数学定理研究。不仅准确性更高，并且还发现了新的公式。要知道后者可是登上Nature封面的研究啊~在函数拟合、偏微分方程求解，甚至处理凝聚态物理方面的任务都比MLP效果要好。而在大模型问题的解决上，KAN天然就能规避掉灾难性遗忘问题，并且注入人类的习惯偏差或领域知识非常容易。来自MIT、加州理工学院、东北大学等团队的研究一出，瞬间引爆一整个科技圈：Yes We KAN！甚至直接引出关于能否替代掉Transformer的MLP层的探讨，有人已经准备开始尝试……有网友表示：这看起来像是机器学习的下一步。让机器学习每个特定神经元的最佳激活，而不是由我们人类决定使用什么激活函数。还有人表示：可能正处于某些历史发展的中间。GitHub上也已经开源，也就短短两三天时间就收获1.1kStar。对MLP“进行一个简单的更改”跟MLP最大、也是最为直观的不同就是，MLP激活函数是在神经元上，而KAN把可学习的激活函数放在权重上。在作者看来，这是一个“简单的更改”。从数学定理方面来看，MLP的灵感来自于通用近似定理，即对于任意一个连续函数，都可以用一个足够深的神经网络来近似。而KAN则是来自于 Kolmogorov-Arnold 表示定理 (KART)，每个多元连续函数都可以表示为单变量连续函数的两层嵌套叠加。KAN的名字也由此而来。正是受到这一定理的启发，研究人员用神经网络将Kolmogorov-Arnold 表示参数化。为了纪念两位伟大的已故数学家Andrey Kolmogorov和Vladimir Arnold，我们称其为科尔莫格罗夫-阿诺德网络（KANs）。而从算法层面上看，MLPs 在神经元上具有（通常是固定的）激活函数，而 KANs 在权重上具有（可学习的）激活函数。这些一维激活函数被参数化为样条曲线。在实际应用过程中，KAN可以直观地可视化，提供MLP无法提供的可解释性和交互性。不过，KAN的缺点就是训练速度较慢。对于训练速度慢的问题，MIT博士生一作Ziming Liu解释道，主要有两个方面的原因。一个是技术原因，可学习的激活函数评估成本比固定激活函数成本更高。另一个则是主观原因，因为体内物理学家属性抑制程序员的个性，因此没有去尝试优化效率。对于是否能适配Transformer，他表示：暂时不知道如何做到这一点。以及对GPU友好吗？他表示：还没有，正在努力中。天然能解决大模型灾难性遗忘再来看看KAN的具体实现效果。神经缩放规律：KAN 的缩放速度比 MLP 快得多。除了数学上以Kolmogorov-Arnold 表示定理为基础，KAN缩放指数也可以通过经验来实现。在函数拟合方面，KAN比MLP更准确。而在偏微分方程求解，比如求解泊松方程，KAN比MLP更准确。研究人员还有个意外发现，就是KAN不会像MLP那样容易灾难性遗忘，它天然就可以规避这个缺陷。好好好，大模型的遗忘问题从源头就能解决。在可解释方面，KAN能通过符号公式揭示合成数据集的组成结构和变量依赖性。人类用户可以与 KANs 交互，使其更具可解释性。在 KAN 中注入人类的归纳偏差或领域知识非常容易。研究人员利用KANs还重新复现了DeepMind当年登上Nature的结果，并且还找到了Knot理论中新的公式，并以无监督的方式发现了新的结不变式关系。△DeepMind登Nature研究成果Deepmind的MLP大约300000 个参数，而KAN大约只有200 个参数。KAN 可以立即进行解释，而 MLP 则需要进行特征归因的后期分析。并且准确性也更高。对于计算要求，团队表示论文中的所有例子都可以在单个CPU上10分钟内重现。虽然KAN所能处理的问题规模比许多机器学习任务要小，但对于科学相关任务来说就刚刚好。比如研究凝固态物理中的一种相变：安德森局域化。好了，那么KAN是否会取代Transformer中的MLP层呢？有网友表示，这取决于两个因素。一点是学习算法，如 SGD、AdamW、Sophia 等—能否找到适合 KANs 参数的局部最小值？另一点则是能否在GPU上高效地实现KANs层，最好能比MLPs跟快。最后，论文中还贴心的给出了“何时该选用KAN？”的决策树。那么，你会开始尝试用KAN吗？还是让子弹再飞一会儿~项目链接：https://kindxiaoming.github.io/pykan/论文链接：https://arxiv.org/abs/2404.19756参考链接：[1]https://twitter.com/ZimingLiu11/status/1785483967719981538[2]https://twitter.com/AnthropicAI/status/1785701418546180326",
        "voteup_count": 5,
        "updated_time": "2024-05-03 14:33:13",
        "question_id": 654782350,
        "user_id": "b6384cfe8c9cf5cb7c310dd652363232"
    },
    {
        "answer_id": 3486895065,
        "content": "这篇论文给我印象最深的一句话：The language of science is functions. 根据问题特点，设计解决方案。专才优于通才。分工合作，合作共赢。KAN甚至不一定需要考虑在NLP、CV上面的性能。Kolmogorov-Arnold Networks (KAN)随想https://zhuanlan.zhihu.com/p/695856512",
        "voteup_count": 3,
        "updated_time": "2024-05-04 09:16:00",
        "question_id": 654782350,
        "user_id": "d7eaac230ba5121925d07b169ed8760d"
    },
    {
        "answer_id": 3486505101,
        "content": "MLP简单有效也许有一天CNN会被抛弃，RNN会被抛弃（现在确实用的偏少了），甚至transformer也会被抛弃，但MLP真的很难被取代。交给时间吧，伟大的工作会得到时间检验奖",
        "voteup_count": 9,
        "updated_time": "2024-05-03 19:34:33",
        "question_id": 654782350,
        "user_id": "1616749f22b3fe1cc92ebcce9b3b6bdd"
    },
    {
        "answer_id": 3485608211,
        "content": "想象你有一个复杂的拼图，需要将其拼完整。传统的拼图是由许多小块组成的，而每块都有一个特定的形状，需要与周围的块恰好契合。而KANs就像是一个特殊的拼图，可以通过学习的方式自我调整每个块的形状和位置，使其与整体更加吻合。那么，如何理解MLP与KANs之间的区别呢？MLP的拼图：MLP的拼图有一个固定的规则，每一层都像是给拼图块添加了一层统一的涂层，使其在某种程度上都能拼接起来。虽然这种方式能够完成拼图，但在面对一些复杂的形状时，它可能需要更多的拼图块才能完成。KAN的拼图：KAN的拼图则是灵活且动态的。每一块拼图都可以根据其他块的情况进行自我调整。每一层就像是对每一块拼图进行单独的雕刻，使其与其他块完美契合。KANs的这种灵活性使得它们在完成拼图的过程中具有一些天然的好处：（1）更准确：由于KANs可以对每一块拼图单独雕刻，它们在面临复杂的数据集或问题时能够提供更准确的解决方案。（2）更快速：因为KANs通过对拼图块进行调整和雕刻，使得整体结构更加协调，因此可以用较少的块数完成拼图，提高了模型的效率。（3）可解释性：KANs就像是一个透明的拼图，让我们能够看到每一块的形状是如何调整的，从而更好地理解模型的工作原理。不过，KANs也有一些缺点有待进一步的改进。首先是雕刻的时间较长：KANs需要花费更多的时间来雕刻每一块拼图，尤其是当拼图块的数量很多时，这个过程会非常耗时。这反映了KANs在训练效率上的不足，需要一些算法上的改进，比如批量化的计算，来加速训练。其次，对不同风格的拼图的适应性有待验证，KANs擅长处理一些特定风格的拼图（如科学计算问题），但对于风格迥异的其他类型的拼图（如语言理解任务）表现如何，还需要更多的实践来检验。",
        "voteup_count": 36,
        "updated_time": "2024-05-03 09:32:55",
        "question_id": 654782350,
        "user_id": "b9ac75fda4638d8013476b9855039c65"
    },
    {
        "answer_id": 3486927106,
        "content": "目前来看，得出结论还为时过早。MLP和KAN的主要区别在于其基础理论不相同，因此优化手段也不一样。KAN暂时还没有找到合适的高效优化手段，而如果想要大规模应用，就需要有人能证明KAN在实际应用中的效果。可是现在效率就制约了KAN的实用性。看来这是个鸡生蛋，蛋生鸡的问题，不知道谁愿意下场投入资源先吃螃蟹。就论文展现出的结果看，甚至还达不到Mamba所表现出的前景效果，所以个人谨慎乐观看待。",
        "voteup_count": 0,
        "updated_time": "2024-05-04 09:53:59",
        "question_id": 654782350,
        "user_id": "5f66ee7ac2f6ea2db4cddbf350d55329"
    },
    {
        "answer_id": 3486673177,
        "content": "感兴趣可以阅读如下文章：求索：深度学习颠覆性论文发布：KANs要取代MLP吗？最近麻省理工、加州理工等学校研究员发表一篇替代多层感知机MLP的论文《KAN: Kolmogorov–Arnold Networks》。论文摘要论文提出了一种新型神经网络架构——Kolmogorov-Arnold Networks（KANs）（为了纪念两位伟大的已故数学家安德烈·科尔莫戈罗夫和弗拉基米尔·阿诺德，我们称他们为科尔莫戈罗夫-阿诺德网络），它受到Kolmogorov-Arnold表示定理的启发，目标是作为多层感知器（MLPs）的替代品。KANs的特点是将激活函数置于网络的边缘（权重），而不是传统的节点上，并且这些激活函数是可学习的，由样条函数参数化。论文开发了KANs的实现代码，并通过GitHub和pip安装包分享给研究社区，促进了进一步的研究和开发。KANs解决了MLPs在非线性回归、数据拟合、偏微分方程求解以及科学发现中的一些限制，如固定激活函数的局限性、参数效率低、可解释性差等。KANs通过网格扩展技术提高准确性，即通过细化样条函数的网格来提高逼近目标函数的精度。引入简化技术，包括稀疏化、可视化、剪枝和符号化，以提高KANs的可解释性。论文核心内容MLP 是如此基础，但还有其他选择吗？MLP 将激活函数放在神经元上，但我们是否可以将（可学习的）激活函数放在权重上？是的，KAN可以！作者提出了 Kolmogorov-Arnold 网络 (KAN)，它比 MLP 更准确、更易于解释。KANs对 MLP 进行简单的更改：将激活函数从节点（神经元）移到边缘（权重）！KANs通过将每个权重参数替换为一个一元函数，利用样条函数来近似这些一元函数。这个变化乍一听似乎有些奇怪，但其实它与数学中的近似理论有着很深的联系。事实证明，Kolmogorov-Arnold 表示对应于 2 层网络，其 (可学习) 激活函数位于边上而不是节点上。从数学角度来看：MLP 受到通用近似定理 (UAT) 的启发，而 KAN 受到柯尔莫哥洛夫-阿诺德表示定理 (KART) 的启发。网络能否以固定宽度实现无限精度？UAT 的答案是“不”，而 KART 的答案是“可以”（但有警告）。从算法方面来看：KAN 和 MLP 是双重的，因为-- MLP 对神经元具有（通常固定的）激活函数，而 KAN 对权重具有（可学习的）激活函数。这些 1D 激活函数被参数化为样条函数。从实际角度来看：作者发现 KAN 比 MLP 更准确、更易于解释，尽管由于 KAN 的激活函数可学习，因此训练速度较慢。KANs在准确性上超越了MLPs，尤其是在数据拟合和PDE求解任务中，展示了更快的神经缩放法则。KAN 的缩放速度比 MLP 快得多，这在数学上基于 Kolmogorov-Arnold 表示定理。KAN 的缩放指数也可通过经验获得。KAN 在函数拟合方面比 MLP 更准确，例如拟合特殊函数。KAN在PDE求解任务上比MLP更快更准确，例如求解泊松方程。1另外，KAN 具有避免灾难性遗忘的天然能力。KANs提供了更好的可解释性，使得网络结构和激活函数可以直观地被理解和解释。KAN可以从符号公式中揭示合成数据集的组成结构和可变依赖性。人类用户可以与 KAN 交互，使其更易于解释。将人类的归纳偏见或领域知识注入 KAN 很容易。论文还探讨了KANs在科学发现中的潜力，KAN 也是科学家的得力助手或合作者。如在数学的结理论和物理的Anderson局域化中的应用。利用 KAN 重新发现了结理论中的数学定律。KAN 不仅以更小的网络和更高的自动化程度重现了GoogleDeepmind的结果，还发现了新的签名公式，并以无监督的方式发现了结不变量的新关系。特别是，Deepmind的MLP有~300000个参数，而KAN只有~200个参数。KAN可以立即解释，而MLP需要特征归因作为后期分析。KAN 可以帮助研究 Anderson 局域化，这是凝聚态物理学中的一种相变。无论是从数值上，还是从物理上，KAN 都使迁移率边缘的提取变得非常简单。根据实证结果，KAN 将成为 AI + Science 的有用模型/工具，因为它具有准确性、参数效率和可解释性。KAN 对机器学习相关任务的实用性更具推测性，有待未来研究。论文中的所有示例都可以在单个 CPU 上在不到 10 分钟的时间内重现（扫描超参数除外）。诚然，实验的问题规模比许多机器学习任务要小，但对于与科学相关的任务来说却是典型的。真的能替代MLP吗？作者的回应：感谢大家的欢呼和建设性批评。我写了几段文字来回应最近的 KAN 炒作。简而言之，我认为现在说KAN将取代MLP 还为时过早，但确实有很多有趣的方向值得探索。为什么训练很慢？原因 1：技术。可学习激活函数（样条函数）比固定激活函数的评估成本更高。原因 2：个人原因。研究作者的物理学家性格会抑制程序员性格，所以没有尝试优化效率。能适适配transformers吗？研究作者不知道该怎么做，虽然一个简单的（但可能有效！）扩展只是用 KAN 取代 MLP。paper：https://arxiv.org/abs/2404.19756code：https://github.com/KindXiaoming/pykandocumentation：https://kindxiaoming.github.io/pykan/",
        "voteup_count": 1,
        "updated_time": "2024-05-03 23:18:00",
        "question_id": 654782350,
        "user_id": "17674eb41cf5ef88cdc1e69d5169a8d7"
    },
    {
        "answer_id": 3486265152,
        "content": "一种全新的神经网络架构KAN，诞生了！与传统的MLP架构截然不同，且能用更少的参数在数学、物理问题上取得更高精度。比如，200个参数的KANs，就能复现DeepMind用30万参数的MLPs发现数学定理研究。不仅准确性更高，并且还发现了新的公式。要知道后者可是登上Nature封面的研究啊~在函数拟合、偏微分方程求解，甚至处理凝聚态物理方面的任务都比MLP效果要好。而在大模型问题的解决上，KAN天然就能规避掉灾难性遗忘问题，并且注入人类的习惯偏差或领域知识非常容易。来自MIT、加州理工学院、东北大学等团队的研究一出，瞬间引爆一整个科技圈：Yes We KAN！甚至直接引出关于能否替代掉Transformer的MLP层的探讨，有人已经准备开始尝试……有网友表示：这看起来像是机器学习的下一步。让机器学习每个特定神经元的最佳激活，而不是由我们人类决定使用什么激活函数。还有人表示：可能正处于某些历史发展的中间。GitHub上也已经开源，也就短短两三天时间就收获1.1kStar。对MLP“进行一个简单的更改”跟MLP最大、也是最为直观的不同就是，MLP激活函数是在神经元上，而KAN把可学习的激活函数放在权重上。在作者看来，这是一个“简单的更改”。从数学定理方面来看，MLP的灵感来自于通用近似定理，即对于任意一个连续函数，都可以用一个足够深的神经网络来近似。而KAN则是来自于 Kolmogorov-Arnold 表示定理 (KART)，每个多元连续函数都可以表示为单变量连续函数的两层嵌套叠加。KAN的名字也由此而来。正是受到这一定理的启发，研究人员用神经网络将Kolmogorov-Arnold 表示参数化。为了纪念两位伟大的已故数学家Andrey Kolmogorov和Vladimir Arnold，我们称其为科尔莫格罗夫-阿诺德网络（KANs）。而从算法层面上看，MLPs 在神经元上具有（通常是固定的）激活函数，而 KANs 在权重上具有（可学习的）激活函数。这些一维激活函数被参数化为样条曲线。在实际应用过程中，KAN可以直观地可视化，提供MLP无法提供的可解释性和交互性。不过，KAN的缺点就是训练速度较慢。对于训练速度慢的问题，MIT博士生一作Ziming Liu解释道，主要有两个方面的原因。一个是技术原因，可学习的激活函数评估成本比固定激活函数成本更高。另一个则是主观原因，因为体内物理学家属性抑制程序员的个性，因此没有去尝试优化效率。对于是否能适配Transformer，他表示：暂时不知道如何做到这一点。以及对GPU友好吗？他表示：还没有，正在努力中。天然能解决大模型灾难性遗忘再来看看KAN的具体实现效果。神经缩放规律：KAN 的缩放速度比 MLP 快得多。除了数学上以Kolmogorov-Arnold 表示定理为基础，KAN缩放指数也可以通过经验来实现。在函数拟合方面，KAN比MLP更准确。而在偏微分方程求解，比如求解泊松方程，KAN比MLP更准确。研究人员还有个意外发现，就是KAN不会像MLP那样容易灾难性遗忘，它天然就可以规避这个缺陷。好好好，大模型的遗忘问题从源头就能解决。在可解释方面，KAN能通过符号公式揭示合成数据集的组成结构和变量依赖性。人类用户可以与 KANs 交互，使其更具可解释性。在 KAN 中注入人类的归纳偏差或领域知识非常容易。研究人员利用KANs还重新复现了DeepMind当年登上Nature的结果，并且还找到了Knot理论中新的公式，并以无监督的方式发现了新的结不变式关系。△DeepMind登Nature研究成果Deepmind的MLP大约300000 个参数，而KAN大约只有200 个参数。KAN 可以立即进行解释，而 MLP 则需要进行特征归因的后期分析。并且准确性也更高。对于计算要求，团队表示论文中的所有例子都可以在单个CPU上10分钟内重现。虽然KAN所能处理的问题规模比许多机器学习任务要小，但对于科学相关任务来说就刚刚好。比如研究凝固态物理中的一种相变：安德森局域化。好了，那么KAN是否会取代Transformer中的MLP层呢？有网友表示，这取决于两个因素。一点是学习算法，如 SGD、AdamW、Sophia 等—能否找到适合 KANs 参数的局部最小值？另一点则是能否在GPU上高效地实现KANs层，最好能比MLPs跟快。最后，论文中还贴心的给出了“何时该选用KAN？”的决策树。那么，你会开始尝试用KAN吗？还是让子弹再飞一会儿~来源：量子位",
        "voteup_count": 3,
        "updated_time": "2024-05-03 13:49:37",
        "question_id": 654782350,
        "user_id": "dd0cf3f9b8809033760b59336b599f6d"
    },
    {
        "answer_id": 3486722354,
        "content": "在MNIST上简单尝试了一下参数和显存都翻倍，收敛变慢，结果变差所以KAN的优势在哪里 ",
        "voteup_count": 6,
        "updated_time": "2024-05-04 00:32:48",
        "question_id": 654782350,
        "user_id": "20baf126484c11c9ed269fe22be93e8f"
    },
    {
        "answer_id": 3486284731,
        "content": "谢邀。作为一个关注AI前沿动态的深度学习从业者,我对 KAN 的出现感到非常兴奋。这里我谈几点自己的拙见,希望能给题主一些启发:1. KAN 的创新点在哪里?KAN 最大的特点就是把激活函数从神经元挪到了权重上,使得每个权重都有自己独立的、可学习的激活函数。这个idea 的灵感来自于 Kolmogorov-Arnold 表示定理。通过给予权重更大的灵活性,KAN 希望能学到更强的特征表达能力。2. KAN 是否有潜力成为\"下一个 MLP\"?从 KAN 刚发布就吸引了大量目光这一点来看,它确实展现出了巨大的潜力。一些实验结果表明,KAN 在特定任务上已经性能超群。但我认为,KAN 要完全取代 MLP 还尚需时日:首先 KAN 还是一个非常年轻的架构,它的实际效果还有待更全面的实验验证;其次从工程角度看,现有的深度学习框架和芯片都是围绕 MLP 设计的,迁移到 KAN 并非易事;最后 MLP 作为一个真正经受住考验的模型,其鲁棒性和泛化性能是 KAN 暂时难以企及的。3. 后续还有哪些值得关注的研究方向?针对 KAN,我认为有几个方向值得深入探索:(1)借鉴 Kolmogorov-Arnold 定理的思路,是否可以把激活函数\"下放\"到更细粒度,比如每个输入到权重的连接上;(2)KAN 的权重激活函数是否可以引入更多的先验知识(比如 Lipschitz 约束),来提升训练稳定性;(3)在 KAN 中再引入一些 MLP 的优秀特性,比如 skip connection,也许能实现 1+1>2 的效果。",
        "voteup_count": 4,
        "updated_time": "2024-05-03 14:18:23",
        "question_id": 654782350,
        "user_id": "4b77088edcd07e1c5d062d49d54b6a76"
    },
    {
        "answer_id": 3486209232,
        "content": "五一期间，新神经网络架构KAN发布，在性能和可解释性上超越MLP架构，但目前训练时间较长。深度学习模型(推动人工智能革命的神秘引擎)不再笼罩在神秘之中。如果我们能够窥视它们的内部运作，理解它们的推理，甚至与它们合作揭开宇宙的秘密，那会怎样？这就是柯尔莫哥洛夫-阿诺德网络(KAN)的承诺，这是一种革命性的新架构，有望改变人工智能的格局。多层感知器(MLP），目前深度学习的主力架构的黑箱性质，阻碍了可解释性，低效率限制了你们的潜力，在高维数据方面的挣扎让广阔的知识领域尚未被探索。现在是时候出现一种新型的神经网络了，它将深度学习的力量与数学的优雅和人类理解的透明性结合起来MLP的核心问题在于其结构。虽然它们的通用近似能力已经得到充分证实，但它们在节点上的固定激活函数和对线性变换的依赖限制了它们有效表示复杂函数的能力，尤其是那些具有组合结构的函数。这种低效率导致模型更大，计算成本增加，并且阻碍了可解释性，因为理解其预测背后的原因变得具有挑战性。此外，MLP 经常受到数据维数灾难的影响，随着输入数据维数的增加，它们的性能会下降。KAN借鉴了柯尔莫哥洛夫-阿诺德表示定理，解决了这些痛点。柯尔莫哥洛夫-阿诺德表示定理指出，任何连续多变量函数都可以分解为单变量函数和加法的组合。KAN不使用节点上的固定激活函数，而是在边缘上使用可学习的激活函数,用样条函数表示。这一关键区别使KAN能够有效地学习函数的组合结构以及该组合中的各个函数。与MLP相比，KAN实现了更高的准确率尤其是在处理高维数据和复杂函数时。此外，KAN在可解释性方面具有显著优势。其结构允许直观地可视化所学习到的函数，从而深入了解模型的决策过程。此外,本文介绍了在不牺牲准确性的情况下简化KAN的技术，进一步提高了其透明度。",
        "voteup_count": 4,
        "updated_time": "2024-05-03 12:32:19",
        "question_id": 654782350,
        "user_id": "cb7237a0b23ea5e3dc80ea2ccb8463b9"
    },
    {
        "answer_id": 3485703777,
        "content": "科尔莫戈洛夫-阿诺德表示定理本质上是一个特殊情况的隐函数定理在一个点，以及一个不等式估计。根本没有用到全空间的刚性性质，没办法对全空间进行很好的编码勿cue Arnold，这套东西对机器学习没有帮助，比trransformer架构弱得多。",
        "voteup_count": 42,
        "updated_time": "2024-05-02 20:23:04",
        "question_id": 654782350,
        "user_id": "15a9f8b1a3ddcb9514a56ddd669846fd"
    },
    {
        "answer_id": 3486311434,
        "content": "全新的神经网络架构KAN因其独特的设计和潜在的优势而受到关注。根据提供的资料，KAN的核心特点是将可学习的激活函数置于网络的边缘（即权重）上，而不是传统MLP中的节点（神经元）上。这种设计允许每个权重参数被一个作为样条（spline）参数化的一元函数所替代，从而为网络提供了更高的灵活性和表达能力。KAN在准确性和可解释性方面展现出了相较于MLP的优势。论文中通过实验表明，KAN在数据拟合和偏微分方程（PDE）求解方面比MLP具有更高的准确性，并且参数效率更高。此外，KAN的可解释性也得到了提升，因为它可以直观地可视化，并且可以与人类用户轻松交互。然而，KAN的训练速度较慢是一个已知的问题。在相同数量的参数下，KAN的训练耗时通常是MLP的10倍。尽管如此，研究者认为这更像是一个未来可以改进的工程问题，而不是一个根本性的限制。关于KAN是否有潜力取代MLP，这还需要更多的研究和实践来验证。目前，KAN在特定领域（如科学计算）中显示出了巨大潜力，但是否能够广泛应用于其他领域还有待观察。此外，社区对于将KAN应用于更大规模问题以及其在GPU上的实现效率也表现出了兴趣。KAN作为一种新型神经网络架构，在理论上和初步实验中表现出了显著的潜力，但要取代广泛使用的MLP，还需要在效率、应用范围和易用性等方面进行更多的探索和改进。Nvidia B100/B200/GB200 关键技术解读 - 知乎 (zhihu.com)大模型训练推理如何选择GPU？一篇文章带你走出困惑（附模型大小GPU推荐图） - 知乎 (zhihu.com)一文看懂英伟达A100、A800、H100、H800各个版本有什么区别？ - 知乎 (zhihu.com)AI核弹B200发布：超级GPU新架构30倍H100单机可训15个GPT-4模型，AI进入新摩尔时代 - 知乎 (zhihu.com)紧跟“智算中心”这波大行情！人工智能引领算力基建革命！ - 知乎 (zhihu.com)先进计算技术路线图（2023） - 知乎 (zhihu.com)建议收藏！大模型100篇必读论文 - 知乎 (zhihu.com)马斯克起诉 OpenAI：精彩程度堪比电视剧，马斯克与奥特曼、OpenAI的「爱恨纠缠史」 - 知乎 (zhihu.com)生物信息学必备网站大全 - 知乎 (zhihu.com)生物信息学简史 - 知乎 (zhihu.com2023第一性原理科研服务器、量化计算平台推荐 - 知乎 (zhihu.com)Llama-2 LLM各个版本GPU服务器的配置要求是什么？ - 知乎 (zhihu.com)人工智能训练与推理工作站、服务器、集群硬件配置推荐整理了一些深度学习，人工智能方面的资料，可以看看机器学习、深度学习和强化学习的关系和区别是什么？ - 知乎 (zhihu.com)人工智能 (Artificial Intelligence, AI)主要应用领域和三种形态：弱人工智能、强人工智能和超级人工智能。买硬件服务器划算还是租云服务器划算？ - 知乎 (zhihu.com)深度学习机器学习知识点全面总结 - 知乎 (zhihu.com)自学机器学习、深度学习、人工智能的网站看这里 - 知乎 (zhihu.com)2023年深度学习GPU服务器配置推荐参考（3） - 知乎 (zhihu.com)多年来一直专注于科学计算服务器，入围政采平台，H100、A100、H800、A800、L40、L40S、RTX6000 Ada，RTX A6000，单台双路256核心服务器等。",
        "voteup_count": 0,
        "updated_time": "2024-05-03 14:58:22",
        "question_id": 654782350,
        "user_id": "fdd23d5babd02447757c668338a4cd59"
    },
    {
        "answer_id": 3486666984,
        "content": "What KAN I say？ Mamba out！",
        "voteup_count": 24,
        "updated_time": "2024-05-03 23:04:37",
        "question_id": 654782350,
        "user_id": "91aaf0fa8e3ece5d105aa121ab9a16c6"
    },
    {
        "answer_id": 3486189092,
        "content": "01 KAN为何一夜爆火？！近日，一种突破性的神经网络架构——KAN(Kolmogorov–Arnold Networks)诞生啦！！ 机器学习范式就要变天啦？！它的设计哲学与传统的MLP(多层感知机)有着明显差异，且在使用更少的参数解决数学和物理问题上显示出了更高的精确度。举个例子，仅用200个参数的KAN就能重现DeepMind利用30万参数的MLP在发现数学定理方面的研究成果。下周奥特曼庆生，GPT-5或将惊艳亮相？Lla…GPT-5就要发布啦，还没体验过原生OpenAI Chat GPT4.0？快戳最新版升级教程，教你如何几分钟搞定：手把手升级ChatGPT4.0 Turbo详细步骤教程（2024年4月）这种新架构不仅加强了精确度，同时还揭示了未知的数学公式。而这些研究，DeepMind的成果曾被登载在Nature杂志的封面上呢~在函数逼近、求解偏微分方程，乃至处理凝聚态物理问题的能力上，KAN都展现出比MLP更出色的表现。 论文地址：https://arxiv.org/abs/2404.19756 项目地址：https://kindxiaoming.github.io/pykan/在解决大规模模型问题时，KAN能自然避免灾难性遗忘并且轻松融合人类的直觉偏好或特定领域知识。MIT、加州理工学院、东北大学等组成的研究团队的这项新研究一经发布，立即在整个科技界引发轰动：Yes We KAN！KAN与MLP在激活函数的配置上存在显著的差异，这也是二者最为直观的区别之一。传统MLP的激活函数通常位于神经元上，而KAN则创新地将可学习的激活函数直接置于权重之上。在作者眼中，这一改动看似“简单”，实则蕴含了深刻的变革。研究团队在MLP的基础上做出了一个微妙而关键的调整：他们把可以学习的激活函数从神经元（即节点）搬到了连接它们的连接点（即边）上！这一变化初听起来似无道理，然而它与数学领域的\"近似理论\"紧密相连，含义深远。实际上，根据Kolmogorov-Arnold展示理论，在两层网络中，拥有可学习激活函数的边而不是节点，确实显示了更大的潜力。研究人员受到展示定理的鼓舞，将Kolmogorov-Arnold表示在神经网络中明确参数化，具现化了其理念。而KAN这个名称，也是为了向两位杰出的数学家Andrey Kolmogorov以及Vladimir Arnold致敬，他们的贡献为这一理论提供了基础。02 KAN如何实现？2.1 理论根基柯尔莫哥洛夫-阿诺德表示定理（Kolmogorov–Arnold representation theorem）告诉我们，在有限界限内定义的任何连续的多变量函数，都能够被看做是单变量连续函数的有限叠加。在机器学习的视角下，这意味着学习高维函数可以转变为学习数量有限的一维函数。然而，这些一维函数可能在实际应用中是不光滑甚至是具有分形属性的，从而导致在实际中难以进行学习，因此这一定理在机器学习领域曾几乎被认为是不可行的——理论上恰当但实用性弱。即便如此，研究者们仍然看好该理论在机器学习中的潜力，并且提出如下两项改良措施：1、而不是局限于原始的方程提到的单一隐藏层（2n+1）以及两层非线性性质，可以将网络扩展为任意的宽度与深度；2、在科学和日常生活中，多数遇到的函数都较为光滑并含有简单的组合结构，能有利于构造更平滑的柯尔莫哥洛夫-阿诺德表示。这就如同区分物理学家与数学家所关注的焦点：物理学家倾向于探究通常状况下的典型例子，而数学家则更多地考虑极限情况。2.2 实现细节KAN网络的设计概念源于通过一系列单变量函数学习多变量函数的简化。在此框架中，各单变量函数可采用可学习参数的B样条曲线来表示。研究团队从增加网络层数以加深MLPs的建构概念中获得启示，提出KAN层概念，构成一维函数的矩阵，每个函数均带可学习参数。依据柯尔莫哥洛夫-阿诺德定理，KAN基层由两类函数组合——内部和外部，对应输入输出维度。堆叠KAN层增强了深度与表达力，保持解释性，其中每层单变量函数独立学习，易于解读。此处的f可以看作KAN的具体实现：03 KAN比MLP强在何处？在解释性方面，KAN能更好地揭示数据集背后的结构和变量依赖关系，通过符号公式提供洞见。神经网络缩放效率：与MLP相比，KAN的扩展效率更高。它不仅基于数学的柯尔莫哥洛夫-阿诺德定理，其缩放性能也可通过实验验证得出。为了证明其有效性，研究团队使用了5个已知可以平滑表达KA（柯尔莫哥洛夫-阿诺德）的案例作为测试数据集，并以每200步增加一个网格点的方式对KANs进行训练，覆盖的G值集合为{3,5,10,20,50,100,200,500,1000}。将不同尺寸的MLPs作为标准对照，并且在相同条件下，即使用LBFGS优化算法训练1800步，KANs与MLPs的性能通过RMSE（均方根误差）进行比较。在函数逼近任务上，KAN展现出了比MLP更高的精度在求解偏微分方程，如泊松方程时，KAN的表现也超越MLP一个意外的发现是，KAN天生能避免MLP常见的灾难性遗忘问题，为大型模型提供了从根本上避免遗忘的解决方案。04 小结关于KAN是否能替代Transformer中的MLP层，社区内部观点分歧。首先，关键在于学习算法——如SGD、AdamW、Sophia等——是否能为KANs参数找到有效的局部最小值其次，考虑到在GPU上实施KANs层的效率问题，理想情况下应优于MLPs的执行速度论文作者还贴心地提供了一个实用的决策树，帮助判断何时采用KAN！那么，您是否考虑尝试KAN，还是暂时观望？把论文丢给GPT-4进行撤稿预测，和人类审…GPT-4都可以预测论文撤稿啦，快戳最新版升级教程：手把手升级ChatGPT4.0 Turbo详细步骤教程（2024年4月）推荐文章：革命性神经网络架构KAN诞生！一夜爆火！…手把手升级ChatGPT4.0 Turbo详细步骤教程（2024年4月）下周奥特曼庆生，GPT-5或将惊艳亮相？Lla…",
        "voteup_count": 6,
        "updated_time": "2024-05-03 12:05:51",
        "question_id": 654782350,
        "user_id": "03d359f96a3d75c1b65f315e4e5ba0d6"
    },
    {
        "answer_id": 3487115633,
        "content": "KAN在结构上和理论上都挺有创意的。MLP激活函数是在神经元上，而KAN把可学习的激活函数放在权重上。MLP是靠万能近似定理，KAN是靠Kolmogorov-Arnold 表示定理 (KART)。这和我之前想过的“对每个权重都要使用激活函数，但不需要对节点使用激活函数”的想法有点相似，但是我当时只想了没有做。MLP多层感知机在1957年就被美国科学家Frank Rosenblatt提出了，但是这个KAN最近才提出来的，所以我认为KAN有很大的改进空间。有没有取代MLP的潜力还要过2年回来看看才知道。",
        "voteup_count": 0,
        "updated_time": "2024-05-04 13:28:51",
        "question_id": 654782350,
        "user_id": "fc36e87769c3724b5b8208f75e6e4c09"
    },
    {
        "answer_id": 3487018340,
        "content": "KAN的创新特性KAN的创新之处在于其知识感知能力，这是一种将先验知识嵌入到神经网络架构中的方法。这种设计使得KAN在处理需要深层次推理和知识关联的任务时，如知识图谱推理和自然语言处理，表现出色。与传统的MLP相比，KAN的这种能力可以显著提高模型的准确性和解释性。KAN与MLP的比较MLP作为一种经典的神经网络架构，因其结构简单、易于训练而在多个领域得到广泛应用。然而，MLP在处理需要大量先验知识的任务时可能会受限。KAN通过整合先验知识，可能在这些特定任务上提供更好的性能。KAN的实际应用考量尽管KAN在理论上具有巨大潜力，但它是否能够在实践中取代MLP还需要更多的实证研究。MLP的成功在于其简单性和广泛的适用性，而KAN作为一种新兴技术，需要在更多的应用场景中证明自己的普适性和实用性。KAN的潜在优势KAN的理论潜力在于其能够整合先验知识，这在处理复杂任务时可能提供比MLP更深层次的理解和推理能力。在知识图谱推理、自然语言处理等领域，KAN的这种能力可以转化为更高的准确性和更好的决策支持。实证研究的必要性尽管KAN在理论上具有吸引力，但要确定其在实际应用中的有效性，需要进行广泛的实证研究。这些研究应该包括KAN在不同类型数据和任务上的表现，以及与传统MLP的比较分析。普适性和实用性的验证为了证明KAN的普适性和实用性，研究者需要在多种应用场景中对其进行测试和评估。这包括但不限于医疗诊断、金融分析、自动驾驶等，这些领域的应用可以全面展示KAN的性能。模型复杂性和训练难度KAN的模型复杂性可能会带来更高的训练难度和计算成本。因此，研究者需要探索如何优化KAN的架构和训练过程，以降低其对资源的需求，同时保持其性能优势。权衡与优化在实际应用中，开发者需要在KAN的性能、解释性和训练成本之间做出权衡。这可能涉及到对KAN架构的调整，或者开发新的训练技术和算法。未来的研究方向未来的研究可能会集中在KAN的可扩展性、泛化能力以及对不同类型知识的整合能力上。此外，研究者也需要关注KAN在多任务学习、持续学习和自适应学习中的潜力。KAN作为一种新兴的神经网络架构，其在实际应用中的考量需要综合其理论潜力、实证研究结果以及在多样化应用场景中的表现。通过不断的研究和优化，KAN有望在特定领域发挥其独特的优势，同时也可能推动整个AI领域的发展。KAN的挑战与权衡KAN可能面临的挑战包括其模型的复杂性和训练难度。在实际应用中，开发者需要在模型的性能、解释性和训练成本之间做出权衡。此外，KAN的训练可能需要大量的标注数据和计算资源，这可能限制了它在资源受限的环境中的应用。未来的发展方向未来的研究可能会集中在如何提高KAN的训练效率、降低其对数据和计算资源的需求，以及如何更好地整合不同类型的先验知识。此外，研究者也需要探索KAN在不同任务和领域的应用潜力，以及如何与传统的MLP架构进行有效的结合。KAN作为一种新兴的神经网络架构，无疑为AI领域带来了新的兴奋点。它在特定任务上展现出的优势让我们对其未来的可能性充满期待。然而，要确定KAN是否能够取代MLP，还需要更多的研究和实践来验证。在此期间，KAN和MLP可能会各自在不同的应用场景中发挥各自的优势。",
        "voteup_count": 1,
        "updated_time": "2024-05-04 11:31:17",
        "question_id": 654782350,
        "user_id": "eefcdc1d60c3f6c07db77c20b7e11474"
    },
    {
        "answer_id": 3486751736,
        "content": "What kan I say, mamba out.",
        "voteup_count": 3,
        "updated_time": "2024-05-04 01:24:50",
        "question_id": 654782350,
        "user_id": "11ea338e2903312a480bd51aa7fd5009"
    },
    {
        "answer_id": 3485979440,
        "content": "目前所谓的爆火仅仅只是新闻传播中的刷屏而已，而在不同场景中去使用KAN产生效果，并广为被人接受。估计这个还需要有一段时间。时间+实践是检验一切的方法，毕竟MLP经过了这么长时间，并在业界被广泛使用。所以KAN到底火不火，还得再等等看。",
        "voteup_count": 6,
        "updated_time": "2024-05-03 07:23:07",
        "question_id": 654782350,
        "user_id": "fbd5c2e8aa9a9f651b2ffbe3aafb3f16"
    },
    {
        "answer_id": 3486790193,
        "content": "感觉谈取代还有点为时过早，不过针对AI4S或者更进一步的AGI，除了在模型结构或者在依据数学与物理底层基础研究上实现算法上的大胆尝试与小心求证外（点个赞），希望能看到更多在数据科学和认知科学上的更多广泛而宏观框架上新的观点和突破，分享我之前的两篇文章：融合RL与LLM思想，探寻世界模型以迈向AGI「上篇」https://zhuanlan.zhihu.com/p/686181862?utm_psn=1769941078436876289融合RL与LLM思想，探寻世界模型以迈向AGI「中·下篇」https://zhuanlan.zhihu.com/p/689818745?utm_psn=1769941890924089344融合RL与LLM思想，探寻世界模型以迈向AGI「合集」https://zhuanlan.zhihu.com/p/692379889?utm_psn=1769944222109884416",
        "voteup_count": 0,
        "updated_time": "2024-05-04 04:30:11",
        "question_id": 654782350,
        "user_id": "c2f413b43c08990fcf586cb347b3a3ef"
    },
    {
        "answer_id": 3485770535,
        "content": "是一个值得关注的方向，但训练慢 10倍，很难和产业落地。KAN 并不是对传统神经网络完全的替代，而是与之融合。这是作者的的图：KAN =  Kolmogorov-Arnold + 传统网络一维函数可能是非光滑甚至是分形的，因此在实践中可能无法学习。由于这个问题吧，科尔莫戈洛夫-阿诺德表示定理基本上被判处在机器学习中死刑，被认为在理论上是正确的，但在实践中是无用的。然而作者的主要贡献是： 将 Kolmogorov-Arnold 和 MLP 结合起来，并用现代方法进行优化 。我们不必坚持原始的方程式 ，该方程式仅具有两层非线性和隐藏层中的少量项 (2n + 1)：我们将将网络推广到任意宽度和深度。其次，科学和日常生活中的大多数函数通常是平滑的，并具有稀疏的组合结构，可能有助于实现平滑的 Kolmogorov-Arnold 表示。这里的哲学与物理学家的思维方式接近，他们通常更关心典型情况而不是最坏情况。毕竟，我们的物理世界和机器学习任务必须具有结构，以使物理学和机器学习在所有情况下都具有用处或可推广性。传统的 MLP 是多层（线性函数 +固定的激活函数），训练过程学习的参数是权重 a+  偏置 b，在 KAN 变成了多层一维函数（B样条函数）组合，而这些一维函数本身的表示都是可以学习的。而线性函数 + 固定激活函数本身就是一维函数的一种，所以 KAN 不需要传统的激活函数。理论上 MLP 是  KAN的一种特殊形态。如果KANs的函数库被限制为只包含特定的线性变换和后续的非线性激活（即MLP中使用的形式），那么KANs就退化为传统的MLPs。然而，在KANs中通常允许更广泛的函数形式和更复杂的参数调整，提供了比MLPs更丰富的模型表达能力。这样做有什么优势呢 ? 除了参数效率、可解释性等。 我觉得最要的是这一点：灾难性遗忘是当前机器学习中的严重问题。当人类掌握一项任务并转而执行另一项任务时，他们不会忘记如何执行第一个任务。神经网络并非如此。当神经网络在任务 1 上训练后转移到任务 2 上训练时，网络很快会忘记如何执行任务 1。一个关键网络很快会忘记如何执行任务 1，人工神经网络和人类大脑之间的区别在于人类大脑在空间中具有功能上不同的模块。当学习新任务时，只有负责相关技能的局部区域发生结构重组[25, 26]，其他区域保持不变。大多数人工神经网络，包括 MLP，没有这种局部性的概念，这可能是灾难性遗忘的原因。作者展示了 KAN 具有局部可塑性，并且可以通过利用样条的局部性来避免灾难性遗忘。这个想法很简单：由于样条基是局部的，一个样本只会影响到几个附近的样条系数，远处的系数保持不变（这是期望的，因为远处的区域可能已经存储了我们想要保留的信息）。相比之下，由于 MLP 通常使用全局激活函数，例如 ReLU/Tanh/SiLU 等，任何局部变化可能无法控制地传播到远处的区域，破坏那里存储的信息。但目前案例都是小规模样本和测试。没有更大规模的验证。最主要的问题是训练效率低下，无法有效利用 GPU 资源。 在当下计算性能缺乏的背景下，如果不做优化，工业化落地是不太可能，但优化方向值得继续研究。",
        "voteup_count": 16,
        "updated_time": "2024-05-02 21:54:57",
        "question_id": 654782350,
        "user_id": "3db8625345af401aeae7628e25c8ddc3"
    },
    {
        "answer_id": 3485628041,
        "content": "训练速度慢10倍",
        "voteup_count": 11,
        "updated_time": "2024-05-02 18:27:17",
        "question_id": 654782350,
        "user_id": "2dc3093d3cd6663673fa57a63056f92d"
    },
    {
        "answer_id": 3486701577,
        "content": "Man！What KAN I say？ Mamba Out。梭斯。1957 年没简化的神经网络梭斯。假如跑 gpu 上节约了显存倒是梭斯如果说 AI运行速度制约在显存的话那么这个模型是可以的梭斯再说这模型是跑科学计算的梭斯。通用用途不太好梭斯但是这模型加速方法很难整出来梭斯",
        "voteup_count": 3,
        "updated_time": "2024-05-03 23:51:19",
        "question_id": 654782350,
        "user_id": "b8e59c8be41a29875c2e26dbd5b7c4a7"
    },
    {
        "answer_id": 3486042975,
        "content": "目前的状态，KAN和MLP只能说各有优势，而且MLP的应用已经很成熟，单从缓慢的处理速度来说KAN要快速应用落地就有难度，要说取代还需要更长的时间和优化升级，不过KAN在某些特定领域上会成为有力的工具比如工业系统相关的设计和控制。KAN与传统的神经网络架构截然不同，基于Kolmogorov-Arnold表示定理，用更少的参数在数学和物理问题上取得更高的精度，能天然地规避灾难性遗忘问题，并且容易注入人类的习惯偏差或领域知识，优点显而易见。但是复杂的多层嵌套单变量连续函数，算法网络结构和参数化方法的特殊性决定了训练过程中需要更多的迭代和更复杂的优化步骤，这会导致处理速度慢，要解决这个问题还需要从算法本身的结构和逻辑去优化。",
        "voteup_count": 4,
        "updated_time": "2024-05-03 09:12:39",
        "question_id": 654782350,
        "user_id": "f279fdadf9b4d24dac5203639a8e849f"
    },
    {
        "answer_id": 3485919027,
        "content": "这么说吧，只有SNN潜力是最大的，因为脉冲神经网络是最接近人脑的网络，然而现在主流的数据输入方式和训练方式都把SNN当做静态神经网络来对待，但其实即使是动态神经网络也不能发挥出SNN的潜力。SNN应该像生物一样保持持续的信号输入和处理，才能发挥出其优势。可惜并没有很好的硬件支持，而其他神经网络和深度学习方式的高回报和成熟路线，也导致没有多少人愿意投入到跨专业知识要求高、回报较低、路线不成熟的SNN里。",
        "voteup_count": 11,
        "updated_time": "2024-05-04 04:22:29",
        "question_id": 654782350,
        "user_id": "d8471aff85aef10552774b981d5887bd"
    },
    {
        "answer_id": 3486182025,
        "content": "KAN is all you need？未必吧，单一场景好用而已，不一定会像MLP那样可以作为通用的架构。简单、好用、通用才是真理。",
        "voteup_count": 0,
        "updated_time": "2024-05-03 11:57:10",
        "question_id": 654782350,
        "user_id": "3eca4d3e7fe6c4c329e5973fb2b792e5"
    },
    {
        "answer_id": 3486911331,
        "content": "我觉得KAN的历史应该比linear+sigmoid都要久远。。。对于70年代的人，KAN肯定更自然，然后搞不出来否决掉了。。。真正值得思考的事，这到底为什么值得一帮人看啊。。。ML届失去判断力了吗？过两天我去把青铜器挖出来水论文，等着",
        "voteup_count": 5,
        "updated_time": "2024-05-04 09:36:34",
        "question_id": 654782350,
        "user_id": "a46be415c292801e59f1cc563b0aebe9"
    },
    {
        "answer_id": 3486353652,
        "content": "感觉做认知心理的能用上，主要是看上可解释性这块，似乎可以用于把行为学实验数据拆解成运算函数。",
        "voteup_count": 2,
        "updated_time": "2024-05-03 15:58:56",
        "question_id": 654782350,
        "user_id": "408bebfa830fca4200328737ae48ad0e"
    },
    {
        "answer_id": 3486131335,
        "content": "我现在唯一的社交 是到处评论， 有人回复 就高兴半天",
        "voteup_count": 8,
        "updated_time": "2024-05-03 10:57:36",
        "question_id": 654782350,
        "user_id": "9377f91fc232d54034a9287377143d7d"
    },
    {
        "answer_id": 3485655941,
        "content": "全新的神经网络架构Kolmogorov–Arnold Networks（KAN）由来自MIT、加州理工、东北大学等机构的研究人员开发，它提出了一种与标准神经网络不同的架构，特别是在处理物理和数学问题时展现出了显著的优势。以下是KAN网络架构的一些关键特点和优势：1. 理论基础：KAN的设计灵感来源于Kolmogorov-Arnold表示定理，该定理表明多变量连续函数可以用单变量连续函数的有限组合表示。2. 结构创新：在KAN中，激活函数是可学习的，并且放置在网络的边缘（权重）上，而不是传统MLP中在节点（神经元）上使用固定的激活函数。3. 参数效率：KAN在实现相同性能的情况下，所需的参数数量远少于MLP，这使得KAN在参数效率上有显著优势。4. 性能：实验结果显示，KAN在数据拟合和偏微分方程（PDE）求解方面，比MLP具有更高的准确性。5. 可解释性：KAN提供了更好的可视化和交互性，有助于科学家发现新的数学和物理规律，增强了模型的可解释性。6. 持续学习：KAN展示了在持续学习中的优势，能够避免灾难性遗忘问题，这是传统MLP面临的一个重要挑战。7. 科学应用：KAN在数学的纽结理论和物理学中的Anderson局域化现象等领域展示了其潜在的应用价值。尽管KAN在多个方面展现出了潜力，但它是否能够取代MLP还需要更多的研究和实践来验证。MLP作为深度学习领域长期的基础架构，具有成熟的理论和广泛的应用基础。KAN作为一种新兴的架构，尽管在特定领域表现出色，但在通用性、训练效率、以及大规模应用方面是否能够超越MLP，还需要进一步的探索和优化。目前，KAN的爆火显示了学术界对于创新神经网络架构的渴望，以及对于提高模型性能、效率和可解释性的关注。随着进一步的研究和发展，KAN或其衍生架构有潜力在特定的机器学习和人工智能应用中发挥重要作用。",
        "voteup_count": 6,
        "updated_time": "2024-05-02 19:11:46",
        "question_id": 654782350,
        "user_id": "bfd9d54b7de2d7f65623ea9713d894a4"
    },
    {
        "answer_id": 3487119249,
        "content": "我就是说，既然纯线性linear加非线性激活函数可以做，KAN这样的也可以做，那为什么不试试神奇的傅里叶变换呢？直接找几个正余弦函数加上可学习的参数，相位，频率，振幅，以及权重，这样不也是能学的吗？同时拓宽神经网络的宽度，一个输入同时过不同的网络层，然后学个权重参数加在一起，不也挺好的。比如输入x，同时过linear的mlp，可学习激活函数的kan，以及刚刚提到的正余弦函数层，然后再加上一个可学习的权重参数，加在一起，然后送给下一层。这层就叫做moe，面对不同任务的时候，不同的层的权重不一样，学到的知识不一样，可以对各个任务都有较好的效果。当然，这些都是吹水。等我哪天试试行不行，万一能行又是篇论文。",
        "voteup_count": 1,
        "updated_time": "2024-05-04 13:31:58",
        "question_id": 654782350,
        "user_id": "0ed5d48232f054bb3c12b4d7677ffd07"
    },
    {
        "answer_id": 3486339539,
        "content": "全新的神经网络架构Kolmogorov-Arnold Networks（KANs）确实在学术界和工业界引起了广泛关注，并且有潜力在某些领域取代多层感知器（MLP）。以下是对KAN的评价：1. **创新的理论基础**：KANs的灵感来源于Kolmogorov-Arnold表示定理，这表明它们具有坚实的数学基础，能够通过函数复合来构建复杂的模型。2. **参数效率**：KANs能够在使用更少的参数情况下取得较高的精度。例如，在数学和物理问题上，仅需200个参数的KANs就能复现DeepMind用30万参数的MLPs发现的数学定理。3. **潜在的性能优势**：由于其独特的结构，KANs可能在解决特定问题时比传统的MLP更加高效和准确。4. **新的研究方向**：KANs的出现为深度学习领域提供了新的研究方向，可能会推动更多基于此架构的创新和应用。然而，尽管KANs显示出了巨大的潜力，但是否能取代MLP还需要更多的研究和实践来验证：1. **通用性的挑战**：MLP因其简单性和通用近似能力而被广泛使用。KANs是否能在所有类型的任务中都表现出色，还需要进一步的研究和实验支持。2. **实际应用的考验**：理论研究和小规模实验的成功并不总是能直接转化为实际应用中的性能提升。KANs在现实世界复杂数据和任务中的表现如何，还有待观察。3. **技术的成熟度**：作为新兴的神经网络架构，KANs需要时间来发展成熟的训练技术和优化策略，以克服可能遇到的挑战。综上所述，KANs作为一种全新的神经网络架构，确实展现出了令人瞩目的特性和潜力，但要断言它能否取代MLP还为时尚早。未来，随着更多的研究和实践，我们可能会看到KANs在特定领域内的应用和发展，甚至可能在某些任务中替代MLP成为更优的选择。 ",
        "voteup_count": 1,
        "updated_time": "2024-05-03 15:38:45",
        "question_id": 654782350,
        "user_id": "177be166f36a03ebab23ca7ba903362e"
    },
    {
        "answer_id": 3485917784,
        "content": "KAN (Kolmogorov-Arnold Network)作为一种全新的神经网络架构,的确在学术界引起了广泛关注。它有几个有趣的特点和潜在优势:1. KAN受Kolmogorov-Arnold表示定理的启发,该定理指出任何连续函数都可以表示为一系列一维函数的叠加。KAN试图利用这一思想,用一系列可学习的一维函数来拟合高维非线性映射。2. 与传统MLP将激活函数放在神经元上不同,KAN将可学习的激活函数放在权重矩阵上。这种架构上的改变可能带来更强的表达能力和灵活性。3. KAN中权重矩阵的参数化形式允许网络自适应地学习特定任务所需的激活函数,而无需像MLP那样预先确定激活函数的具体形式(如ReLU、sigmoid等)。这可能有利于自动搜索出更适合的激活函数。4. 一些实验结果表明,在图像分类、语言建模等任务上,KAN能够取得与SOTA模型相媲美的性能,参数量和计算量还更小。这展现了KAN的潜力。不过,我认为要判断KAN能否取代MLP还为时尚早:1. KAN提出的时间还很短,在更广泛的任务和数据集上的表现还有待更全面的评估验证。目前的实验还不足以下定论。2. MLP是一个成熟的通用架构,在工业界已得到大规模应用,积累了丰富的改进经验。KAN要想完全取代MLP,除了理论优势,还需在工程实践中证明其鲁棒性、可扩展性、易用性等。3. KAN本身还有一些局限,比如对高维输入的计算效率问题。后续还需要在架构和训练上做更多优化。4. 即使KAN最终不能完全取代MLP,其引入的一些新颖思路(如将激活函数与权重绑定)也可能被吸收到未来的其他架构中,推动神经网络架构的持续演进。总的来说,KAN是一个很有潜力的全新架构,但能否取代MLP还需要更长时间的发展和验证。无论如何,KAN的出现都为深度学习注入了新的活力,其思路很可能会启发更多的后续工作。让我们拭目以待。",
        "voteup_count": 0,
        "updated_time": "2024-05-03 01:59:42",
        "question_id": 654782350,
        "user_id": "7682bb4cf3288c9f14ab87f8e6e90870"
    },
    {
        "answer_id": 3485724004,
        "content": "mlp之所以经久不衰，就是因为他简单，快速，能scale-up。KAN让人想起来之前的Neural ODE，催生出来比如LTC（liquid time constant）网络这种宣称19个神经元做自动驾驶。（当然只是名字噱头，其实只是自动驾驶最简单的车道保持任务）再广一点，不管是我们熟知的ANN（人工神经网络），还是BNN（贝叶斯神经网络），SNN（脉冲神经网络），GNN（图神经网络）。我觉得对我们这种科研狗来说多一个新的网络绝对不是坏事，因为就能在上面做（shui）好多论文（bushi。但是工业界来说，这么多年还是只有ANN发扬光大，就是因为简单，快速，能scale up。KAN和这些老前辈一样面临的问题就是怎么堆大，怎么更方便硬件调度，很显然这方面KAN是大短板。如何解决这个问题是替代mlp的核心，毕竟如果只是单论可学习激活，这个topic已经是老话题了。只不过这次KAN。yysy我觉得MIT的人真的很聪明，像之前LTC一样，KAN的写作包装是值得所有科研人学习的。nature那个例子真的是太会找了",
        "voteup_count": 197,
        "updated_time": "2024-05-02 20:55:38",
        "question_id": 654782350,
        "user_id": "d66e9eacd12ccb0beb70e68dc7c6fa6d"
    },
    {
        "answer_id": 3485818375,
        "content": "实际上主体部分可以用矩阵乘做，不会很慢。flops打满还是问题不大的。简单写了个高效实现，能拟合数学函数，还没有做正经测试。坑点在README里有简单提及。https://github.com/Blealtan/efficient-kan如果有必要的话再来开坑多写点东西吧，我感觉这玩意对大模型用处不大，但是对ai4science的小模型和可解释性应该会非常有用。b-spline拟合连续函数太有优势了，relu系在它面前不堪一击（）",
        "voteup_count": 123,
        "updated_time": "2024-05-02 22:55:07",
        "question_id": 654782350,
        "user_id": "070de1975285b0ba734532c84d002c8a"
    },
    {
        "answer_id": 3486166838,
        "content": "我看到新智元的标题里“轻松复现Nature封面AI数学研究“这一部分时，心里想的是， 我们学数学的上辈子是欠了dl圈的人一个亿吗， 隔三差五地就来一波。。。自从我老婆怀孕生娃月经暂停了一段时间后，我能接触的最接近月经的事情就是新智元有事没事说某种dl方法能在数学上怎么怎么大展拳脚了。老实说，我到现在还没从之前deepmind硬蹭矩阵乘法那件事里缓过来。当时看到标题，然后再看实际的paper，差点没给我气心梗。本来工作就不好找，题目也不好做，然后这边还隔三差五地来碰个瓷。心好累。",
        "voteup_count": 78,
        "updated_time": "2024-05-04 13:00:37",
        "question_id": 654782350,
        "user_id": "6f2a1765b7b406d262de8d3002af0d3e"
    },
    {
        "answer_id": 3485866401,
        "content": "更新：展示的经验结果表明KAN在函数近似领域应该还是有用的。单个MLP确实比较容易卡在1e-4 RMSE（当然multi-stage会好很多）。这并不是fp32的问题（fp32只要实现不是太奇怪支持1e-5肯定是没问题的，理想情况应该能1e-7）。KAN能实现1e-7 RMSE，这是一个显著的优势。虽然作者的实现中每step时间是MLP的10倍，但考虑到KAN等价于向量值激活函数，应该不难优化到和同参数量的MLP每step时间接近。主要是炒作。KAN本质上就是向量值激活函数，把1个实数映射成1+m个实数，m是B-spline基函数B_i的个数。从理论上讲KAN是把权重矩阵中的元素从实数（考虑乘法，也可以看作实数到实数的映射，把x映到wx）推广为实数到实数的映射。但实际上这个映射的参数化方式是w(σ(x)+c_i*B_i(x))，其中σ是SiLU，所以也可以看作先把x提升为σ(x)和B_i(x)拼成的1+m维向量（i取1到m），再与w和wc_i拼成的向量做内积。也就是说KAN的一层相当于output=einsum('ijk,jk->i', weight, h(input))，其中input是形状为(d,)的输入特征，h(input)是形状为(d, 1+m)的激活值，weight是形状为(out_d, d, 1+m)的权重。常规MLP的一层相当于output=einsum('ij,j->i', weight, σ(input))。不难看出，KAN甚至可以被视为每一支采用不同激活函数的Inception结构。另见 @Yiping Lu 的一些考古（截图已删除，参见其回答https://www.zhihu.com/question/654774505/answer/3486199822）仔细看了一眼，1e-16指的是machine precision，需要4个stage才能nearly reach machine precision达到1e-13（Fig.5），1个stage是小于1e-4，2个stage是1e-8。虽然2个MLP就超过KAN了，但KAN给MLP标1e-4也是有一定合理性的。",
        "voteup_count": 103,
        "updated_time": "2024-05-03 17:03:49",
        "question_id": 654782350,
        "user_id": "de55655c466549f343ad2f175e593b9d"
    },
    {
        "answer_id": 3486235672,
        "content": "看到了好多大家的夸奖和批评（知乎上批评居多哈哈），受宠若惊。我设计网络和编程的时候，脑子里面想的都是数学物理的应用，所以模块化/效率等等就没有太怎么考虑，请大家多多包涵。然后也没有想到AI/ML大家这么关注。我的目标受众本来是做科学发现的群体，比较小众的。大家还是理性看待吧，什么是公众号的噱头什么真的只有自己试了才知道。欢迎大家多多尝试，探索KAN的边界在哪里，它和MLP的关系是什么，存不存在更大的框架可以包含两者。KAN/MLP肯定是各有优缺点的，看应用场景了。另外，我的默认参数都是我在文章的数学物理场景的例子中调的，不一定可以直接迁移到其它场景，可能需要仔细调调，尤其是优化部分。当然也有可能其它场景（比如大规模计算），KAN现阶段就是不如MLP合适。KAN更适合高精度和可解释的计算和科学发现。了解到大家的负面结果我也会很开心，因为能让我更好理解KAN的局限。理解大家喷喷，但也更希望大家去GitHub提提有建设性的建议。",
        "voteup_count": 155,
        "updated_time": "2024-05-03 13:09:38",
        "question_id": 654782350,
        "user_id": "34a8434ebc778731ade3734d8f291ccf"
    },
    {
        "answer_id": 3486218400,
        "content": "挺好的，会让越来越多的人意识到面向公众号做科研的危害性",
        "voteup_count": 104,
        "updated_time": "2024-05-03 12:45:26",
        "question_id": 654782350,
        "user_id": "c62e94ffec30821174e7c07a8ad6ff52"
    },
    {
        "answer_id": 3486239665,
        "content": "最近 KAN模型突然炒的火起来，但实际上我们团队在5年前就比号称nature封面的这篇更早的发表了相近工作，并奠定了核心思路。其优势是使用非线性算子（典型的是多项式或样条）可以更快的逼近任意函数，难度在于训练的算力要求过高。这个工作的本质是借鉴了MLP及SVM Kernel Function的思路。关键点有2个：1）构造算子空间，提升表征能力。把原来的MLP实数空间扩展到“实数+算子”的泛函空间，将算子/核函数视为空间中的离散元素，将MLP中的乘法扩展为算子空间的元素。2）参考泰勒展开，减小算子空间元素数量。参考泰勒展开的思路，可以使用多项式（非线性函数）累加做任意实数函数的逼近。这也就解释了为什么这类方法可以很好的应用于数学计算。关于KAN的详细解读，可以看我们团队的长文：陈巍 博士：KAN架构最全解析—最热KAN能否干掉MLP？（收录于GPT-4/ChatGPT技术与产业分析）KAN退化后就是MLP结构，是建立在MLP思路基础上的。谈不上替代，其实是新一代的MLP。但最大的训练算力问题，仍然需要攻克。相比之下，这类网络的训练算力要求增长是比MLP高几个数量级的。具体的倍数与算子空间的大小有关。核心要点：1）精度比传统MLP更高；2）训练算力要求更高，训练难度更高；3）现在看起来toy的主要原因是因为Training的算力完全跟不上；4）对AI芯片/算力的挑战巨大，目前的AI芯片Architecture普遍对这类模型支持不好；5）能否成为Main Stream Foundation架构要看有没有足够好的算力支持；6）是否做稀疏化不是难点，非线性算子的稠密inference代价可以通过结构优化方法降低，难的还是训练。我们团队在2019年发表的EDA algorithm dissertation中就定义了类似KAN层的算符隐层（具备非线性表征能力），直接将非线性函数/算子引入MLP结构，使之成为更加通用，适配范围更大的基础架构。延伸阅读陈巍：LLaMA3大模型技术精要——模型架构与训练方法详解（收录于GPT-4/ChatGPT技术与产业分析）kimi chat大模型的200万长度无损上下文可能是如何做到的？陈巍：Sora大模型技术精要万字详解——原理、关键技术、模型架构详解与应用（收录于GPT-4/ChatGPT技术与产业分析）陈巍：LLaMA-2的多模态版本架构与训练详解（收录于GPT-4/ChatGPT技术与产业分析）陈巍：AI大模型 & GPT-4技术学习与产业资源地图（上次更新于23/07/25）陈巍：GPT-4核心技术分析报告（2）——GPT-4的技术分析（收录于GPT-4/ChatGPT技术与产业分析）陈巍：GPT-4核心技术分析报告（5）——GPT-4的算力要点与芯片（收录于GPT-4/ChatGPT技术与产业分析）",
        "voteup_count": 31,
        "updated_time": "2024-05-04 01:39:51",
        "question_id": 654782350,
        "user_id": "455fe9c3762126aad1a7bf808902266a"
    },
    {
        "answer_id": 3486478763,
        "content": "看到这个不禁想说一句: 有木有量化的道友，挖过因子的，看看这个，眼熟不？[doge]",
        "voteup_count": 11,
        "updated_time": "2024-05-03 18:56:31",
        "question_id": 654782350,
        "user_id": "9517fc65fb05acb1933bce2ac5ecc4d0"
    },
    {
        "answer_id": 3486716784,
        "content": "期待以下网络、算法：Weighted Heuristic Attention Time-series networkScalable Activation Y-networkOntology-based User-centric Training这样就可以组成 what kan I say, mamba out",
        "voteup_count": 30,
        "updated_time": "2024-05-04 19:42:04",
        "question_id": 654782350,
        "user_id": "43ef6eaa8fffe307ef2a6d44994f7bd2"
    },
    {
        "answer_id": 3485782154,
        "content": "论文链接：https://arxiv.org/pdf/2404.19756关于论文的介绍，推荐 @量子位 的文章介绍，包括了一些研究人员的看法、文章概要以及实验，比较全面。量子位：全新神经网络架构KAN一夜爆火！200参数顶30万，MIT华人一作，轻松复现Nature封面AI数学研究KAN与MLP在架构上的不同，在于采用了可学习的非线性激活函数而不是可学习的线性变换。从文章的初步实验来看，有以下优势：精度更高：在一些toy datasets、求解PDE、某些小规模科学问题（文章中复现了DeepMind发表在Nature的结果）具有更高的精度。对于规模不大、精度要求高的任务，不妨尝试使用KAN代替MLP，冲击一波SOTA。参数更少：文章中复现了DeepMind发表在Nature的结果，MLP要用大约300000 个参数，而KAN大约要200 个参数。有望解决灾难性遗忘问题、缩放速度更快：显然，这都是指向大模型的。文章中使用了简单的case来说明KAN的这两点优势。可惜没能看到直接应用在大语言模型上的效果。如果能够结合最近发布的Llama3进行验证，感觉会是一个很有意义的工作。缺点：训练时间长：作者在文章中表明，KAN通常比MLP慢10倍。这里比较好奇的是居然是一个常数倍数关系。而且作者表示自己没有像一个engineer一样尽力去优化。不过，如果只是慢10倍的话，考虑到更快的缩放速度以及更少的模型权重参数，应用于LLM中也未尝不可。就当前KAN发展阶段来看，在精度要求高或者模型规模小的时候，可以尝试使用KAN代替MLP，是否有潜力取代MLP，得看最终实践效果。--------------------------摘选一些网友的意见评论：看完了这些例子，优化器是 LBFGS 的原因是什么？看到 Adam 在 pykan 中也有用，但在例子中没有用到，它也能用吗？作者回复：在高精度的情况下，LBFGS 优于 Adam（因为它是准二阶和线搜索）。但如果用户不执着于高精度，Adam 也可以很好地发挥作用。评论：我认为这是一个不公平的比较。你可以将更复杂的模型在拓扑上压缩为相同大小的架构。但是你隐藏了任意多个权重，而 MLP 包含单独的权重。（赞同这个观点，KAN中单个神经元的权重更多了，训练也更加困难。同时，堆叠KAN层的时候，这些参数将会指数级）评论：你在论文中提到，与 MLP 相比，训练速度慢约 10 倍 - 这种额外的架构复杂性是否会影响训练稳定性？如果我们将许多 KAN 层堆叠在一起，网络在训练过程中是否总是会收敛？（训练稳定问题需要搭建深层KAN进行实验探究，如果不稳定，增加残差连接或者dense Net的方式会不会解决问题？）评论：我有一个很深层次的问题，我们可以在 transformer 架构中使用这个 KANs 算法，并在 Q、K、V 或输出线性层中应用这个可学习函数。这会影响(+) ive 方向或(-) ive 方向的准确性吗？（没太看懂这个(+) ive direction是什么意思。不过尝试使用可学习的非线性激活函数代替  确实是个有意思的想法，KAFormer什么的，感觉很快会有了吧）评论：KAN在训练过程中的反向梯度传播是怎么实现的呢？网友回复：单变量函数（可学习函数，可有效充当网络中的权重和激活函数）。与 MLP 类似，误差梯度在输出处计算并通过网络向后传播，但不是更新权重，而是使用它来调整每层单变量函数的参数。网友回复：有啥区别？除非它具有另一个参数，否则如何更新单变量函数——这不是与权重完全相同的东西吗？评论：没太明白为什么KAN能克服灾难性遗忘。能解释下嘛？灾难性遗忘问题指的是在task 1中训练好的神经网络模型，然后送到task 2中训练，则该网络会迅速遗忘task 1中的知识。从类比人脑的角度去想，文章说到：人工神经网络和人类大脑之间的一个关键区别在于，人类在大脑空间局部放置了功能不同的模块。 当学习新任务时，结构重组仅发生在负责相关技能的局部区域，而其他区域保持不变。 大多数人工神经网络，包括 MLP，没有这种局部性概念，这可能是灾难性遗忘的原因。而KAN可以解决拥有这种局部放置不同模块的能力，这是因为：样条基数是局部的，因此样本只会影响一些附近的样条系数，而使远处的系数保持不变（这是可取的，因为远处的区域可能已经存储了我们想要保留的信息）。文章里做了这样的一个实验：一维回归任务由 5 个高斯峰组成。 每个峰周围的数据按顺序（而不是一次全部）呈现给 KAN 和 MLP，如图 3.4 顶行所示。 每个训练阶段后的 KAN 和 MLP 预测显示在中间和底部行中。 正如预期的那样，KAN 仅重构当前阶段存在数据的区域，而使之前的区域保持不变。 相比之下，MLP 在看到新的数据样本后会重塑整个区域，从而导致灾难性的遗忘。",
        "voteup_count": 30,
        "updated_time": "2024-05-03 15:54:35",
        "question_id": 654782350,
        "user_id": "d2d2b75a434bab27f88a991084485e42"
    },
    {
        "answer_id": 3486812967,
        "content": "讲一个改进思路吧。一个样条激活函数本质上就是使用n个2维点来插值近似一个函数。换句话说，激活函数可以等价的用二维点云刻画。不难类比最近爆火的gaussian splatting，我们可以反传梯度来自适应更新point 的split 和merge，实现自适应的函数复杂度。传统的比如基于relu的方法，亦或是KAN应该可以看作feature 都是zero-centered 的，因为他们需要设计各种normalization 方法来确保特征分布在零的周围。如果使用我这种自适应函数复杂度的样条，想必可以在远离zero center的地方照样提供非线性能力。因而在这套体系下，可以预想到，batch normalization 和 weight initialization等策略是不必要的，因为他们最初是为了解决feature shifted 的问题而设计的；而自适应函数天生地会自动在函数复杂的地方插入更多的样本点增加复杂度，相当于相对的自动引入一个bias来纠正均值偏差。当然，实际做出做的出来是另一回事。考虑到浮点数密度是不均匀的，在-1~1这个区间内密度更大精度更大，如果feature的scale太大便会损失精度。这间接说明了为啥之前的方法倾向于zero centered，变是由于机器的浮点数表示法制约的。纯数学来说，不考虑精度问题，我的思路也许是可行的？但是不好说，这么简单的idea说不定几百年前就有人做过了。也许不会完全一样，但可能会很相似，只是应用在别的领域，甚至不是ml领域罢了。反正我也不做这个方向，不妨说出来让大家思考一下。要是发现idea确实work能挂我个作者我就心满意足了，到时候就写inspired by a zhihu comment, we propose...",
        "voteup_count": 15,
        "updated_time": "2024-05-04 06:55:46",
        "question_id": 654782350,
        "user_id": "773b9041dbc3a49ddf8ef0d0fbc3a113"
    },
    {
        "answer_id": 3485932943,
        "content": "笑点解析：toy dataset。好歹跑个MNIST数据集给大伙看看比MLP高多少啊。",
        "voteup_count": 122,
        "updated_time": "2024-05-03 03:09:40",
        "question_id": 654782350,
        "user_id": "279d5560809d4fbc6ba06c1ba942f0b8"
    },
    {
        "answer_id": 3485883305,
        "content": "媒体宣传的力量太可怕了，KAN一天拿下了三大号其中两个的头版头条和一个的次条（上一次颠覆的架构这么牛还是mamba，听说mamba有几篇ICML收了，看来mamba已经势不可挡了）。这波造势，Github星标已经突破了2.2k。Github：https://github.com/KindXiaoming/pykan甚至已经写好了API文档：https://kindxiaoming.github.io/pykan/intro.html推上影响力也是非凡的，有接近60W的浏览，近900的转帖。咱们再来看看远处的mamba，从去年12月到现在也才79W浏览，400多转帖。华为的刘群老师认为这个想法很insightful。不知道是不是很快会出现各种KAN的变体，建议结合Transformer的标题可以来一篇论文的title就叫”KAN KAN Need“（doge。另外，发现好多学物理的大佬搞AI这真的是降维打击（数不过来了）。",
        "voteup_count": 35,
        "updated_time": "2024-05-03 02:15:02",
        "question_id": 654782350,
        "user_id": "3822721cffa3715db62037c499353ccd"
    },
    {
        "answer_id": 3486022922,
        "content": "COLT: Conference of Langeng in Tieba:KAN KAN NEED: two KANs are all you NEED to replace one transformer layer名字出处：如何评价全新的神经网络架构KAN的爆火，是否有潜力取代MLP？",
        "voteup_count": 26,
        "updated_time": "2024-05-03 08:42:55",
        "question_id": 654782350,
        "user_id": "10c387ed0815758d03d320568ee51efd"
    },
    {
        "answer_id": 3486895065,
        "content": "这篇论文给我印象最深的一句话：The language of science is functions. 根据问题特点，设计解决方案。专才优于通才。分工合作，合作共赢。KAN甚至不一定需要考虑在NLP、CV上面的性能。Kolmogorov-Arnold Networks (KAN)随想https://zhuanlan.zhihu.com/p/695856512",
        "voteup_count": 4,
        "updated_time": "2024-05-04 09:16:00",
        "question_id": 654782350,
        "user_id": "d7eaac230ba5121925d07b169ed8760d"
    },
    {
        "answer_id": 3487816186,
        "content": "在github上已经有老哥搞了个kan-gpt，但是好像还没有放出实验结果，期待一波。占坑的还有kan-dit,kan-transformer.越来越多的朋友正在加速验证kan的性能，能否超越，应该很快就可以揭晓!",
        "voteup_count": 1,
        "updated_time": "2024-05-05 09:37:04",
        "question_id": 654782350,
        "user_id": "aed350437795fa4113bc7f5036962754"
    },
    {
        "answer_id": 3486294965,
        "content": "来源 | 量子位  ID | QbitAI一种全新的神经网络架构KAN，诞生了！与传统的MLP架构截然不同，且能用更少的参数在数学、物理问题上取得更高精度。比如，200个参数的KANs，就能复现DeepMind用30万参数的MLPs发现数学定理研究。不仅准确性更高，并且还发现了新的公式。要知道后者可是登上Nature封面的研究啊~在函数拟合、偏微分方程求解，甚至处理凝聚态物理方面的任务都比MLP效果要好。而在大模型问题的解决上，KAN天然就能规避掉灾难性遗忘问题，并且注入人类的习惯偏差或领域知识非常容易。来自MIT、加州理工学院、东北大学等团队的研究一出，瞬间引爆一整个科技圈：Yes We KAN！甚至直接引出关于能否替代掉Transformer的MLP层的探讨，有人已经准备开始尝试……有网友表示：这看起来像是机器学习的下一步。让机器学习每个特定神经元的最佳激活，而不是由我们人类决定使用什么激活函数。还有人表示：可能正处于某些历史发展的中间。GitHub上也已经开源，也就短短两三天时间就收获1.1kStar。对MLP“进行一个简单的更改”跟MLP最大、也是最为直观的不同就是，MLP激活函数是在神经元上，而KAN把可学习的激活函数放在权重上。在作者看来，这是一个“简单的更改”。从数学定理方面来看，MLP的灵感来自于通用近似定理，即对于任意一个连续函数，都可以用一个足够深的神经网络来近似。而KAN则是来自于 Kolmogorov-Arnold 表示定理 (KART)，每个多元连续函数都可以表示为单变量连续函数的两层嵌套叠加。KAN的名字也由此而来。正是受到这一定理的启发，研究人员用神经网络将Kolmogorov-Arnold 表示参数化。为了纪念两位伟大的已故数学家Andrey Kolmogorov和Vladimir Arnold，我们称其为科尔莫格罗夫-阿诺德网络（KANs）。而从算法层面上看，MLPs 在神经元上具有（通常是固定的）激活函数，而 KANs 在权重上具有（可学习的）激活函数。这些一维激活函数被参数化为样条曲线。在实际应用过程中，KAN可以直观地可视化，提供MLP无法提供的可解释性和交互性。不过，KAN的缺点就是训练速度较慢。对于训练速度慢的问题，MIT博士生一作Ziming Liu解释道，主要有两个方面的原因。一个是技术原因，可学习的激活函数评估成本比固定激活函数成本更高。另一个则是主观原因，因为体内物理学家属性抑制程序员的个性，因此没有去尝试优化效率。对于是否能适配Transformer，他表示：暂时不知道如何做到这一点。以及对GPU友好吗？他表示：还没有，正在努力中。天然能解决大模型灾难性遗忘再来看看KAN的具体实现效果。神经缩放规律：KAN 的缩放速度比 MLP 快得多。除了数学上以Kolmogorov-Arnold 表示定理为基础，KAN缩放指数也可以通过经验来实现。在函数拟合方面，KAN比MLP更准确。而在偏微分方程求解，比如求解泊松方程，KAN比MLP更准确。研究人员还有个意外发现，就是KAN不会像MLP那样容易灾难性遗忘，它天然就可以规避这个缺陷。好好好，大模型的遗忘问题从源头就能解决。在可解释方面，KAN能通过符号公式揭示合成数据集的组成结构和变量依赖性。人类用户可以与 KANs 交互，使其更具可解释性。在 KAN 中注入人类的归纳偏差或领域知识非常容易。研究人员利用KANs还重新复现了DeepMind当年登上Nature的结果，并且还找到了Knot理论中新的公式，并以无监督的方式发现了新的结不变式关系。△DeepMind登Nature研究成果Deepmind的MLP大约300000 个参数，而KAN大约只有200 个参数。KAN 可以立即进行解释，而 MLP 则需要进行特征归因的后期分析。并且准确性也更高。对于计算要求，团队表示论文中的所有例子都可以在单个CPU上10分钟内重现。虽然KAN所能处理的问题规模比许多机器学习任务要小，但对于科学相关任务来说就刚刚好。比如研究凝固态物理中的一种相变：安德森局域化。好了，那么KAN是否会取代Transformer中的MLP层呢？有网友表示，这取决于两个因素。一点是学习算法，如 SGD、AdamW、Sophia 等—能否找到适合 KANs 参数的局部最小值？另一点则是能否在GPU上高效地实现KANs层，最好能比MLPs跟快。最后，论文中还贴心的给出了“何时该选用KAN？”的决策树。那么，你会开始尝试用KAN吗？还是让子弹再飞一会儿~项目链接：https://kindxiaoming.github.io/pykan/论文链接：https://arxiv.org/abs/2404.19756参考链接：[1]https://twitter.com/ZimingLiu11/status/1785483967719981538[2]https://twitter.com/AnthropicAI/status/1785701418546180326",
        "voteup_count": 5,
        "updated_time": "2024-05-03 14:33:13",
        "question_id": 654782350,
        "user_id": "b6384cfe8c9cf5cb7c310dd652363232"
    },
    {
        "answer_id": 3486505101,
        "content": "MLP简单有效也许有一天CNN会被抛弃，RNN会被抛弃（现在确实用的偏少了），甚至transformer也会被抛弃，但MLP真的很难被取代。交给时间吧，伟大的工作会得到时间检验奖",
        "voteup_count": 13,
        "updated_time": "2024-05-03 19:34:33",
        "question_id": 654782350,
        "user_id": "1616749f22b3fe1cc92ebcce9b3b6bdd"
    },
    {
        "answer_id": 3486911331,
        "content": "我觉得KAN的历史应该比linear+sigmoid都要久远对于70年代的人，KAN肯定更自然，然后搞不出来否决掉了真正值得思考的事，这到底为什么这么一个连MNIST实验都没有的想法能在推特上火起来啊？ML届失去判断力了吗？过两天我去把青铜器挖出来水论文，等着",
        "voteup_count": 17,
        "updated_time": "2024-05-05 02:22:58",
        "question_id": 654782350,
        "user_id": "a46be415c292801e59f1cc563b0aebe9"
    },
    {
        "answer_id": 3486666984,
        "content": "What KAN I say？ Mamba out！",
        "voteup_count": 100,
        "updated_time": "2024-05-03 23:04:37",
        "question_id": 654782350,
        "user_id": "91aaf0fa8e3ece5d105aa121ab9a16c6"
    },
    {
        "answer_id": 3485608211,
        "content": "想象你有一个复杂的拼图，需要将其拼完整。传统的拼图是由许多小块组成的，而每块都有一个特定的形状，需要与周围的块恰好契合。而KANs就像是一个特殊的拼图，可以通过学习的方式自我调整每个块的形状和位置，使其与整体更加吻合。那么，如何理解MLP与KANs之间的区别呢？MLP的拼图：MLP的拼图有一个固定的规则，每一层都像是给拼图块添加了一层统一的涂层，使其在某种程度上都能拼接起来。虽然这种方式能够完成拼图，但在面对一些复杂的形状时，它可能需要更多的拼图块才能完成。KAN的拼图：KAN的拼图则是灵活且动态的。每一块拼图都可以根据其他块的情况进行自我调整。每一层就像是对每一块拼图进行单独的雕刻，使其与其他块完美契合。KANs的这种灵活性使得它们在完成拼图的过程中具有一些天然的好处：（1）更准确：由于KANs可以对每一块拼图单独雕刻，它们在面临复杂的数据集或问题时能够提供更准确的解决方案。（2）更快速：因为KANs通过对拼图块进行调整和雕刻，使得整体结构更加协调，因此可以用较少的块数完成拼图，提高了模型的效率。（3）可解释性：KANs就像是一个透明的拼图，让我们能够看到每一块的形状是如何调整的，从而更好地理解模型的工作原理。不过，KANs也有一些缺点有待进一步的改进。首先是雕刻的时间较长：KANs需要花费更多的时间来雕刻每一块拼图，尤其是当拼图块的数量很多时，这个过程会非常耗时。这反映了KANs在训练效率上的不足，需要一些算法上的改进，比如批量化的计算，来加速训练。其次，对不同风格的拼图的适应性有待验证，KANs擅长处理一些特定风格的拼图（如科学计算问题），但对于风格迥异的其他类型的拼图（如语言理解任务）表现如何，还需要更多的实践来检验。",
        "voteup_count": 41,
        "updated_time": "2024-05-03 09:32:55",
        "question_id": 654782350,
        "user_id": "b9ac75fda4638d8013476b9855039c65"
    },
    {
        "answer_id": 3486927106,
        "content": "目前来看，得出结论还为时过早。MLP和KAN的主要区别在于其基础理论不相同，因此优化手段也不一样。KAN暂时还没有找到合适的高效优化手段，而如果想要大规模应用，就需要有人能证明KAN在实际应用中的效果。可是现在效率就制约了KAN的实用性。看来这是个鸡生蛋，蛋生鸡的问题，不知道谁愿意下场投入资源先吃螃蟹。就论文展现出的结果看，甚至还达不到Mamba所表现出的前景效果，所以个人谨慎乐观看待。",
        "voteup_count": 2,
        "updated_time": "2024-05-04 09:53:59",
        "question_id": 654782350,
        "user_id": "5f66ee7ac2f6ea2db4cddbf350d55329"
    },
    {
        "answer_id": 3486673177,
        "content": "感兴趣可以阅读如下文章：求索：深度学习颠覆性论文发布：KANs要取代MLP吗？最近麻省理工、加州理工等学校研究员发表一篇替代多层感知机MLP的论文《KAN: Kolmogorov–Arnold Networks》。论文摘要论文提出了一种新型神经网络架构——Kolmogorov-Arnold Networks（KANs）（为了纪念两位伟大的已故数学家安德烈·科尔莫戈罗夫和弗拉基米尔·阿诺德，我们称他们为科尔莫戈罗夫-阿诺德网络），它受到Kolmogorov-Arnold表示定理的启发，目标是作为多层感知器（MLPs）的替代品。KANs的特点是将激活函数置于网络的边缘（权重），而不是传统的节点上，并且这些激活函数是可学习的，由样条函数参数化。论文开发了KANs的实现代码，并通过GitHub和pip安装包分享给研究社区，促进了进一步的研究和开发。KANs解决了MLPs在非线性回归、数据拟合、偏微分方程求解以及科学发现中的一些限制，如固定激活函数的局限性、参数效率低、可解释性差等。KANs通过网格扩展技术提高准确性，即通过细化样条函数的网格来提高逼近目标函数的精度。引入简化技术，包括稀疏化、可视化、剪枝和符号化，以提高KANs的可解释性。论文核心内容MLP 是如此基础，但还有其他选择吗？MLP 将激活函数放在神经元上，但我们是否可以将（可学习的）激活函数放在权重上？是的，KAN可以！作者提出了 Kolmogorov-Arnold 网络 (KAN)，它比 MLP 更准确、更易于解释。KANs对 MLP 进行简单的更改：将激活函数从节点（神经元）移到边缘（权重）！KANs通过将每个权重参数替换为一个一元函数，利用样条函数来近似这些一元函数。这个变化乍一听似乎有些奇怪，但其实它与数学中的近似理论有着很深的联系。事实证明，Kolmogorov-Arnold 表示对应于 2 层网络，其 (可学习) 激活函数位于边上而不是节点上。从数学角度来看：MLP 受到通用近似定理 (UAT) 的启发，而 KAN 受到柯尔莫哥洛夫-阿诺德表示定理 (KART) 的启发。网络能否以固定宽度实现无限精度？UAT 的答案是“不”，而 KART 的答案是“可以”（但有警告）。从算法方面来看：KAN 和 MLP 是双重的，因为-- MLP 对神经元具有（通常固定的）激活函数，而 KAN 对权重具有（可学习的）激活函数。这些 1D 激活函数被参数化为样条函数。从实际角度来看：作者发现 KAN 比 MLP 更准确、更易于解释，尽管由于 KAN 的激活函数可学习，因此训练速度较慢。KANs在准确性上超越了MLPs，尤其是在数据拟合和PDE求解任务中，展示了更快的神经缩放法则。KAN 的缩放速度比 MLP 快得多，这在数学上基于 Kolmogorov-Arnold 表示定理。KAN 的缩放指数也可通过经验获得。KAN 在函数拟合方面比 MLP 更准确，例如拟合特殊函数。KAN在PDE求解任务上比MLP更快更准确，例如求解泊松方程。1另外，KAN 具有避免灾难性遗忘的天然能力。KANs提供了更好的可解释性，使得网络结构和激活函数可以直观地被理解和解释。KAN可以从符号公式中揭示合成数据集的组成结构和可变依赖性。人类用户可以与 KAN 交互，使其更易于解释。将人类的归纳偏见或领域知识注入 KAN 很容易。论文还探讨了KANs在科学发现中的潜力，KAN 也是科学家的得力助手或合作者。如在数学的结理论和物理的Anderson局域化中的应用。利用 KAN 重新发现了结理论中的数学定律。KAN 不仅以更小的网络和更高的自动化程度重现了GoogleDeepmind的结果，还发现了新的签名公式，并以无监督的方式发现了结不变量的新关系。特别是，Deepmind的MLP有~300000个参数，而KAN只有~200个参数。KAN可以立即解释，而MLP需要特征归因作为后期分析。KAN 可以帮助研究 Anderson 局域化，这是凝聚态物理学中的一种相变。无论是从数值上，还是从物理上，KAN 都使迁移率边缘的提取变得非常简单。根据实证结果，KAN 将成为 AI + Science 的有用模型/工具，因为它具有准确性、参数效率和可解释性。KAN 对机器学习相关任务的实用性更具推测性，有待未来研究。论文中的所有示例都可以在单个 CPU 上在不到 10 分钟的时间内重现（扫描超参数除外）。诚然，实验的问题规模比许多机器学习任务要小，但对于与科学相关的任务来说却是典型的。真的能替代MLP吗？作者的回应：感谢大家的欢呼和建设性批评。我写了几段文字来回应最近的 KAN 炒作。简而言之，我认为现在说KAN将取代MLP 还为时过早，但确实有很多有趣的方向值得探索。为什么训练很慢？原因 1：技术。可学习激活函数（样条函数）比固定激活函数的评估成本更高。原因 2：个人原因。研究作者的物理学家性格会抑制程序员性格，所以没有尝试优化效率。能适适配transformers吗？研究作者不知道该怎么做，虽然一个简单的（但可能有效！）扩展只是用 KAN 取代 MLP。paper：https://arxiv.org/abs/2404.19756code：https://github.com/KindXiaoming/pykandocumentation：https://kindxiaoming.github.io/pykan/",
        "voteup_count": 1,
        "updated_time": "2024-05-03 23:18:00",
        "question_id": 654782350,
        "user_id": "17674eb41cf5ef88cdc1e69d5169a8d7"
    },
    {
        "answer_id": 3486265152,
        "content": "一种全新的神经网络架构KAN，诞生了！与传统的MLP架构截然不同，且能用更少的参数在数学、物理问题上取得更高精度。比如，200个参数的KANs，就能复现DeepMind用30万参数的MLPs发现数学定理研究。不仅准确性更高，并且还发现了新的公式。要知道后者可是登上Nature封面的研究啊~在函数拟合、偏微分方程求解，甚至处理凝聚态物理方面的任务都比MLP效果要好。而在大模型问题的解决上，KAN天然就能规避掉灾难性遗忘问题，并且注入人类的习惯偏差或领域知识非常容易。来自MIT、加州理工学院、东北大学等团队的研究一出，瞬间引爆一整个科技圈：Yes We KAN！甚至直接引出关于能否替代掉Transformer的MLP层的探讨，有人已经准备开始尝试……有网友表示：这看起来像是机器学习的下一步。让机器学习每个特定神经元的最佳激活，而不是由我们人类决定使用什么激活函数。还有人表示：可能正处于某些历史发展的中间。GitHub上也已经开源，也就短短两三天时间就收获1.1kStar。对MLP“进行一个简单的更改”跟MLP最大、也是最为直观的不同就是，MLP激活函数是在神经元上，而KAN把可学习的激活函数放在权重上。在作者看来，这是一个“简单的更改”。从数学定理方面来看，MLP的灵感来自于通用近似定理，即对于任意一个连续函数，都可以用一个足够深的神经网络来近似。而KAN则是来自于 Kolmogorov-Arnold 表示定理 (KART)，每个多元连续函数都可以表示为单变量连续函数的两层嵌套叠加。KAN的名字也由此而来。正是受到这一定理的启发，研究人员用神经网络将Kolmogorov-Arnold 表示参数化。为了纪念两位伟大的已故数学家Andrey Kolmogorov和Vladimir Arnold，我们称其为科尔莫格罗夫-阿诺德网络（KANs）。而从算法层面上看，MLPs 在神经元上具有（通常是固定的）激活函数，而 KANs 在权重上具有（可学习的）激活函数。这些一维激活函数被参数化为样条曲线。在实际应用过程中，KAN可以直观地可视化，提供MLP无法提供的可解释性和交互性。不过，KAN的缺点就是训练速度较慢。对于训练速度慢的问题，MIT博士生一作Ziming Liu解释道，主要有两个方面的原因。一个是技术原因，可学习的激活函数评估成本比固定激活函数成本更高。另一个则是主观原因，因为体内物理学家属性抑制程序员的个性，因此没有去尝试优化效率。对于是否能适配Transformer，他表示：暂时不知道如何做到这一点。以及对GPU友好吗？他表示：还没有，正在努力中。天然能解决大模型灾难性遗忘再来看看KAN的具体实现效果。神经缩放规律：KAN 的缩放速度比 MLP 快得多。除了数学上以Kolmogorov-Arnold 表示定理为基础，KAN缩放指数也可以通过经验来实现。在函数拟合方面，KAN比MLP更准确。而在偏微分方程求解，比如求解泊松方程，KAN比MLP更准确。研究人员还有个意外发现，就是KAN不会像MLP那样容易灾难性遗忘，它天然就可以规避这个缺陷。好好好，大模型的遗忘问题从源头就能解决。在可解释方面，KAN能通过符号公式揭示合成数据集的组成结构和变量依赖性。人类用户可以与 KANs 交互，使其更具可解释性。在 KAN 中注入人类的归纳偏差或领域知识非常容易。研究人员利用KANs还重新复现了DeepMind当年登上Nature的结果，并且还找到了Knot理论中新的公式，并以无监督的方式发现了新的结不变式关系。△DeepMind登Nature研究成果Deepmind的MLP大约300000 个参数，而KAN大约只有200 个参数。KAN 可以立即进行解释，而 MLP 则需要进行特征归因的后期分析。并且准确性也更高。对于计算要求，团队表示论文中的所有例子都可以在单个CPU上10分钟内重现。虽然KAN所能处理的问题规模比许多机器学习任务要小，但对于科学相关任务来说就刚刚好。比如研究凝固态物理中的一种相变：安德森局域化。好了，那么KAN是否会取代Transformer中的MLP层呢？有网友表示，这取决于两个因素。一点是学习算法，如 SGD、AdamW、Sophia 等—能否找到适合 KANs 参数的局部最小值？另一点则是能否在GPU上高效地实现KANs层，最好能比MLPs跟快。最后，论文中还贴心的给出了“何时该选用KAN？”的决策树。那么，你会开始尝试用KAN吗？还是让子弹再飞一会儿~来源：量子位",
        "voteup_count": 3,
        "updated_time": "2024-05-03 13:49:37",
        "question_id": 654782350,
        "user_id": "dd0cf3f9b8809033760b59336b599f6d"
    },
    {
        "answer_id": 3486284731,
        "content": "谢邀。作为一个关注AI前沿动态的深度学习从业者,我对 KAN 的出现感到非常兴奋。这里我谈几点自己的拙见,希望能给题主一些启发:1. KAN 的创新点在哪里?KAN 最大的特点就是把激活函数从神经元挪到了权重上,使得每个权重都有自己独立的、可学习的激活函数。这个idea 的灵感来自于 Kolmogorov-Arnold 表示定理。通过给予权重更大的灵活性,KAN 希望能学到更强的特征表达能力。2. KAN 是否有潜力成为\"下一个 MLP\"?从 KAN 刚发布就吸引了大量目光这一点来看,它确实展现出了巨大的潜力。一些实验结果表明,KAN 在特定任务上已经性能超群。但我认为,KAN 要完全取代 MLP 还尚需时日:首先 KAN 还是一个非常年轻的架构,它的实际效果还有待更全面的实验验证;其次从工程角度看,现有的深度学习框架和芯片都是围绕 MLP 设计的,迁移到 KAN 并非易事;最后 MLP 作为一个真正经受住考验的模型,其鲁棒性和泛化性能是 KAN 暂时难以企及的。3. 后续还有哪些值得关注的研究方向?针对 KAN,我认为有几个方向值得深入探索:(1)借鉴 Kolmogorov-Arnold 定理的思路,是否可以把激活函数\"下放\"到更细粒度,比如每个输入到权重的连接上;(2)KAN 的权重激活函数是否可以引入更多的先验知识(比如 Lipschitz 约束),来提升训练稳定性;(3)在 KAN 中再引入一些 MLP 的优秀特性,比如 skip connection,也许能实现 1+1>2 的效果。",
        "voteup_count": 5,
        "updated_time": "2024-05-03 14:18:23",
        "question_id": 654782350,
        "user_id": "4b77088edcd07e1c5d062d49d54b6a76"
    },
    {
        "answer_id": 3486209232,
        "content": "五一期间，新神经网络架构KAN发布，在性能和可解释性上超越MLP架构，但目前训练时间较长。深度学习模型(推动人工智能革命的神秘引擎)不再笼罩在神秘之中。如果我们能够窥视它们的内部运作，理解它们的推理，甚至与它们合作揭开宇宙的秘密，那会怎样？这就是柯尔莫哥洛夫-阿诺德网络(KAN)的承诺，这是一种革命性的新架构，有望改变人工智能的格局。多层感知器(MLP），目前深度学习的主力架构的黑箱性质，阻碍了可解释性，低效率限制了你们的潜力，在高维数据方面的挣扎让广阔的知识领域尚未被探索。现在是时候出现一种新型的神经网络了，它将深度学习的力量与数学的优雅和人类理解的透明性结合起来MLP的核心问题在于其结构。虽然它们的通用近似能力已经得到充分证实，但它们在节点上的固定激活函数和对线性变换的依赖限制了它们有效表示复杂函数的能力，尤其是那些具有组合结构的函数。这种低效率导致模型更大，计算成本增加，并且阻碍了可解释性，因为理解其预测背后的原因变得具有挑战性。此外，MLP 经常受到数据维数灾难的影响，随着输入数据维数的增加，它们的性能会下降。KAN借鉴了柯尔莫哥洛夫-阿诺德表示定理，解决了这些痛点。柯尔莫哥洛夫-阿诺德表示定理指出，任何连续多变量函数都可以分解为单变量函数和加法的组合。KAN不使用节点上的固定激活函数，而是在边缘上使用可学习的激活函数,用样条函数表示。这一关键区别使KAN能够有效地学习函数的组合结构以及该组合中的各个函数。与MLP相比，KAN实现了更高的准确率尤其是在处理高维数据和复杂函数时。此外，KAN在可解释性方面具有显著优势。其结构允许直观地可视化所学习到的函数，从而深入了解模型的决策过程。此外,本文介绍了在不牺牲准确性的情况下简化KAN的技术，进一步提高了其透明度。",
        "voteup_count": 6,
        "updated_time": "2024-05-03 12:32:19",
        "question_id": 654782350,
        "user_id": "cb7237a0b23ea5e3dc80ea2ccb8463b9"
    },
    {
        "answer_id": 3487018340,
        "content": "KAN的创新特性KAN的创新之处在于其知识感知能力，这是一种将先验知识嵌入到神经网络架构中的方法。这种设计使得KAN在处理需要深层次推理和知识关联的任务时，如知识图谱推理和自然语言处理，表现出色。与传统的MLP相比，KAN的这种能力可以显著提高模型的准确性和解释性。KAN与MLP的比较MLP作为一种经典的神经网络架构，因其结构简单、易于训练而在多个领域得到广泛应用。然而，MLP在处理需要大量先验知识的任务时可能会受限。KAN通过整合先验知识，可能在这些特定任务上提供更好的性能。KAN的实际应用考量尽管KAN在理论上具有巨大潜力，但它是否能够在实践中取代MLP还需要更多的实证研究。MLP的成功在于其简单性和广泛的适用性，而KAN作为一种新兴技术，需要在更多的应用场景中证明自己的普适性和实用性。KAN的潜在优势KAN的理论潜力在于其能够整合先验知识，这在处理复杂任务时可能提供比MLP更深层次的理解和推理能力。在知识图谱推理、自然语言处理等领域，KAN的这种能力可以转化为更高的准确性和更好的决策支持。实证研究的必要性尽管KAN在理论上具有吸引力，但要确定其在实际应用中的有效性，需要进行广泛的实证研究。这些研究应该包括KAN在不同类型数据和任务上的表现，以及与传统MLP的比较分析。普适性和实用性的验证为了证明KAN的普适性和实用性，研究者需要在多种应用场景中对其进行测试和评估。这包括但不限于医疗诊断、金融分析、自动驾驶等，这些领域的应用可以全面展示KAN的性能。模型复杂性和训练难度KAN的模型复杂性可能会带来更高的训练难度和计算成本。因此，研究者需要探索如何优化KAN的架构和训练过程，以降低其对资源的需求，同时保持其性能优势。权衡与优化在实际应用中，开发者需要在KAN的性能、解释性和训练成本之间做出权衡。这可能涉及到对KAN架构的调整，或者开发新的训练技术和算法。未来的研究方向未来的研究可能会集中在KAN的可扩展性、泛化能力以及对不同类型知识的整合能力上。此外，研究者也需要关注KAN在多任务学习、持续学习和自适应学习中的潜力。KAN作为一种新兴的神经网络架构，其在实际应用中的考量需要综合其理论潜力、实证研究结果以及在多样化应用场景中的表现。通过不断的研究和优化，KAN有望在特定领域发挥其独特的优势，同时也可能推动整个AI领域的发展。KAN的挑战与权衡KAN可能面临的挑战包括其模型的复杂性和训练难度。在实际应用中，开发者需要在模型的性能、解释性和训练成本之间做出权衡。此外，KAN的训练可能需要大量的标注数据和计算资源，这可能限制了它在资源受限的环境中的应用。未来的发展方向未来的研究可能会集中在如何提高KAN的训练效率、降低其对数据和计算资源的需求，以及如何更好地整合不同类型的先验知识。此外，研究者也需要探索KAN在不同任务和领域的应用潜力，以及如何与传统的MLP架构进行有效的结合。KAN作为一种新兴的神经网络架构，无疑为AI领域带来了新的兴奋点。它在特定任务上展现出的优势让我们对其未来的可能性充满期待。然而，要确定KAN是否能够取代MLP，还需要更多的研究和实践来验证。在此期间，KAN和MLP可能会各自在不同的应用场景中发挥各自的优势。",
        "voteup_count": 6,
        "updated_time": "2024-05-04 11:31:17",
        "question_id": 654782350,
        "user_id": "eefcdc1d60c3f6c07db77c20b7e11474"
    },
    {
        "answer_id": 3485703777,
        "content": "科尔莫戈洛夫-阿诺德表示定理本质上是一个特殊情况的隐函数定理在一个点，以及一个不等式估计。根本没有用到全空间的刚性性质，没办法对全空间进行很好的编码勿cue Arnold，这套东西对机器学习没有帮助，比trransformer架构弱得多。",
        "voteup_count": 45,
        "updated_time": "2024-05-02 20:23:04",
        "question_id": 654782350,
        "user_id": "15a9f8b1a3ddcb9514a56ddd669846fd"
    },
    {
        "answer_id": 3487196690,
        "content": "比起大架构 大家更喜欢某个组件的革新。因为排列组合一下 无数篇ccf c和二三四区就有了。没有功劳也有苦劳嘛。毕业 不寒暄。",
        "voteup_count": 4,
        "updated_time": "2024-05-04 15:13:21",
        "question_id": 654782350,
        "user_id": "0aad266f238e1a9b7f121124a1f3ec37"
    },
    {
        "answer_id": 3486311434,
        "content": "全新的神经网络架构KAN因其独特的设计和潜在的优势而受到关注。根据提供的资料，KAN的核心特点是将可学习的激活函数置于网络的边缘（即权重）上，而不是传统MLP中的节点（神经元）上。这种设计允许每个权重参数被一个作为样条（spline）参数化的一元函数所替代，从而为网络提供了更高的灵活性和表达能力。KAN在准确性和可解释性方面展现出了相较于MLP的优势。论文中通过实验表明，KAN在数据拟合和偏微分方程（PDE）求解方面比MLP具有更高的准确性，并且参数效率更高。此外，KAN的可解释性也得到了提升，因为它可以直观地可视化，并且可以与人类用户轻松交互。然而，KAN的训练速度较慢是一个已知的问题。在相同数量的参数下，KAN的训练耗时通常是MLP的10倍。尽管如此，研究者认为这更像是一个未来可以改进的工程问题，而不是一个根本性的限制。关于KAN是否有潜力取代MLP，这还需要更多的研究和实践来验证。目前，KAN在特定领域（如科学计算）中显示出了巨大潜力，但是否能够广泛应用于其他领域还有待观察。此外，社区对于将KAN应用于更大规模问题以及其在GPU上的实现效率也表现出了兴趣。KAN作为一种新型神经网络架构，在理论上和初步实验中表现出了显著的潜力，但要取代广泛使用的MLP，还需要在效率、应用范围和易用性等方面进行更多的探索和改进。Nvidia B100/B200/GB200 关键技术解读 - 知乎 (zhihu.com)大模型训练推理如何选择GPU？一篇文章带你走出困惑（附模型大小GPU推荐图） - 知乎 (zhihu.com)一文看懂英伟达A100、A800、H100、H800各个版本有什么区别？ - 知乎 (zhihu.com)AI核弹B200发布：超级GPU新架构30倍H100单机可训15个GPT-4模型，AI进入新摩尔时代 - 知乎 (zhihu.com)紧跟“智算中心”这波大行情！人工智能引领算力基建革命！ - 知乎 (zhihu.com)先进计算技术路线图（2023） - 知乎 (zhihu.com)建议收藏！大模型100篇必读论文 - 知乎 (zhihu.com)马斯克起诉 OpenAI：精彩程度堪比电视剧，马斯克与奥特曼、OpenAI的「爱恨纠缠史」 - 知乎 (zhihu.com)生物信息学必备网站大全 - 知乎 (zhihu.com)生物信息学简史 - 知乎 (zhihu.com2023第一性原理科研服务器、量化计算平台推荐 - 知乎 (zhihu.com)Llama-2 LLM各个版本GPU服务器的配置要求是什么？ - 知乎 (zhihu.com)人工智能训练与推理工作站、服务器、集群硬件配置推荐整理了一些深度学习，人工智能方面的资料，可以看看机器学习、深度学习和强化学习的关系和区别是什么？ - 知乎 (zhihu.com)人工智能 (Artificial Intelligence, AI)主要应用领域和三种形态：弱人工智能、强人工智能和超级人工智能。买硬件服务器划算还是租云服务器划算？ - 知乎 (zhihu.com)深度学习机器学习知识点全面总结 - 知乎 (zhihu.com)自学机器学习、深度学习、人工智能的网站看这里 - 知乎 (zhihu.com)2023年深度学习GPU服务器配置推荐参考（3） - 知乎 (zhihu.com)多年来一直专注于科学计算服务器，入围政采平台，H100、A100、H800、A800、L40、L40S、RTX6000 Ada，RTX A6000，单台双路256核心服务器等。",
        "voteup_count": 0,
        "updated_time": "2024-05-03 14:58:22",
        "question_id": 654782350,
        "user_id": "fdd23d5babd02447757c668338a4cd59"
    },
    {
        "answer_id": 3486189092,
        "content": "01 KAN为何一夜爆火？！近日，一种突破性的神经网络架构——KAN(Kolmogorov–Arnold Networks)诞生啦！！ 机器学习范式就要变天啦？！它的设计哲学与传统的MLP(多层感知机)有着明显差异，且在使用更少的参数解决数学和物理问题上显示出了更高的精确度。举个例子，仅用200个参数的KAN就能重现DeepMind利用30万参数的MLP在发现数学定理方面的研究成果。下周奥特曼庆生，GPT-5或将惊艳亮相？Lla…GPT-5就要发布啦，还没体验过原生OpenAI Chat GPT4.0？快戳最新版升级教程，教你如何几分钟搞定：手把手升级ChatGPT4.0 Turbo详细步骤教程（2024年4月）这种新架构不仅加强了精确度，同时还揭示了未知的数学公式。而这些研究，DeepMind的成果曾被登载在Nature杂志的封面上呢~在函数逼近、求解偏微分方程，乃至处理凝聚态物理问题的能力上，KAN都展现出比MLP更出色的表现。 论文地址：https://arxiv.org/abs/2404.19756 项目地址：https://kindxiaoming.github.io/pykan/在解决大规模模型问题时，KAN能自然避免灾难性遗忘并且轻松融合人类的直觉偏好或特定领域知识。MIT、加州理工学院、东北大学等组成的研究团队的这项新研究一经发布，立即在整个科技界引发轰动：Yes We KAN！KAN与MLP在激活函数的配置上存在显著的差异，这也是二者最为直观的区别之一。传统MLP的激活函数通常位于神经元上，而KAN则创新地将可学习的激活函数直接置于权重之上。在作者眼中，这一改动看似“简单”，实则蕴含了深刻的变革。研究团队在MLP的基础上做出了一个微妙而关键的调整：他们把可以学习的激活函数从神经元（即节点）搬到了连接它们的连接点（即边）上！这一变化初听起来似无道理，然而它与数学领域的\"近似理论\"紧密相连，含义深远。实际上，根据Kolmogorov-Arnold展示理论，在两层网络中，拥有可学习激活函数的边而不是节点，确实显示了更大的潜力。研究人员受到展示定理的鼓舞，将Kolmogorov-Arnold表示在神经网络中明确参数化，具现化了其理念。而KAN这个名称，也是为了向两位杰出的数学家Andrey Kolmogorov以及Vladimir Arnold致敬，他们的贡献为这一理论提供了基础。02 KAN如何实现？2.1 理论根基柯尔莫哥洛夫-阿诺德表示定理（Kolmogorov–Arnold representation theorem）告诉我们，在有限界限内定义的任何连续的多变量函数，都能够被看做是单变量连续函数的有限叠加。在机器学习的视角下，这意味着学习高维函数可以转变为学习数量有限的一维函数。然而，这些一维函数可能在实际应用中是不光滑甚至是具有分形属性的，从而导致在实际中难以进行学习，因此这一定理在机器学习领域曾几乎被认为是不可行的——理论上恰当但实用性弱。即便如此，研究者们仍然看好该理论在机器学习中的潜力，并且提出如下两项改良措施：1、而不是局限于原始的方程提到的单一隐藏层（2n+1）以及两层非线性性质，可以将网络扩展为任意的宽度与深度；2、在科学和日常生活中，多数遇到的函数都较为光滑并含有简单的组合结构，能有利于构造更平滑的柯尔莫哥洛夫-阿诺德表示。这就如同区分物理学家与数学家所关注的焦点：物理学家倾向于探究通常状况下的典型例子，而数学家则更多地考虑极限情况。2.2 实现细节KAN网络的设计概念源于通过一系列单变量函数学习多变量函数的简化。在此框架中，各单变量函数可采用可学习参数的B样条曲线来表示。研究团队从增加网络层数以加深MLPs的建构概念中获得启示，提出KAN层概念，构成一维函数的矩阵，每个函数均带可学习参数。依据柯尔莫哥洛夫-阿诺德定理，KAN基层由两类函数组合——内部和外部，对应输入输出维度。堆叠KAN层增强了深度与表达力，保持解释性，其中每层单变量函数独立学习，易于解读。此处的f可以看作KAN的具体实现：03 KAN比MLP强在何处？在解释性方面，KAN能更好地揭示数据集背后的结构和变量依赖关系，通过符号公式提供洞见。神经网络缩放效率：与MLP相比，KAN的扩展效率更高。它不仅基于数学的柯尔莫哥洛夫-阿诺德定理，其缩放性能也可通过实验验证得出。为了证明其有效性，研究团队使用了5个已知可以平滑表达KA（柯尔莫哥洛夫-阿诺德）的案例作为测试数据集，并以每200步增加一个网格点的方式对KANs进行训练，覆盖的G值集合为{3,5,10,20,50,100,200,500,1000}。将不同尺寸的MLPs作为标准对照，并且在相同条件下，即使用LBFGS优化算法训练1800步，KANs与MLPs的性能通过RMSE（均方根误差）进行比较。在函数逼近任务上，KAN展现出了比MLP更高的精度在求解偏微分方程，如泊松方程时，KAN的表现也超越MLP一个意外的发现是，KAN天生能避免MLP常见的灾难性遗忘问题，为大型模型提供了从根本上避免遗忘的解决方案。04 小结关于KAN是否能替代Transformer中的MLP层，社区内部观点分歧。首先，关键在于学习算法——如SGD、AdamW、Sophia等——是否能为KANs参数找到有效的局部最小值其次，考虑到在GPU上实施KANs层的效率问题，理想情况下应优于MLPs的执行速度论文作者还贴心地提供了一个实用的决策树，帮助判断何时采用KAN！那么，您是否考虑尝试KAN，还是暂时观望？把论文丢给GPT-4进行撤稿预测，和人类审…GPT-4都可以预测论文撤稿啦，快戳最新版升级教程：手把手升级ChatGPT4.0 Turbo详细步骤教程（2024年4月）推荐文章：革命性神经网络架构KAN诞生！一夜爆火！…手把手升级ChatGPT4.0 Turbo详细步骤教程（2024年4月）下周奥特曼庆生，GPT-5或将惊艳亮相？Lla…",
        "voteup_count": 9,
        "updated_time": "2024-05-03 12:05:51",
        "question_id": 654782350,
        "user_id": "03d359f96a3d75c1b65f315e4e5ba0d6"
    },
    {
        "answer_id": 3487603214,
        "content": "关于这个问题，在解答之前，我想我们需要明白MLP为什么这么受欢迎？多层感知器（MLP）是一种前馈人工神经网络，它在神经网络的领域中有着广泛的应用。MLP的主要优点包括：非线性激活函数：MLP使用非线性激活函数（如ReLU、Sigmoid、Tanh等），这使得它能够学习和模拟复杂的非线性关系和模式。万能逼近定理：理论上，MLP可以逼近任何连续函数到任意精度，只要提供足够的隐藏层神经元和训练数据。易于理解和实现：与更复杂的网络结构相比，MLP的结构相对简单，易于理解和实现。这使得它在教学中和初学者中非常受欢迎。广泛的应用：MLP被广泛应用于各种机器学习任务，包括分类、回归、异常检测等。可扩展性：可以通过增加隐藏层的数量和神经元的数量来增加模型的复杂度，以适应更复杂的任务。并行处理能力：MLP可以通过在GPU上进行训练来利用并行处理能力，这使得训练大规模网络成为可能。成熟和广泛的研究：由于MLP在神经网络的历史中占有重要地位，因此它有着成熟和广泛的研究基础，许多优化和改进技术都是针对MLP开发的。说完MLP的优点，我们再来看看他的局限性：容易过拟合：过拟合是指模型在训练数据上表现良好，但在新的、未见过的数据上表现不佳。这种情况通常发生在模型过于复杂，拥有大量参数时，导致模型“记住”了训练数据中的噪声和细节，而未能捕捉到数据的真实分布。解决过拟合的方法包括使用正则化技术（如L1或L2正则化）、减少模型复杂性、采用数据增强或集成学习等。训练速度可能较慢：模型的训练速度受多种因素影响，包括模型的复杂性、数据集的大小、硬件性能等。一个复杂的模型，如深度神经网络，通常需要较长的训练时间。此外，大规模的数据集也会增加训练时间。为了提高训练速度，可以采用优化算法（如Adam、RMSprop）、使用更高效的硬件（如GPU）、进行批量归一化、使用预训练模型等技术。对特征缩放敏感：特征缩放是指将数据集中的特征转换到相似的尺度。某些算法（如梯度下降）对特征缩放非常敏感，因为如果特征尺度差异很大，模型可能会在迭代过程中产生不必要的震荡，导致收敛速度慢甚至无法收敛。常用的特征缩放方法包括标准化（将特征缩放到均值为0，标准差为1）和归一化（将特征缩放到特定范围，如0-1）。正确的特征缩放可以显著提高模型的性能和训练速度。简单介绍完MLP，我们再来看看KAN。KAN的灵感来源于数学中的Kolmogorov-Arnold表示定理。这一定理在数学界具有深远的影响，它揭示了函数表示与神经网络之间的深刻联系。KAN通过将这一理论应用于神经网络设计，实现了对传统MLP架构的突破。与传统的MLP相比，KAN的一个主要不同点在于其对激活函数的处理。在MLP中，激活函数是作用于每个神经元上的，而在KAN中，激活函数被转移到了权重上。这意味着在KAN中，权重本身是可学习的，而不仅仅是传递信号的媒介。这种设计使得KAN在处理复杂函数时表现出更高的灵活性和效率。其架构设计使其在函数逼近能力上具有显著优势。由于权重本身可以学习激活函数的特性，KAN能够更有效地逼近复杂的非线性函数，这对于处理现实世界中的复杂问题至关重要。在训练效率方面，KAN也展现出其优越性。由于其权重的可学习性，KAN在训练过程中可以更快地收敛，减少了所需的迭代次数，从而提高了训练效率。KAN的另一个亮点是其强大的泛化能力。在处理未见过的数据时，KAN能够更好地保持其性能，这在实际应用中尤为重要。KAN神经网络架构以其独特的创新点和优异的性能，确实具有挑战MLP地位的潜力。然而，是否能够完全取代MLP，我只能说，时间会说明一切。",
        "voteup_count": 3,
        "updated_time": "2024-05-04 23:54:52",
        "question_id": 654782350,
        "user_id": "c6fee7ae6961895fc6161ef65aa1a16d"
    },
    {
        "answer_id": 3487115633,
        "content": "KAN在结构上和理论上都挺有创意的。MLP激活函数是在神经元上，而KAN把可学习的激活函数放在权重上。MLP是靠万能近似定理，KAN是靠Kolmogorov-Arnold 表示定理 (KART)。这和我之前想过的“对每个权重都要使用激活函数，但不需要对节点使用激活函数”的想法有点相似，但是我当时只想了没有做。MLP多层感知机在1957年就被美国科学家Frank Rosenblatt提出了，但是这个KAN最近才提出来的，有很大的改进空间。KAN的潜力还要过2年回来看看才知道，但我猜最终还是要用MLP。",
        "voteup_count": 0,
        "updated_time": "2024-05-04 13:48:59",
        "question_id": 654782350,
        "user_id": "fc36e87769c3724b5b8208f75e6e4c09"
    },
    {
        "answer_id": 3486751736,
        "content": "What kan I say, mamba out.",
        "voteup_count": 5,
        "updated_time": "2024-05-04 01:24:50",
        "question_id": 654782350,
        "user_id": "11ea338e2903312a480bd51aa7fd5009"
    },
    {
        "answer_id": 3487119249,
        "content": "我就是说，既然纯线性linear加非线性激活函数可以做，KAN这样的也可以做，那为什么不试试神奇的傅里叶变换呢？直接找几个正余弦函数加上可学习的参数，相位，频率，振幅，以及权重，这样不也是能学的吗？同时拓宽神经网络的宽度，一个输入同时过不同的网络层，然后学个权重参数加在一起，不也挺好的。比如输入x，同时过linear的mlp，可学习激活函数的kan，以及刚刚提到的正余弦函数层，然后再加上一个可学习的权重参数，加在一起，然后送给下一层。这层就叫做moe，面对不同任务的时候，不同的层的权重不一样，学到的知识不一样，可以对各个任务都有较好的效果。当然，这些都是吹水。等我哪天试试行不行，万一能行又是篇论文。",
        "voteup_count": 20,
        "updated_time": "2024-05-04 13:31:58",
        "question_id": 654782350,
        "user_id": "0ed5d48232f054bb3c12b4d7677ffd07"
    },
    {
        "answer_id": 3487339797,
        "content": "What KAN I say, mamba out.",
        "voteup_count": 5,
        "updated_time": "2024-05-04 18:14:31",
        "question_id": 654782350,
        "user_id": "03acb3e28935e280bc8958e07e8464d1"
    },
    {
        "answer_id": 3486790193,
        "content": "感觉谈取代还有点为时过早，不过针对AI4S或者更进一步的AGI，除了在模型结构或者在依据数学与物理底层基础研究上实现算法上的大胆尝试与小心求证外（点个赞），希望能看到更多在数据科学和认知科学上的更多广泛而宏观框架上新的观点和突破，分享我之前的两篇文章：融合RL与LLM思想，探寻世界模型以迈向AGI「上篇」https://zhuanlan.zhihu.com/p/686181862?utm_psn=1769941078436876289融合RL与LLM思想，探寻世界模型以迈向AGI「中·下篇」https://zhuanlan.zhihu.com/p/689818745?utm_psn=1769941890924089344融合RL与LLM思想，探寻世界模型以迈向AGI「合集」https://zhuanlan.zhihu.com/p/692379889?utm_psn=1769944222109884416",
        "voteup_count": 1,
        "updated_time": "2024-05-04 04:30:11",
        "question_id": 654782350,
        "user_id": "c2f413b43c08990fcf586cb347b3a3ef"
    },
    {
        "answer_id": 3485979440,
        "content": "目前所谓的爆火仅仅只是新闻传播中的刷屏而已，而在不同场景中去使用KAN产生效果，并广为被人接受。估计这个还需要有一段时间。时间+实践是检验一切的方法，毕竟MLP经过了这么长时间，并在业界被广泛使用。所以KAN到底火不火，还得再等等看。",
        "voteup_count": 6,
        "updated_time": "2024-05-03 07:23:07",
        "question_id": 654782350,
        "user_id": "fbd5c2e8aa9a9f651b2ffbe3aafb3f16"
    },
    {
        "answer_id": 3485770535,
        "content": "是一个值得关注的方向，但训练慢 10倍，很难和产业落地。KAN 并不是对传统神经网络完全的替代，而是与之融合。这是作者的的图：KAN =  Kolmogorov-Arnold + 传统网络一维函数可能是非光滑甚至是分形的，因此在实践中可能无法学习。由于这个问题吧，科尔莫戈洛夫-阿诺德表示定理基本上被判处在机器学习中死刑，被认为在理论上是正确的，但在实践中是无用的。然而作者的主要贡献是： 将 Kolmogorov-Arnold 和 MLP 结合起来，并用现代方法进行优化 。我们不必坚持原始的方程式 ，该方程式仅具有两层非线性和隐藏层中的少量项 (2n + 1)：我们将将网络推广到任意宽度和深度。其次，科学和日常生活中的大多数函数通常是平滑的，并具有稀疏的组合结构，可能有助于实现平滑的 Kolmogorov-Arnold 表示。这里的哲学与物理学家的思维方式接近，他们通常更关心典型情况而不是最坏情况。毕竟，我们的物理世界和机器学习任务必须具有结构，以使物理学和机器学习在所有情况下都具有用处或可推广性。传统的 MLP 是多层（线性函数 +固定的激活函数），训练过程学习的参数是权重 a+  偏置 b，在 KAN 变成了多层一维函数（B样条函数）组合，而这些一维函数本身的表示都是可以学习的。而线性函数 + 固定激活函数本身就是一维函数的一种，所以 KAN 不需要传统的激活函数。理论上 MLP 是  KAN的一种特殊形态。如果KANs的函数库被限制为只包含特定的线性变换和后续的非线性激活（即MLP中使用的形式），那么KANs就退化为传统的MLPs。然而，在KANs中通常允许更广泛的函数形式和更复杂的参数调整，提供了比MLPs更丰富的模型表达能力。这样做有什么优势呢 ? 除了参数效率、可解释性等。 我觉得最要的是这一点：灾难性遗忘是当前机器学习中的严重问题。当人类掌握一项任务并转而执行另一项任务时，他们不会忘记如何执行第一个任务。神经网络并非如此。当神经网络在任务 1 上训练后转移到任务 2 上训练时，网络很快会忘记如何执行任务 1。一个关键网络很快会忘记如何执行任务 1，人工神经网络和人类大脑之间的区别在于人类大脑在空间中具有功能上不同的模块。当学习新任务时，只有负责相关技能的局部区域发生结构重组[25, 26]，其他区域保持不变。大多数人工神经网络，包括 MLP，没有这种局部性的概念，这可能是灾难性遗忘的原因。作者展示了 KAN 具有局部可塑性，并且可以通过利用样条的局部性来避免灾难性遗忘。这个想法很简单：由于样条基是局部的，一个样本只会影响到几个附近的样条系数，远处的系数保持不变（这是期望的，因为远处的区域可能已经存储了我们想要保留的信息）。相比之下，由于 MLP 通常使用全局激活函数，例如 ReLU/Tanh/SiLU 等，任何局部变化可能无法控制地传播到远处的区域，破坏那里存储的信息。但目前案例都是小规模样本和测试。没有更大规模的验证。最主要的问题是训练效率低下，无法有效利用 GPU 资源。 在当下计算性能缺乏的背景下，如果不做优化，工业化落地是不太可能，但优化方向值得继续研究。",
        "voteup_count": 19,
        "updated_time": "2024-05-02 21:54:57",
        "question_id": 654782350,
        "user_id": "3db8625345af401aeae7628e25c8ddc3"
    },
    {
        "answer_id": 3485628041,
        "content": "训练速度慢10倍",
        "voteup_count": 14,
        "updated_time": "2024-05-02 18:27:17",
        "question_id": 654782350,
        "user_id": "2dc3093d3cd6663673fa57a63056f92d"
    },
    {
        "answer_id": 3486701577,
        "content": "Man！What KAN I say？ Mamba Out。梭斯。1957 年没简化的神经网络梭斯。假如跑 gpu 上节约了显存倒是梭斯如果说 AI运行速度制约在显存的话那么这个模型是可以的梭斯再说这模型是跑科学计算的梭斯。通用用途不太好梭斯但是这模型加速方法很难整出来梭斯",
        "voteup_count": 4,
        "updated_time": "2024-05-03 23:51:19",
        "question_id": 654782350,
        "user_id": "b8e59c8be41a29875c2e26dbd5b7c4a7"
    },
    {
        "answer_id": 3486042975,
        "content": "目前的状态，KAN和MLP只能说各有优势，而且MLP的应用已经很成熟，单从缓慢的处理速度来说KAN要快速应用落地就有难度，要说取代还需要更长的时间和优化升级，不过KAN在某些特定领域上会成为有力的工具比如工业系统相关的设计和控制。KAN与传统的神经网络架构截然不同，基于Kolmogorov-Arnold表示定理，用更少的参数在数学和物理问题上取得更高的精度，能天然地规避灾难性遗忘问题，并且容易注入人类的习惯偏差或领域知识，优点显而易见。但是复杂的多层嵌套单变量连续函数，算法网络结构和参数化方法的特殊性决定了训练过程中需要更多的迭代和更复杂的优化步骤，这会导致处理速度慢，要解决这个问题还需要从算法本身的结构和逻辑去优化。",
        "voteup_count": 5,
        "updated_time": "2024-05-03 09:12:39",
        "question_id": 654782350,
        "user_id": "f279fdadf9b4d24dac5203639a8e849f"
    },
    {
        "answer_id": 3485919027,
        "content": "这么说吧，只有SNN潜力是最大的，因为脉冲神经网络是最接近人脑的网络，然而现在主流的数据输入方式和训练方式都把SNN当做静态神经网络来对待，但其实即使是动态神经网络也不能发挥出SNN的潜力。SNN应该像生物一样保持持续的信号输入和处理，才能发挥出其优势。可惜并没有很好的硬件支持，而其他神经网络和深度学习方式的高回报和成熟路线，也导致没有多少人愿意投入到跨专业知识要求高、回报较低、路线不成熟的SNN里。",
        "voteup_count": 12,
        "updated_time": "2024-05-04 04:22:29",
        "question_id": 654782350,
        "user_id": "d8471aff85aef10552774b981d5887bd"
    },
    {
        "answer_id": 3486722354,
        "content": "在MNIST上简单尝试了一下参数和显存都翻倍，收敛变慢，结果变差所以KAN的优势在哪里 ",
        "voteup_count": 12,
        "updated_time": "2024-05-04 00:32:48",
        "question_id": 654782350,
        "user_id": "20baf126484c11c9ed269fe22be93e8f"
    },
    {
        "answer_id": 3486182025,
        "content": "KAN is all you need？未必吧，单一场景好用而已，不一定会像MLP那样可以作为通用的架构。简单、好用、通用才是真理。",
        "voteup_count": 0,
        "updated_time": "2024-05-03 11:57:10",
        "question_id": 654782350,
        "user_id": "3eca4d3e7fe6c4c329e5973fb2b792e5"
    },
    {
        "answer_id": 3487532537,
        "content": "全新神经网络架构KAN：评价与潜力分析近期，一种名为KAN（Kolmogorov-Arnold Network）的新型神经网络架构引起了广泛关注。KAN的灵感来源于数学中的Kolmogorov-Arnold表示定理，其独特之处在于将可学习的激活函数置于权重之上，这与传统的多层感知机（MLP）有着显著的区别。本文将深入探讨KAN的特性和潜力，以及其是否能取代MLP成为新一代的主流神经网络架构。KAN的突破：理论与创新KAN的核心创新在于其对激活函数的处理方式。在传统的MLP中，激活函数通常应用于每个神经元的输出。而KAN则将激活函数与权重结合，使得权重本身具有激活功能。这种方法的理论基础是Kolmogorov-Arnold表示定理，该定理指出任何连续函数都可以表示为加法、乘法和开方运算的组合。KAN利用这一理论，通过学习得到具有激活功能的权重，从而简化了网络结构，提高了计算效率。KAN vs. MLP：性能与效率KAN相较于MLP的一个显著优势是其参数效率。由于KAN的权重本身就包含了激活功能，因此它需要的参数数量远少于传统MLP。这不仅降低了模型的复杂度，也提高了计算效率，尤其适用于需要大量计算资源的任务。在性能方面，KAN展现出了强大的潜力。由于其独特的激活权重设计，KAN能够更有效地捕捉和表示复杂的数据特征。在一些基准测试中，KAN已经在图像识别、自然语言处理等领域显示出了与MLP相比具有竞争力的性能。KAN的挑战与前景尽管KAN具有许多优势，但它也面临着一些挑战。首先，KAN的学习过程可能比MLP更为复杂，因为它需要在权重中学习激活函数，这可能导致训练过程的不稳定性和优化难度增加。其次，KAN的理论和应用仍在初期阶段，其性能和适用性需要更多的实验和验证。至于KAN是否能够取代MLP，目前来看还为时尚早。MLP作为经典的神经网络架构，已经在多个领域证明了其稳定性和有效性。KAN作为一种新兴架构，虽然展现出了一定的潜力和优势，但要完全取代MLP，还需要在更多的实际应用中证明其稳定性和广泛适用性。结论KAN作为一种创新的神经网络架构，以其独特的激活权重设计和高效的参数利用引起了广泛关注。尽管它在理论上具有吸引力，在实际应用中也展现出了潜力，但要完全取代MLP，KAN还需要在更多领域和任务中进行验证。未来，随着对KAN研究的深入，我们可以期待它为AI领域带来更多突破和进步。",
        "voteup_count": 5,
        "updated_time": "2024-05-04 22:27:26",
        "question_id": 654782350,
        "user_id": "3c5603c1cc7edc093ddaa8dd86f26ebe"
    },
    {
        "answer_id": 3487544191,
        "content": "最简单最朴素的方法才是最优的，除非新方法对效果是质的提升，目前来看不太可能",
        "voteup_count": 1,
        "updated_time": "2024-05-04 22:41:06",
        "question_id": 654782350,
        "user_id": "ecf05b89befaef485eead0d6d8bedb6e"
    },
    {
        "answer_id": 3487387916,
        "content": "“ 导读”全新神经网络架构KAN诞生！资讯速读由麻省理工学院、加州理工学院、东北大学等团队研究的KAN架构，一经推出立刻震撼科学界！仅200个参数的KANs，成功复现了DeepMind使用30万参数的MLPs进行的数学定理研究。并且在函数拟合等任务上，KAN的效果都优于MLP。目前的AI大模型，使用的是Transformer架构，而Transformer中就有MLP层。如果KAN优于MLP，能否替代MLP层，让大模型参数大幅下降，效果却提升呢？对此研究团队表示，KAN有训练速度较慢的缺点，并且暂且不知道如何适配Transformer，而且KAN目前使用CPU工作，而不是GPU。尽管如此，网友依然热情高涨，很多团队已经开始着手尝试！公告宣布一个好消息，从5月起“野生app主”将进行一日双更！一篇「脚踏实地」，推荐AI产品、尝试AI搞钱！一篇「仰望星空」，速递AI资讯、解读AI趋势！欢迎关注我，一起探索AI的世界！“ See you ” 我是一名AI从业者 ｜ 大学生AI产品导师。如果你偏爱以轻松的方式了解AI领域的最新动态，欢迎关注我！欢迎一键三连   ",
        "voteup_count": 1,
        "updated_time": "2024-05-04 19:21:16",
        "question_id": 654782350,
        "user_id": "8e92ed1aa80e32aae95219e39101ce47"
    },
    {
        "answer_id": 3486353652,
        "content": "感觉做认知心理的能用上，主要是看上可解释性这块，似乎可以用于把行为学实验数据拆解成运算函数。",
        "voteup_count": 2,
        "updated_time": "2024-05-03 15:58:56",
        "question_id": 654782350,
        "user_id": "408bebfa830fca4200328737ae48ad0e"
    },
    {
        "answer_id": 3486131335,
        "content": "我现在唯一的社交 是到处评论， 有人回复 就高兴半天",
        "voteup_count": 10,
        "updated_time": "2024-05-03 10:57:36",
        "question_id": 654782350,
        "user_id": "9377f91fc232d54034a9287377143d7d"
    },
    {
        "answer_id": 3485655941,
        "content": "全新的神经网络架构Kolmogorov–Arnold Networks（KAN）由来自MIT、加州理工、东北大学等机构的研究人员开发，它提出了一种与标准神经网络不同的架构，特别是在处理物理和数学问题时展现出了显著的优势。以下是KAN网络架构的一些关键特点和优势：1. 理论基础：KAN的设计灵感来源于Kolmogorov-Arnold表示定理，该定理表明多变量连续函数可以用单变量连续函数的有限组合表示。2. 结构创新：在KAN中，激活函数是可学习的，并且放置在网络的边缘（权重）上，而不是传统MLP中在节点（神经元）上使用固定的激活函数。3. 参数效率：KAN在实现相同性能的情况下，所需的参数数量远少于MLP，这使得KAN在参数效率上有显著优势。4. 性能：实验结果显示，KAN在数据拟合和偏微分方程（PDE）求解方面，比MLP具有更高的准确性。5. 可解释性：KAN提供了更好的可视化和交互性，有助于科学家发现新的数学和物理规律，增强了模型的可解释性。6. 持续学习：KAN展示了在持续学习中的优势，能够避免灾难性遗忘问题，这是传统MLP面临的一个重要挑战。7. 科学应用：KAN在数学的纽结理论和物理学中的Anderson局域化现象等领域展示了其潜在的应用价值。尽管KAN在多个方面展现出了潜力，但它是否能够取代MLP还需要更多的研究和实践来验证。MLP作为深度学习领域长期的基础架构，具有成熟的理论和广泛的应用基础。KAN作为一种新兴的架构，尽管在特定领域表现出色，但在通用性、训练效率、以及大规模应用方面是否能够超越MLP，还需要进一步的探索和优化。目前，KAN的爆火显示了学术界对于创新神经网络架构的渴望，以及对于提高模型性能、效率和可解释性的关注。随着进一步的研究和发展，KAN或其衍生架构有潜力在特定的机器学习和人工智能应用中发挥重要作用。",
        "voteup_count": 6,
        "updated_time": "2024-05-02 19:11:46",
        "question_id": 654782350,
        "user_id": "bfd9d54b7de2d7f65623ea9713d894a4"
    },
    {
        "answer_id": 3486339539,
        "content": "全新的神经网络架构Kolmogorov-Arnold Networks（KANs）确实在学术界和工业界引起了广泛关注，并且有潜力在某些领域取代多层感知器（MLP）。以下是对KAN的评价：1. **创新的理论基础**：KANs的灵感来源于Kolmogorov-Arnold表示定理，这表明它们具有坚实的数学基础，能够通过函数复合来构建复杂的模型。2. **参数效率**：KANs能够在使用更少的参数情况下取得较高的精度。例如，在数学和物理问题上，仅需200个参数的KANs就能复现DeepMind用30万参数的MLPs发现的数学定理。3. **潜在的性能优势**：由于其独特的结构，KANs可能在解决特定问题时比传统的MLP更加高效和准确。4. **新的研究方向**：KANs的出现为深度学习领域提供了新的研究方向，可能会推动更多基于此架构的创新和应用。然而，尽管KANs显示出了巨大的潜力，但是否能取代MLP还需要更多的研究和实践来验证：1. **通用性的挑战**：MLP因其简单性和通用近似能力而被广泛使用。KANs是否能在所有类型的任务中都表现出色，还需要进一步的研究和实验支持。2. **实际应用的考验**：理论研究和小规模实验的成功并不总是能直接转化为实际应用中的性能提升。KANs在现实世界复杂数据和任务中的表现如何，还有待观察。3. **技术的成熟度**：作为新兴的神经网络架构，KANs需要时间来发展成熟的训练技术和优化策略，以克服可能遇到的挑战。综上所述，KANs作为一种全新的神经网络架构，确实展现出了令人瞩目的特性和潜力，但要断言它能否取代MLP还为时尚早。未来，随着更多的研究和实践，我们可能会看到KANs在特定领域内的应用和发展，甚至可能在某些任务中替代MLP成为更优的选择。 ",
        "voteup_count": 1,
        "updated_time": "2024-05-03 15:38:45",
        "question_id": 654782350,
        "user_id": "177be166f36a03ebab23ca7ba903362e"
    },
    {
        "answer_id": 3485917784,
        "content": "KAN (Kolmogorov-Arnold Network)作为一种全新的神经网络架构,的确在学术界引起了广泛关注。它有几个有趣的特点和潜在优势:1. KAN受Kolmogorov-Arnold表示定理的启发,该定理指出任何连续函数都可以表示为一系列一维函数的叠加。KAN试图利用这一思想,用一系列可学习的一维函数来拟合高维非线性映射。2. 与传统MLP将激活函数放在神经元上不同,KAN将可学习的激活函数放在权重矩阵上。这种架构上的改变可能带来更强的表达能力和灵活性。3. KAN中权重矩阵的参数化形式允许网络自适应地学习特定任务所需的激活函数,而无需像MLP那样预先确定激活函数的具体形式(如ReLU、sigmoid等)。这可能有利于自动搜索出更适合的激活函数。4. 一些实验结果表明,在图像分类、语言建模等任务上,KAN能够取得与SOTA模型相媲美的性能,参数量和计算量还更小。这展现了KAN的潜力。不过,我认为要判断KAN能否取代MLP还为时尚早:1. KAN提出的时间还很短,在更广泛的任务和数据集上的表现还有待更全面的评估验证。目前的实验还不足以下定论。2. MLP是一个成熟的通用架构,在工业界已得到大规模应用,积累了丰富的改进经验。KAN要想完全取代MLP,除了理论优势,还需在工程实践中证明其鲁棒性、可扩展性、易用性等。3. KAN本身还有一些局限,比如对高维输入的计算效率问题。后续还需要在架构和训练上做更多优化。4. 即使KAN最终不能完全取代MLP,其引入的一些新颖思路(如将激活函数与权重绑定)也可能被吸收到未来的其他架构中,推动神经网络架构的持续演进。总的来说,KAN是一个很有潜力的全新架构,但能否取代MLP还需要更长时间的发展和验证。无论如何,KAN的出现都为深度学习注入了新的活力,其思路很可能会启发更多的后续工作。让我们拭目以待。",
        "voteup_count": 0,
        "updated_time": "2024-05-03 01:59:42",
        "question_id": 654782350,
        "user_id": "7682bb4cf3288c9f14ab87f8e6e90870"
    }
]